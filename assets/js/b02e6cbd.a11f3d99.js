"use strict";(self.webpackChunktestdoc=self.webpackChunktestdoc||[]).push([[8605],{13025:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"big-data/hdfs","title":"hdfs","description":"Namenode*","source":"@site/docs/big-data/hdfs.md","sourceDirName":"big-data","slug":"/big-data/hdfs","permalink":"/Docs/docs/next/big-data/hdfs","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/big-data/hdfs.md","tags":[],"version":"current","frontMatter":{},"sidebar":"bigdataSidebar","previous":{"title":"Flink","permalink":"/Docs/docs/next/big-data/flink/concepts"},"next":{"title":"architecture","permalink":"/Docs/docs/next/big-data/old/hbase/architecture"}}');var l=i(74848),r=i(28453);const d={},t=void 0,c={},o=[{value:"Block",id:"block",level:2},{value:"Utility of blocks",id:"utility-of-blocks",level:3},{value:"Block caching",id:"block-caching",level:2},{value:"HA",id:"ha",level:2},{value:"HDFS Federation",id:"hdfs-federation",level:2},{value:"Common problems",id:"common-problems",level:2}];function a(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Namenode"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Open, Close, Rename, Delete operations on files and directories"}),"\n",(0,l.jsx)(n.li,{children:"Determines the mapping between blocks and databnode"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Datanode"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Read, Write"}),"\n",(0,l.jsx)(n.li,{children:"Block creation, replication and deletion"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Replica selection:"})," in order to optimize bandwith and latency hdfs always read from the replica closest to reader"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["If replica exist in the same ",(0,l.jsx)(n.strong,{children:"rack"})," read it"]}),"\n",(0,l.jsx)(n.li,{children:"if cluster is spans  on multiple data center read replica from local data center"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"SafeMode:"})," at start up Name node enter in safemode state (No replication is accepted at this state)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Datanode sends Block report (list of data blocks that it holds)"}),"\n",(0,l.jsx)(n.li,{children:"Block is said in safemode if the minimum block of replicas is checked by Namenode"}),"\n",(0,l.jsx)(n.li,{children:"Once all block are check Name Node exist the safemode state"}),"\n",(0,l.jsx)(n.li,{children:"If any block doesn't satisfy the nombre of replica, Name node will replicate"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"EditLog"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Any action in FS is stored in edit log (delete a file, change file replica number, ...)"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"FsImage:"})," it stores"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"All FS namespace"}),"\n",(0,l.jsx)(n.li,{children:"Mapping of blocks to files"}),"\n",(0,l.jsx)(n.li,{children:"FS properties"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Checkpoint:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Are occurred","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["After certain time period (",(0,l.jsx)(n.strong,{children:"dfs.namenode.checkpoint.period"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:["Give number of the transactions on FS have accumulated (",(0,l.jsx)(n.strong,{children:"dfs.namenode.checkpoint.txns"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Checksum"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"When data is written or read"}),"\n",(0,l.jsx)(n.li,{children:"The datanodes are responsible for verifying the data they receive before storing the data and its checksum"}),"\n",(0,l.jsxs)(n.li,{children:["HDFS run DataBlockScanner (n a background thread that periodically verifies all the blocks stored on the datanode)\r\nThis is to guard against corruption due to ",(0,l.jsx)(n.strong,{children:"bit rot"})," in the physical storage media"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Re-replication:"})," it may occur when"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Datanode become unavailable"}),"\n",(0,l.jsx)(n.li,{children:"Replica is corrupted"}),"\n",(0,l.jsx)(n.li,{children:"Datanode hard disk fail"}),"\n",(0,l.jsx)(n.li,{children:"Replication factor is decreased"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Trash"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["When files are deleted they are moved to ",(0,l.jsx)(n.code,{children:"/user/<username>/.Trash"})]}),"\n",(0,l.jsxs)(n.li,{children:["How to configure it ",(0,l.jsx)(n.strong,{children:"???"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Backup and checkpoint nodes"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"dfs.namenode.backup.address"}),"\n",(0,l.jsx)(n.li,{children:"dfs.namenode.backup.http-address"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"block",children:"Block"}),"\n",(0,l.jsx)(n.h3,{id:"utility-of-blocks",children:"Utility of blocks"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Estimate the size easily"}),"\n",(0,l.jsx)(n.li,{children:"Separate metadata from file itself"}),"\n",(0,l.jsx)(n.li,{children:"Allows replication for providing fault-tolerant and availability"}),"\n",(0,l.jsx)(n.li,{children:"Insure against corrupted blocks and disk and machine failures"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"block-caching",children:"Block caching"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"cache directive"}),"\n",(0,l.jsx)(n.li,{children:"cache pool"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"ha",children:"HA"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Solve Single Point Of Failure (SPOF)"}),"\n",(0,l.jsxs)(n.li,{children:["Two or more Namenodes","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Active"}),"\n",(0,l.jsx)(n.li,{children:"StandBy"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["To keep the state of the cluster synchronized, Namenodes use separate group of daemons called ",(0,l.jsx)(n.strong,{children:"JournalNodes"}),"\r\nto read all edit logs from active Namenode"]}),"\n",(0,l.jsxs)(n.li,{children:["To set HA add options in ",(0,l.jsx)(n.strong,{children:"hdfs-site.xml"})]}),"\n",(0,l.jsx)(n.li,{children:"The number of JournalNodes must be at least 3"}),"\n",(0,l.jsx)(n.li,{children:"Minimum number of HA nodes is 2. It's suggested to not exceed 5 - with a recommended 3 NameNodes - due to communication overheads"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"hdfs-federation",children:"HDFS Federation"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Uses multiple Namenodes/namespaces"}),"\n",(0,l.jsx)(n.li,{children:"Namenodes are independents and don't need coordination"}),"\n",(0,l.jsx)(n.li,{children:"Datanodes are used as common storage for blocks by all Namenodes"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"common-problems",children:"Common problems"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"A massive amount of small files in hdfs => More pressure on Namenode => Slow latency"}),"\n",(0,l.jsx)(n.li,{children:"Split brain scenario"}),"\n",(0,l.jsx)(n.li,{children:"Hadoop and parquet don't support updates"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>t});var s=i(96540);const l={},r=s.createContext(l);function d(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:d(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);