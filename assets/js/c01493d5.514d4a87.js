"use strict";(self.webpackChunktestdoc=self.webpackChunktestdoc||[]).push([[2220],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>c});var s=r(96540);const i={},l=s.createContext(i);function o(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(l.Provider,{value:n},e.children)}},71788:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>t,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"big-data/Apache Spark/tuning","title":"Tuning","description":"In Driver","source":"@site/docs/big-data/Apache Spark/tuning.md","sourceDirName":"big-data/Apache Spark","slug":"/big-data/Apache Spark/tuning","permalink":"/Docs/docs/next/big-data/Apache Spark/tuning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/big-data/Apache Spark/tuning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"bigdataSidebar","previous":{"title":"structuredStream","permalink":"/Docs/docs/next/big-data/Apache Spark/structuredStream"},"next":{"title":"Data Gouverance","permalink":"/Docs/docs/next/big-data/Datagouv/"}}');var i=r(74848),l=r(28453);const o={},c="Tuning",t={},a=[{value:"In Driver",id:"in-driver",level:2},{value:"In Executor",id:"in-executor",level:2},{value:"Cache / persist",id:"cache--persist",level:2},{value:"Coalesce / repartition",id:"coalesce--repartition",level:2},{value:"Broadcast join",id:"broadcast-join",level:2},{value:"Serialization",id:"serialization",level:2},{value:"Partition tuning",id:"partition-tuning",level:2},{value:"Shuffle",id:"shuffle",level:2},{value:"Data locality",id:"data-locality",level:2},{value:"Example",id:"example",level:3},{value:"Links",id:"links",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"tuning",children:"Tuning"})}),"\n",(0,i.jsx)(n.h2,{id:"in-driver",children:"In Driver"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dynamic Executor Allocation","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Enables Spark jobs to add and remove executors on the fly"}),"\n",(0,i.jsx)(n.li,{children:"Get how much resource you need not more"}),"\n",(0,i.jsx)(n.li,{children:"Good for multi-tenant environment"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"in-executor",children:"In Executor"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Important ...."}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"spark.executor.cores"}),": the number of virtual cores to assign for each executor"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The number of threads per executor"}),"\n",(0,i.jsx)(n.li,{children:"Large number of virtual cores leads to low number of executors => reduce the parallelism"}),"\n",(0,i.jsx)(n.li,{children:"Low number of virtual cores leads to high number of executors => large amount of I/O operations"}),"\n",(0,i.jsxs)(n.li,{children:["Recommended number based of benchmarks is: ",(0,i.jsx)(n.code,{children:"spark.executor.cores=5"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"spark.executor.memory:"})," amount of memory per executor"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Assign one virtual core to Hadoop daemons"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"nb of executor per instance = (nb of total virtual cores - 1) / spark.executor.cores"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Assign ",(0,i.jsx)(n.strong,{children:"1 GB"})," for Hadoop daemons"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Total executor memory = (total RAM - 1) / nb of executor per instance"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"spark.executor.instances (--num-executor):"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The number of tasks that can be run in parallel"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Off-Heap"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.memory.offHeap.enabled"})," => true"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.memory.offHeap.size"})," => e.g : 2g"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Garbage collector"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Eliminating disk I/O bottleneck"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Disk access is 10-100K time slower than memory"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"spark.shuffle.file.buffer"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.io.compression.lz4.blockSize"})," This will reduce the size of shuffles","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Default is 32k and is sub-optimal"}),"\n",(0,i.jsx)(n.li,{children:"512k gives the best performance"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"cache--persist",children:"Cache / persist"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use cache or persist when small data is accessed frequently (like lookup table)\r\nor using iterative algorithm."}),"\n",(0,i.jsx)(n.li,{children:"Don't forget to un-cache or un-persist otherwise you'll see some spill to disk which will\r\nincrease pressure on GC."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"coalesce--repartition",children:"Coalesce / repartition"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use coalesce instead of repartition when you want to reduce partitions size. It'll avoid\r\nshuffling the data."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"broadcast-join",children:"Broadcast join"}),"\n",(0,i.jsx)(n.h2,{id:"serialization",children:"Serialization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Convert object to stream of bytes or vice versa"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Help when"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Saving data to disk"}),"\n",(0,i.jsx)(n.li,{children:"Send data over network"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Happen when things are shuffled around"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Helps to"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Decrease memory usage"}),"\n",(0,i.jsx)(n.li,{children:"Reduce network bottleneck"}),"\n",(0,i.jsx)(n.li,{children:"Enhance performance tuning"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Two types of serialization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Java serialization"}),"\n",(0,i.jsx)(n.li,{children:"Kryo serialization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Kryo serialization is exceptionally 10x faster and more compact than Java serialization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"spark.serializer => org.apache.spark.serializer.KryoSerializer"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"spark.kryoserializer.buffer.mb => 24"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"partition-tuning",children:"Partition tuning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["More issue","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Too few partitions","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Less concurrency"}),"\n",(0,i.jsx)(n.li,{children:"More susceptible to data skew"}),"\n",(0,i.jsx)(n.li,{children:"Increased memory pressure for groupBy, reduceBy, ..."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Too many partitions","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"It'll take many times to schedule task"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Need ",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"reasonable number"})})," of partition"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lower bound:"}),"  At least ~3x number of cores in the cluster"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Upper bound:"})," Ensure tasks take at least 100ms"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"shuffle",children:"Shuffle"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.shuffle.compress:"})," whether to compress map output files (true by default)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.shuffle.spill.compress:"})," wheter to compress data spilled during the shuffles (true by default)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"spark.shuffle.manager:"})," specify spark shuffle algorithm","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Hash"}),"\n",(0,i.jsx)(n.li,{children:"Sort"}),"\n",(0,i.jsx)(n.li,{children:"Tungsten"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"data-locality",children:"Data locality"}),"\n",(0,i.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["6 nodes (",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"number of nodes"})}),")"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["16 cores per node (",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"number of cores per node"})}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["One core should be reserved to Hadoop process, so ",(0,i.jsx)(n.code,{children:"number of cores per node = 15"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["64 GB RAM per node (",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"memory per node"})}),")"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The optimal value is 5 vcores per executor (",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"number of cores per executor"})}),", --executor-cores)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"number of executor per node = number of cores per node / number of vcores per executor \r\n                              = 15 / 5 \r\n                              = 3                           \n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The total number of executors (--num-executors)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"One executor (JVM process) should be left to yarn (Application Master)."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"total number of executors = (number of nodes * number of executors per node) - 1 \r\n                               = (6 * 3) - 1 \r\n                               = 17                           \n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Memory per executor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"initial memory per executor = memory per node / number of executor per node \r\n                               = 64 / 3\r\n                               = 21 GB                           \n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"This isn't the final memory we should allocate small overhead memory needed by yarn process"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"overhead memory = max(384, 0.07 * spark.executor.memory)\r\n                = max(384, 0.07 * 21)\r\n                = 1.47\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"memory per executor = initial memory per executor -  overhead memory\r\n                        = 21 - 1.47\r\n                        ~= 19 GB\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"links",children:"Links"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://aws.amazon.com/fr/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/",children:"Link-1"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);