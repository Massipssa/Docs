"use strict";(self.webpackChunktestdoc=self.webpackChunktestdoc||[]).push([[528],{28453:(e,r,o)=>{o.d(r,{R:()=>n,x:()=>d});var t=o(96540);const a={},s=t.createContext(a);function n(e){const r=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function d(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:n(e.components),t.createElement(s.Provider,{value:r},e.children)}},66784:(e,r,o)=>{o.r(r),o.d(r,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>n,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"cloud/gcp/bigdata/composer","title":"Composer","description":"- Fully managed service of Apache Airflow","source":"@site/versioned_docs/version-1.0/cloud/gcp/bigdata/composer.md","sourceDirName":"cloud/gcp/bigdata","slug":"/cloud/gcp/bigdata/composer","permalink":"/Docs/docs/cloud/gcp/bigdata/composer","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/versioned_docs/version-1.0/cloud/gcp/bigdata/composer.md","tags":[],"version":"1.0","frontMatter":{},"sidebar":"cloudSidebar","previous":{"title":"Cloud Stroage","permalink":"/Docs/docs/cloud/gcp/bigdata/cloud-storage"},"next":{"title":"Dataflow","permalink":"/Docs/docs/cloud/gcp/bigdata/dataflow"}}');var a=o(74848),s=o(28453);const n={},d="Composer",c={},l=[];function i(e){const r={code:"code",em:"em",h1:"h1",header:"header",li:"li",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.header,{children:(0,a.jsx)(r.h1,{id:"composer",children:"Composer"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["Fully managed service of ",(0,a.jsx)(r.em,{children:(0,a.jsx)(r.strong,{children:"Apache Airflow"})})]}),"\n",(0,a.jsx)(r.li,{children:"Allows to: create, schedule and monitor a data workflow"}),"\n",(0,a.jsx)(r.li,{children:"Composer = Airflow + Google Kubernetes Engine (GKE) + Cloud Storage"}),"\n"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"\r\nfrom airflow import DAG\r\nfrom airflow.operators import BashOperator\r\nfrom datetime import datetime, timedelta\r\n\r\n# 1 - declare defautl arguments\r\ndefault_args = {\r\n    'owner': 'massipssa',\r\n    'depends_on_past': False,\r\n    'start_date': datetime(2020, 6, 12),\r\n    'email': ['kerrache.massipssa@gmail.com'],\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=1),\r\n}\r\n\r\n# 2 - define a DAG\r\ndag = DAG('helloworld_dag', default_args=default_args)\r\n\r\n# 3 - define DAG's tasks\r\ntask_1 = BashOperator(\r\n    task_id='task_1',\r\n    bash_command='echo \"Hello World from Task 1\"',\r\n    dag=dag)\r\n\r\ntask_1 = BashOperator(\r\n    task_id='task_2',\r\n    bash_command='echo \"Hello World from Task 2\"',\r\n    dag=dag)\r\n\r\n# 4 - set task dependencies\r\ntask_1 >> task_2\n"})})]})}function m(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(i,{...e})}):i(e)}}}]);