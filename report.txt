import org.scalatest.funsuite.AnyFunSuite
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import com.package.InterfaceProfondeurAllowedValuesCheck  // Adjust to your package

class InterfaceProfondeurAllowedValuesCheckTest extends AnyFunSuite {

  val spark = SparkSession.builder()
    .appName("InterfaceProfondeurCheckTest")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  // Define nested schema for interfaceDto
  val schema = StructType(Seq(
    StructField("interfaceDto", StructType(Seq(
      StructField("profondeur", StringType, nullable = true),
      StructField("codeEvenement", StringType, nullable = true)
    )))
  ))

  test("check should flag rows with invalid interfaceDto.profondeur values") {
    val data = Seq(
      Row(Row("invalid_value", "EVT1")),
      Row(Row("", "EVT2")),
      Row(Row(null, "EVT3")),
      Row(Row("VALID_VALUE", "EVT4")) // assume this is valid
    )

    val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)

    val result = InterfaceProfondeurAllowedValuesCheck.check(df)

    assert(result.size == 3)

    val messages = result.flatMap(_.message)
    assert(messages.exists(_.contains("Found profondeur: invalid_value")))
    assert(messages.exists(_.contains("Found profondeur: ")))
    assert(messages.exists(_.contains("Found profondeur: null")) || messages.exists(_.contains("Found profondeur:")))
  }

  test("check should return empty list when all values are valid") {
    val data = Seq(
      Row(Row("VALID_VALUE", "EVT1")),
      Row(Row("VALID_VALUE", "EVT2"))
    )

    val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)

    val result = InterfaceProfondeurAllowedValuesCheck.check(df)

    // No errors expected
    assert(result.isEmpty)
  }
}
