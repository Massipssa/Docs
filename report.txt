import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

case class StructureInterfaceDto(
  famille: String,
  identifiant: String,
  libelleLong: String,
  nomDonnee: String,
  obligatoire: String,
  propagation: String,
  definition: String,
  position: Int
)

case class InterfaceDto(
  codeEvenement: String,
  codeBoite: String,
  libelleLong: String,
  dateMaj: String,
  profondeur: String,
  description: String,
  structureInterface: List[StructureInterfaceDto]
)

case class MetadataLineDto(
  creId: String,
  env: String,
  interfaceDto: InterfaceDto
)

object MetadataParser {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("MetadataLineDto Parser")
      .master("local[*]") // Run locally
      .getOrCreate()

    import spark.implicits._

    // Load the text file using read.text()
    val rawData = spark.read.text("path/to/your/file.txt")

    // Split each line into three parts: creId, env, JSON Data
    val jsonData = rawData.withColumn("splitData", split(col("value"), ",", 3))
      .select(
        col("splitData").getItem(0).alias("creId"),
        col("splitData").getItem(1).alias("env"),
        col("splitData").getItem(2).alias("jsonData")
      )

    // Define schema for InterfaceDto
    val interfaceDtoSchema = Encoders.product[InterfaceDto].schema

    // Parse JSON column into `Row` first, then extract
    val parsedDF = jsonData.withColumn("interfaceDto", from_json(col("jsonData"), interfaceDtoSchema))

    // Convert DataFrame to Dataset[MetadataLineDto] manually
    val metadataDataset = parsedDF.map(row =>
      MetadataLineDto(
        creId = row.getString(0),
        env = row.getString(1),
        interfaceDto = row.getAs[Row]("interfaceDto").toSeq.map(_.asInstanceOf[InterfaceDto]).head
      )
    )

    // Show parsed data
    metadataDataset.show(truncate = false)

    // Stop Spark session
    spark.stop()
  }
}
