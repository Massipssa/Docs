import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.amazonaws.services.s3.model._
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}

import java.io._
import java.util.UUID
import scala.collection.mutable.ListBuffer
import scala.concurrent.{ExecutionContext, Future}
import scala.util.{Failure, Success}
import scala.jdk.CollectionConverters._
import java.util.concurrent.Executors

object S3TarBz2ParallelThreadedUpload {

  val bucketName = "your-bucket-name"
  val inputKey = "input-folder/archive.tar.bz2"
  val outputPrefix = "extracted-folder/"
  val partSize = 100 * 1024 * 1024 // 100MB par partie

  // Cr√©er un ThreadPool avec 5 threads pour les uploads
  implicit val ec: ExecutionContext = ExecutionContext.fromExecutor(Executors.newFixedThreadPool(5))

  def extractAndUploadParallel(s3Client: AmazonS3): Unit = {
    // 1Ô∏è‚É£ Lire le fichier `.tar.bz2` en streaming depuis S3
    val s3Object = s3Client.getObject(new GetObjectRequest(bucketName, inputKey))
    val s3InputStream = s3Object.getObjectContent

    // 2Ô∏è‚É£ D√©compresser le fichier en m√©moire
    val bz2Stream = new BZip2CompressorInputStream(s3InputStream)
    val tarInput = new TarArchiveInputStream(bz2Stream)

    var entry: TarArchiveEntry = tarInput.getNextTarEntry

    val uploadFutures = ListBuffer[Future[Unit]]()

    while (entry != null) {
      if (!entry.isDirectory) {
        val fileName = entry.getName
        val s3TargetKey = outputPrefix + fileName

        println(s"Extraction et Upload en parall√®le (Thread) : $fileName -> $s3TargetKey")

        // Lire le fichier extrait en m√©moire
        val outputStream = new ByteArrayOutputStream()
        val buffer = new Array[Byte](16 * 1024) // 16KB buffer
        var bytesRead = 0

        while ({ bytesRead = tarInput.read(buffer); bytesRead != -1 }) {
          outputStream.write(buffer, 0, bytesRead)
        }

        val fileData = outputStream.toByteArray
        outputStream.close()

        // 3Ô∏è‚É£ Lancer l'upload en parall√®le avec plusieurs threads
        val futureUpload = Future {
          multipartUpload(s3Client, bucketName, s3TargetKey, fileData)
        }

        uploadFutures.append(futureUpload)
      }
      entry = tarInput.getNextTarEntry
    }

    // 4Ô∏è‚É£ Fermer les flux
    tarInput.close()
    bz2Stream.close()
    s3InputStream.close()

    // 5Ô∏è‚É£ Attendre la fin de tous les uploads
    Future.sequence(uploadFutures).onComplete {
      case Success(_) => println("‚úÖ Tous les fichiers ont √©t√© upload√©s avec succ√®s üöÄ")
      case Failure(ex) => println(s"‚ùå Erreur lors de l'upload: ${ex.getMessage}")
    }
  }

  def multipartUpload(s3Client: AmazonS3, bucket: String, key: String, data: Array[Byte]): Unit = {
    val uploadId = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(bucket, key)).getUploadId
    val partETags = ListBuffer[PartETag]()
    val totalSize = data.length
    var start = 0
    var partNumber = 1

    while (start < totalSize) {
      val end = Math.min(start + partSize, totalSize)
      val partData = data.slice(start, end)

      val inputStream = new ByteArrayInputStream(partData)
      val uploadPartRequest = new UploadPartRequest()
        .withBucketName(bucket)
        .withKey(key)
        .withUploadId(uploadId)
        .withPartNumber(partNumber)
        .withInputStream(inputStream)
        .withPartSize(partData.length)

      val partResult = s3Client.uploadPart(uploadPartRequest)
      partETags.append(partResult.getPartETag)

      println(s"Partie $partNumber upload√©e avec Thread : ${partData.length} bytes")
      start = end
      partNumber += 
