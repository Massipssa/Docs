import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import fr.ccf.job.commons.dtos.metadata.CheckErrorDto

object CsvMinimalColumnCountCheck {
  def check(dataFileDF: DataFrame, directiveColumnCount: Int)(implicit spark: SparkSession): Option[CheckErrorDto] = {

    import spark.implicits._

    if (dataFileDF.isEmpty) {
      throw new RuntimeException("DATA file content DataFrame is empty.")
    }

    // ✅ Count the number of columns in the DataFrame
    val numberOfColumns: Int = dataFileDF.columns.length

    // ✅ Check if the number of columns exceeds the allowed limit
    if (numberOfColumns <= directiveColumnCount) {
      return None // ✅ No error, return empty
    }

    // ✅ Construct error message
    val checkError = CheckErrorDto(
      message = s"CSV file has more than the allowed number of directive columns ($directiveColumnCount), found $numberOfColumns columns."
    )

    Some(checkError) // ✅ Return error
  }
}
