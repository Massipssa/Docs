import org.apache.spark.sql.SparkSession
import org.apache.iceberg.spark.actions.SparkActions

import java.time.{Instant, ZoneOffset}
import java.time.temporal.ChronoUnit

// -----------------------------
// Configuration
// -----------------------------
final case class IcebergMaintenanceConfig(
  // Compaction
  rewriteDataFiles: Boolean = true,
  targetFileSizeBytes: Long = 512L * 1024 * 1024,  // 512MB
  minFileSizeBytes: Long = 128L * 1024 * 1024,     // ne réécrit que les fichiers < 128MB
  maxFileSizeBytes: Long = 1024L * 1024 * 1024,    // 1GB (évite des énormes fichiers)
  strategy: String = "binpack",                    // "binpack" ou "sort"
  sortColumns: Seq[String] = Seq.empty,            // si strategy="sort"
  shufflePartitions: Option[Int] = Some(48),

  // Manifests
  rewriteManifests: Boolean = true,

  // Delete files (selon version Iceberg)
  rewriteDeleteFiles: Boolean = true,

  // Snapshots
  expireSnapshots: Boolean = true,
  expireOlderThanDays: Int = 14,                   // expire snapshots plus vieux que 14 jours
  retainLast: Int = 10,                            // garde au moins N snapshots

  // Orphans (attention: à exécuter avec marge)
  removeOrphanFiles: Boolean = true,
  orphanOlderThanDays: Int = 3,                    // supprime les orphelins plus vieux que 3 jours

  // Spark tuning
  enableAQE: Boolean = true,
  coalesceAQE: Boolean = true,

  // Catalog refresh
  refreshSparkTable: Boolean = true
)

// -----------------------------
// Runner
// -----------------------------
object IcebergMaintenanceRunner {

  final case class MaintenanceReport(steps: Seq[(String, String)]) {
    override def toString: String =
      steps.map { case (s, r) => s"- $s: $r" }.mkString("\n")
  }

  def runAll(spark: SparkSession, tableIdent: String, cfg: IcebergMaintenanceConfig): MaintenanceReport = {
    val steps = scala.collection.mutable.ArrayBuffer.empty[(String, String)]

    // Spark tuning recommandé pour compaction
    if (cfg.enableAQE) spark.conf.set("spark.sql.adaptive.enabled", "true")
    if (cfg.coalesceAQE) spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    cfg.shufflePartitions.foreach(p => spark.conf.set("spark.sql.shuffle.partitions", p.toString))

    // 0) Refresh (utile si table modifiée ailleurs)
    if (cfg.refreshSparkTable) {
      spark.catalog.refreshTable(tableIdent)
      steps += "refreshTable" -> "OK"
    }

    // 1) Rewrite Data Files (compaction)
    if (cfg.rewriteDataFiles) {
      val action = SparkActions.get(spark)
        .rewriteDataFiles(tableIdent)
        .option("target-file-size-bytes", cfg.targetFileSizeBytes.toString)
        .option("min-file-size-bytes", cfg.minFileSizeBytes.toString)
        .option("max-file-size-bytes", cfg.maxFileSizeBytes.toString)

      cfg.shufflePartitions.foreach(p => action.option("shuffle-partitions", p.toString))

      val res = cfg.strategy.toLowerCase match {
        case "sort" =>
          // Exemple: tri pour regrouper (meilleur scan séquentiel, z-order pas natif ici)
          // NOTE: nécessite de donner sortColumns
          if (cfg.sortColumns.isEmpty)
            throw new IllegalArgumentException("strategy=sort mais sortColumns est vide.")
          action.sort(cfg.sortColumns: _*).execute()

        case _ =>
          action.binPack().execute()
      }

      steps += "rewriteDataFiles" -> s"rewrittenDataFiles=${res.rewrittenDataFilesCount()}, addedDataFiles=${res.addedDataFilesCount()}"
    }

    // 2) Rewrite Manifests
    if (cfg.rewriteManifests) {
      val res = SparkActions.get(spark).rewriteManifests(tableIdent).execute()
      steps += "rewriteManifests" -> s"rewrittenManifests=${res.rewrittenManifestsCount()}"
    }

    // 3) Rewrite Delete Files (selon version Iceberg: rewriteDeleteFiles ou rewritePositionDeleteFiles)
    if (cfg.rewriteDeleteFiles) {
      val msg =
        if (tryRewriteDeleteFiles(spark, tableIdent, cfg)) "OK"
        else "SKIPPED (action non supportée par ta version Iceberg)"
      steps += "rewriteDeleteFiles" -> msg
    }

    // 4) Expire snapshots (et cleanup metadata associé)
    if (cfg.expireSnapshots) {
      val olderThanMillis =
        Instant.now().minus(cfg.expireOlderThanDays.toLong, ChronoUnit.DAYS).toEpochMilli

      val res = SparkActions.get(spark)
        .expireSnapshots(tableIdent)
        .expireOlderThan(olderThanMillis)
        .retainLast(cfg.retainLast)
        .execute()

      steps += "expireSnapshots" -> s"deletedSnapshots=${res.deletedSnapshotsCount()}, deletedDataFiles=${res.deletedDataFilesCount()}"
    }

    // 5) Remove orphan files (à exécuter avec une marge de sécurité !)
    if (cfg.removeOrphanFiles) {
      val olderThanMillis =
        Instant.now().minus(cfg.orphanOlderThanDays.toLong, ChronoUnit.DAYS).toEpochMilli

      val res = SparkActions.get(spark)
        .removeOrphanFiles(tableIdent)
        .olderThan(olderThanMillis)
        .execute()

      steps += "removeOrphanFiles" -> s"deletedFiles=${res.orphanFileCount()}"
    }

    // Final refresh
    if (cfg.refreshSparkTable) {
      spark.catalog.refreshTable(tableIdent)
      steps += "refreshTable(end)" -> "OK"
    }

    MaintenanceReport(steps.toSeq)
  }

  /**
   * Essaie différentes actions selon les versions Iceberg.
   * - rewriteDeleteFiles (plus récent)
   * - rewritePositionDeleteFiles (selon version)
   */
  private def tryRewriteDeleteFiles(spark: SparkSession, tableIdent: String, cfg: IcebergMaintenanceConfig): Boolean = {
    // 1) rewriteDeleteFiles (si disponible)
    val ok1 = tryInvokeSparkActions(spark, "rewriteDeleteFiles", tableIdent, cfg)
    if (ok1) return true

    // 2) rewritePositionDeleteFiles (si disponible)
    val ok2 = tryInvokeSparkActions(spark, "rewritePositionDeleteFiles", tableIdent, cfg)
    ok2
  }

  private def tryInvokeSparkActions(
    spark: SparkSession,
    methodName: String,
    tableIdent: String,
    cfg: IcebergMaintenanceConfig
  ): Boolean = {
    try {
      val actions = SparkActions.get(spark)
      val m = actions.getClass.getMethods.find(m => m.getName == methodName && m.getParameterCount == 1)
      m match {
        case Some(mm) =>
          val actionObj = mm.invoke(actions, tableIdent) // retourne une Action
          // Optionnel: target size + shuffle-partitions si l'action supporte option(key,val)
          cfg.shufflePartitions.foreach(p => invokeOptionIfExists(actionObj, "shuffle-partitions", p.toString))
          invokeOptionIfExists(actionObj, "target-file-size-bytes", cfg.targetFileSizeBytes.toString)
          // execute()
          val exec = actionObj.getClass.getMethods.find(_.getName == "execute").get
          exec.invoke(actionObj)
          true
        case None => false
      }
    } catch {
      case _: Throwable => false
    }
  }

  private def invokeOptionIfExists(actionObj: AnyRef, k: String, v: String): Unit = {
    val opt = actionObj.getClass.getMethods.find(m =>
      m.getName == "option" && m.getParameterCount == 2 &&
        m.getParameterTypes.apply(0) == classOf[String] &&
        m.getParameterTypes.apply(1) == classOf[String]
    )
    opt.foreach(_.invoke(actionObj, k, v))
  }
}
