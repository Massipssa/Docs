import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.{S3Client, S3Utilities}
import software.amazon.awssdk.services.s3.model.GetObjectRequest
import java.io._
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}
import scala.concurrent.{ExecutionContext, Future}
import java.util.concurrent.Executors

object S3GlueUntar {

  implicit val ec: ExecutionContext = ExecutionContext.fromExecutor(Executors.newFixedThreadPool(4))

  // Create S3 Client with credentials
  def createS3Client(): S3Client = {
    val accessKey = "your-access-key"
    val secretKey = "your-secret-key"
    val credentials = AwsBasicCredentials.create(accessKey, secretKey)
    val credentialsProvider = StaticCredentialsProvider.create(credentials)

    S3Client.builder()
      .region(Region.US_EAST_1) // Set correct AWS region
      .credentialsProvider(credentialsProvider)
      .build()
  }

  val s3: S3Client = createS3Client()

  // Stream .tar.bz2 from S3, untar, and upload extracted files back to S3
  def untarFromS3(bucket: String, tarKey: String, outputPrefix: String): Future[Unit] = Future {
    val request = GetObjectRequest.builder().bucket(bucket).key(tarKey).build()
    val s3Object = s3.getObject(request)

    val inputStream = new BZip2CompressorInputStream(new BufferedInputStream(s3Object))
    val tarInputStream = new TarArchiveInputStream(inputStream)

    // Collect all entries first
    val entries = Iterator
      .continually(tarInputStream.getNextTarEntry)
      .takeWhile(_ != null)
      .toList

    // Process extraction in parallel using Future
    val extractionFutures = entries.map { entry =>
      Future {
        if (!entry.isDirectory) {
          val buffer = new Array(4096) // 4KB buffer for efficiency
          val outputKey = s"$outputPrefix/${entry.getName}"
          val outputStream = new ByteArrayOutputStream()
          var bytesRead = 0

          while ({ bytesRead = tarInputStream.read(buffer, 0, buffer.length); bytesRead != -1 }) {
            outputStream.write(buffer, 0, bytesRead)
          }

          // Upload extracted file back to S3
          uploadToS3(bucket, outputKey, outputStream.toByteArray)
          outputStream.close()
        }
      }
    }

    Future.sequence(extractionFutures).map(_ => tarInputStream.close())
  }

  // Upload extracted file to S3
  def uploadToS3(bucket: String, key: String, data: Array[Byte]): Unit = {
    val request = software.amazon.awssdk.services.s3.model.PutObjectRequest.builder()
      .bucket(bucket)
      .key(key)
      .build()

    val inputStream = new ByteArrayInputStream(data)
    s3.putObject(request, software.amazon.awssdk.core.sync.RequestBody.fromInputStream(inputStream, data.length))

    println(s"Uploaded extracted file: s3://$bucket/$key")
  }

  def main(args: Array[String]): Unit = {
    val bucketName = "your-bucket-name"
    val tarFileKey = "path/to/file.tar.bz2"
    val outputS3Prefix = "extracted-files/"

    // Start untar operation without downloading the tar file
    untarFromS3(bucketName, tarFileKey, outputS3Prefix).onComplete { _ =>
      println("Untar and upload to S3 complete!")
    }
  }
}
