import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.scalatest.funsuite.AnyFunSuite

// Dummy error DTO
case class CheckErrorDto(message: String, details: Option[String])

// Mock of your check class
object InterfaceCodeBoiteEmptinessCheck extends MetadataCheck {
  override def check(metadataDf: DataFrame): List[Option[CheckErrorDto]] = {
    val envColName = "interfaceDto.codeBoite"
    val nomTableColName = "nomTable"
    val columnToCheck = col(envColName) // dot notation for nested field

    CommonChecks.notEmptyColumnCheck(
      metadataDf,
      columnToCheck,
      nomTableColName,
      envColName,
      getCheckErrorDto
    )
  }

  def getCheckErrorDto: (String, Option[String]) => Option[CheckErrorDto] =
    (field, value) => Some(CheckErrorDto(s"$field is null or empty", value))
}

class InterfaceCodeBoiteEmptinessCheckTest extends AnyFunSuite {

  val spark = SparkSession.builder()
    .master("local[*]")
    .appName("InterfaceCodeBoiteCheckTest")
    .getOrCreate()

  import spark.implicits._

  test("detects empty or null codeBoite in nested object") {
    val df = Seq(
      ("t1", Some("BOITE1")),
      ("t2", Some("")),
      ("t3", None)
    ).toDF("nomTable", "codeBoiteValue")
      .select(
        $"nomTable",
        struct($"codeBoiteValue".as("codeBoite")).as("interfaceDto")
      )

    val result = InterfaceCodeBoiteEmptinessCheck.check(df)

    assert(result.flatten.size == 2)
    assert(result.flatten.exists(_.details.contains("t2")))
    assert(result.flatten.exists(_.details.contains("t3")))
  }

  test("returns empty when all codeBoite are valid") {
    val df = Seq(
      ("t1", Some("B1")),
      ("t2", Some("B2"))
    ).toDF("nomTable", "codeBoiteValue")
      .select(
        $"nomTable",
        struct($"codeBoiteValue".as("codeBoite")).as("interfaceDto")
      )

    val result = InterfaceCodeBoiteEmptinessCheck.check(df)
    assert(result.flatten.isEmpty)
  }
}
