import com.amazonaws.services.s3.AmazonS3
import com.amazonaws.services.s3.model._
import java.io.{InputStream, ByteArrayInputStream}
import scala.collection.mutable.ListBuffer
import scala.jdk.CollectionConverters._
import scala.util.control.NonFatal

trait Logging {
  def info(msg: String): Unit = println(s"[INFO] $msg")
  def warn(msg: String): Unit = println(s"[WARN] $msg")
  def error(msg: String): Unit = println(s"[ERROR] $msg")
}

trait S3Service {
  def read(bucket: String, key: String): S3ObjectInputStream
  def exist(bucket: String, key: String): Boolean
  def put(bucket: String, key: String, inputStream: InputStream, contentLength: Long, contentType: String): Unit
  def putMultiPart(bucket: String, key: String, inputStream: InputStream, uploadPartSize: Int): Unit
  def copy(bucketName: String, keySource: String, keyDest: String, uploadPartSize: Int = 128 * 1024 * 1024): Unit
  def delete(bucket: String, key: String): Unit
}

class DefaultS3Service(val s3Client: AmazonS3) extends S3Service with Logging {

  // Limits
  private val MinPartSize: Long = 5L * 1024 * 1024            // 5MB
  private val MaxSingleCopy: Long = 5L * 1024 * 1024 * 1024   // 5GB

  override def read(bucket: String, key: String): S3ObjectInputStream =
    s3Client.getObject(bucket, key).getObjectContent

  override def exist(bucket: String, key: String): Boolean =
    s3Client.doesObjectExist(bucket, key)

  override def put(bucket: String, key: String, inputStream: InputStream, contentLength: Long, contentType: String): Unit = {
    val meta = new ObjectMetadata()
    meta.setContentLength(contentLength)
    if (contentType != null) meta.setContentType(contentType)
    val req = new PutObjectRequest(bucket, key, inputStream, meta)
    s3Client.putObject(req)
  }

  /**
    * ✅ Robust multipart upload from InputStream
    * - NO usage of available()
    * - Read first chunk BEFORE initiating multipart => prevents 0-part complete (XML 400)
    * - Abort on failure
    * - Never completes with empty PartETags
    */
  override def putMultiPart(bucket: String, key: String, inputStream: InputStream, uploadPartSize: Int): Unit = {
    val desired = uploadPartSize.toLong
    val partSize = math.max(desired, MinPartSize).toInt
    val buffer = new Array[Byte](partSize)

    // Read first chunk before initiating multipart (critical fix)
    var bytesRead = inputStream.read(buffer)

    // Stream empty => create empty object
    if (bytesRead == -1) {
      val empty = new ByteArrayInputStream(Array.emptyByteArray)
      put(bucket, key, empty, 0L, "application/octet-stream")
      return
    }

    val initRes = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(bucket, key))
    val uploadId = initRes.getUploadId
    val partResults = ListBuffer.empty[UploadPartResult]

    var partNumber = 1

    try {
      while (bytesRead > 0) {
        val partStream = new ByteArrayInputStream(buffer, 0, bytesRead)

        val partRequest = new UploadPartRequest()
          .withBucketName(bucket)
          .withKey(key)
          .withUploadId(uploadId)
          .withPartNumber(partNumber)
          .withPartSize(bytesRead.toLong)
          .withInputStream(partStream)

        val res = s3Client.uploadPart(partRequest)
        if (res == null || res.getETag == null) {
          throw new IllegalStateException(s"Missing ETag for part=$partNumber on s3://$bucket/$key (uploadId=$uploadId)")
        }
        partResults += res

        partNumber += 1
        bytesRead = inputStream.read(buffer)
      }

      if (partResults.isEmpty) {
        // Absolute safety: never complete without parts
        throw new IllegalStateException(s"No parts uploaded for s3://$bucket/$key (uploadId=$uploadId)")
      }

      val partETags = partResults.map(r => new PartETag(r.getPartNumber, r.getETag)).asJava
      val completeReq = new CompleteMultipartUploadRequest(bucket, key, uploadId, partETags)
      s3Client.completeMultipartUpload(completeReq)

      info(s"Multipart upload completed: s3://$bucket/$key parts=${partETags.size}")

    } catch {
      case NonFatal(e) =>
        error(s"Multipart upload failed, aborting: s3://$bucket/$key uploadId=$uploadId reason=${e.getMessage}")
        try s3Client.abortMultipartUpload(new AbortMultipartUploadRequest(bucket, key, uploadId))
        catch { case NonFatal(abortErr) => warn(s"Abort failed: ${abortErr.getMessage}") }
        throw e
    } finally {
      try inputStream.close() catch { case NonFatal(_) => () }
    }
  }

  /**
    * ✅ Robust copy
    * - Uses server-side CopyObject if <= 5GB
    * - Uses Multipart Copy if > 5GB
    * - Handles empty objects
    *
    * NOTE: This avoids "read+upload" completely (better perf & cost).
    */
  override def copy(bucketName: String, keySource: String, keyDest: String, uploadPartSize: Int = 128 * 1024 * 1024): Unit = {
    val meta = s3Client.getObjectMetadata(bucketName, keySource)
    val size = meta.getContentLength

    if (size == 0L) {
      info(s"Copy empty object: s3://$bucketName/$keySource -> s3://$bucketName/$keyDest")
      val empty = new ByteArrayInputStream(Array.emptyByteArray)
      put(bucketName, keyDest, empty, 0L, Option(meta.getContentType).getOrElse("application/octet-stream"))
      return
    }

    if (size <= MaxSingleCopy) {
      info(s"CopyObject: size=$size src=s3://$bucketName/$keySource dest=s3://$bucketName/$keyDest")
      val req = new CopyObjectRequest(bucketName, keySource, bucketName, keyDest)
      s3Client.copyObject(req)
      return
    }

    multipartCopy(bucketName, keySource, bucketName, keyDest, size, uploadPartSize.toLong)
  }

  private def multipartCopy(
    srcBucket: String,
    srcKey: String,
    dstBucket: String,
    dstKey: String,
    contentLength: Long,
    desiredPartSize: Long
  ): Unit = {
    val partSize = math.max(desiredPartSize, MinPartSize)

    info(s"MultipartCopy: size=$contentLength partSize=$partSize src=s3://$srcBucket/$srcKey dest=s3://$dstBucket/$dstKey")

    val initRes = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(dstBucket, dstKey))
    val uploadId = initRes.getUploadId
    val etags = new java.util.ArrayList[PartETag]()

    try {
      var partNumber = 1
      var pos = 0L

      while (pos < contentLength) {
        val lastByte = math.min(pos + partSize - 1, contentLength - 1)

        val copyReq = new CopyPartRequest()
          .withSourceBucketName(srcBucket)
          .withSourceKey(srcKey)
          .withDestinationBucketName(dstBucket)
          .withDestinationKey(dstKey)
          .withUploadId(uploadId)
          .withFirstByte(pos)
          .withLastByte(lastByte)
          .withPartNumber(partNumber)

        val copyRes = s3Client.copyPart(copyReq)
        val partEtag = copyRes.getPartETag
        if (partEtag == null || partEtag.getETag == null) {
          throw new IllegalStateException(s"Missing ETag for copy part=$partNumber range=[$pos-$lastByte] uploadId=$uploadId")
        }

        etags.add(partEtag)

        pos = lastByte + 1
        partNumber += 1
      }

      if (etags.isEmpty) {
        throw new IllegalStateException(s"No parts copied for multipart copy uploadId=$uploadId")
      }

      s3Client.completeMultipartUpload(new CompleteMultipartUploadRequest(dstBucket, dstKey, uploadId, etags))
      info(s"MultipartCopy completed: s3://$dstBucket/$dstKey parts=${etags.size}")

    } catch {
      case NonFatal(e) =>
        error(s"MultipartCopy failed, aborting: uploadId=$uploadId reason=${e.getMessage}")
        try s3Client.abortMultipartUpload(new AbortMultipartUploadRequest(dstBucket, dstKey, uploadId))
        catch { case NonFatal(abortErr) => warn(s"Abort failed: ${abortErr.getMessage}") }
        throw e
    }
  }

  override def delete(bucket: String, key: String): Unit = {
    try s3Client.deleteObject(new DeleteObjectRequest(bucket, key))
    catch {
      case ase: AmazonServiceException =>
        error(s"Service error: ${ase.getErrorCode} - ${ase.getErrorMessage}")
        throw ase
      case NonFatal(e) =>
        error(s"Unexpected error: ${e.getMessage}")
        throw e
    }
  }
}
