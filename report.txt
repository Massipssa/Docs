import org.apache.spark.{SparkConf, SparkContext}
import org.apache.hadoop.io.compress.BZip2Codec
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}
import java.io.{ByteArrayInputStream, InputStream}
import scala.collection.mutable.ArrayBuffer

// Spark Configuration
val conf = new SparkConf().setAppName("DecompressTarBz2").setMaster("local[*]") // Change master for cluster mode
val sc = new SparkContext(conf)

// Function to Extract Tar Entries from InputStream
def extractTarEntries(inputStream: InputStream): Seq[(String, Array[Byte])] = {
  val tarInput = new TarArchiveInputStream(inputStream)
  val extractedFiles = ArrayBuffer[(String, Array[Byte])]()

  var entry: TarArchiveEntry = tarInput.getNextTarEntry
  while (entry != null) {
    if (!entry.isDirectory) {
      val fileBytes = new Array[Byte](entry.getSize.toInt)
      tarInput.read(fileBytes)
      extractedFiles.append((entry.getName, fileBytes))
    }
    entry = tarInput.getNextTarEntry
  }
  tarInput.close()
  extractedFiles
}

// Read and Decompress .tar.bz2 File from S3 or HDFS
val tarBz2RDD = sc.binaryFiles("s3://your-bucket/path/to/file.tar.bz2")

// Decompress and Extract Files
val extractedFilesRDD = tarBz2RDD.flatMap { case (path, content) =>
  val bz2InputStream = new BZip2Codec().createInputStream(content.open())
  extractTarEntries(bz2InputStream)
}

// Save Extracted Files to HDFS (Optional)
extractedFilesRDD.foreach { case (fileName, fileData) =>
  val outputPath = s"hdfs://your-hdfs-path/extracted/$fileName"
  sc.parallelize(fileData.toSeq).saveAsTextFile(outputPath) // Save files in parallel
}

println("Decompression and Extraction of .tar.bz2 completed successfully!")
