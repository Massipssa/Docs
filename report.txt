import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.amazonaws.services.s3.model._
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}

import java.io._
import scala.collection.parallel.CollectionConverters._
import scala.collection.mutable.ListBuffer
import scala.collection.JavaConverters._

object S3TarBz2ParallelUpload {

  val bucketName = "your-bucket-name"
  val inputKey = "input-folder/archive.tar.bz2"
  val outputPrefix = "extracted-folder/"
  val partSize = 100 * 1024 * 1024 // 100MB par partie

  def extractAndUploadParallel(s3Client: AmazonS3): Unit = {
    // 1ï¸âƒ£ Lire le fichier `.tar.bz2` en streaming depuis S3
    val s3Object = s3Client.getObject(new GetObjectRequest(bucketName, inputKey))
    val s3InputStream = s3Object.getObjectContent

    // 2ï¸âƒ£ DÃ©compresser le fichier en mÃ©moire
    val bz2Stream = new BZip2CompressorInputStream(s3InputStream)
    val tarInput = new TarArchiveInputStream(bz2Stream)

    var entry: TarArchiveEntry = tarInput.getNextTarEntry
    val extractedFiles = ListBuffer[(String, Array[Byte])]()

    while (entry != null) {
      if (!entry.isDirectory) {
        val fileName = entry.getName
        val s3TargetKey = outputPrefix + fileName

        println(s"Extraction: $fileName -> $s3TargetKey")

        // Lire le fichier extrait en mÃ©moire
        val outputStream = new ByteArrayOutputStream()
        val buffer = new Array[Byte](16 * 1024) // 16KB buffer
        var bytesRead = 0

        while ({ bytesRead = tarInput.read(buffer); bytesRead != -1 }) {
          outputStream.write(buffer, 0, bytesRead)
        }

        extractedFiles.append((s3TargetKey, outputStream.toByteArray))
        outputStream.close()
      }
      entry = tarInput.getNextTarEntry
    }

    // 3ï¸âƒ£ Fermer les flux
    tarInput.close()
    bz2Stream.close()
    s3InputStream.close()

    println(s"Nombre de fichiers extraits: ${extractedFiles.size}, dÃ©but de l'upload parallÃ¨le...")

    // 4ï¸âƒ£ Transformer la liste en collection parallÃ¨le et lancer les uploads en parallÃ¨le
    extractedFiles.par.foreach { case (s3Key, fileData) =>
      multipartUpload(s3Client, bucketName, s3Key, fileData)
    }

    println("âœ… Tous les fichiers ont Ã©tÃ© uploadÃ©s avec succÃ¨s ğŸš€")
  }

  def multipartUpload(s3Client: AmazonS3, bucket: String, key: String, data: Array[Byte]): Unit = {
    val uploadId = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(bucket, key)).getUploadId
    val partETags = ListBuffer[PartETag]()
    val totalSize = data.length
    var start = 0
    var partNumber = 1

    while (start < totalSize) {
      val end = Math.min(start + partSize, totalSize)
      val partData = data.slice(start, end)

      val inputStream = new ByteArrayInputStream(partData)
      val uploadPartRequest = new UploadPartRequest()
        .withBucketName(bucket)
        .withKey(key)
        .withUploadId(uploadId)
        .withPartNumber(partNumber)
        .withInputStream(inputStream)
        .withPartSize(partData.length)

      val partResult = s3Client.uploadPart(uploadPartRequest)
      partETags.append(partResult.getPartETag)

      println(s"Partie $partNumber uploadÃ©e : ${partData.length} bytes")
      start = end
      partNumber += 1
    }

    // âœ… Convertir Scala ListBuffer en Java List (compatible Scala 2.12)
    s3Client.completeMultipartUpload(new CompleteMultipartUploadRequest(bucket, key, uploadId, partETags.asJava))
    println(s"âœ… Upload terminÃ© pour $key")
  }

  def main(args: Array[String]): Unit = {
    val s3Client = AmazonS3ClientBuilder.standard().build()
    extractAndUploadParallel(s3Client)
  }
}
