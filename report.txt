import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.scalatest.funsuite.AnyFunSuite

class NotEmptyColumnCheckTest extends AnyFunSuite {

  val spark = SparkSession.builder()
    .master("local[*]")
    .appName("NotEmptyColumnCheckTest")
    .getOrCreate()

  import spark.implicits._

  def getCheckErrorDto(field: String, value: Option[String]): Option[CheckErrorDto] =
    Some(CheckErrorDto(s"$field is null or empty", value))

  def notEmptyColumnCheck(
      df: DataFrame,
      columnToCheck: Column,
      keyColumn: String,
      valueColumn: String,
      getCheckErrorDto: (String, Option[String]) => Option[CheckErrorDto]
  ): List[Option[CheckErrorDto]] = {
    val resultDf = df.filter(columnToCheck === "" || columnToCheck.isNull)
    if (!resultDf.isEmpty) {
      val emptyColsValues = SparkDfUtil.listColumnAsListStr(resultDf, keyColumn)
      emptyColsValues.map(p => getCheckErrorDto(s"${valueColumn.capitalize} is null or empty", Option(p)))
    } else {
      List.empty
    }
  }

  test("should return error for null or empty values") {
    val df = Seq(
      ("1", "abc"),
      ("2", null),
      ("3", ""),
      ("4", "xyz")
    ).toDF("id", "name")

    val result = notEmptyColumnCheck(
      df,
      col("name"),
      "id",
      "name",
      getCheckErrorDto
    )

    assert(result.flatten.size == 2)
    assert(result.flatten.exists(_.details.contains("2")))
    assert(result.flatten.exists(_.details.contains("3")))
  }

  test("should return empty list when no null or empty") {
    val df = Seq(
      ("1", "abc"),
      ("2", "def")
    ).toDF("id", "name")

    val result = notEmptyColumnCheck(
      df,
      col("name"),
      "id",
      "name",
      getCheckErrorDto
    )

    assert(result.flatten.isEmpty)
  }
}
