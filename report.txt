src/main/scala/fr/ccf/job/ingest/
  IngestRunner.scala
  IngestService.scala
  ReportReader.scala
  TableInfoService.scala
  DataFrameLoader.scala
  DataFrameTransformer.scala
  TableWriter.scala
  model.scala

src/test/scala/fr/ccf/job/ingest/
  SparkTestSession.scala
  ReportReaderSpec.scala
  DataFrameTransformerSpec.scala
  IngestServiceSpec.scala

package fr.ccf.job.ingest

final case class JobConf(rzBucket: String)
final case class DbConf(/* ... */)

final case class TableInfo(tableName: String, keyInRz: String)

final case class IngestionConfig(
  formatVersion: String = "2",
  targetFileSizeBytes: Long = 536870912L, // 512MB
  castColumnsToString: Seq[String] = Seq("year", "month", "day")
)

final case class IngestResult(success: List[String], failed: Map[String, String])


package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions.col

trait ReportReader {
  def listIngestedKeys(reportPath: String): List[String]
}

final class SparkCsvReportReader(
  implicit spark: SparkSession,
  keyCol: String = "key_in_raw_zone",
  statusCol: String = "status",
  ingestedValue: String = "Ingested",
  delimiter: String = ",",
  header: Boolean = true
) extends ReportReader {

  override def listIngestedKeys(reportPath: String): List[String] = {
    import spark.implicits._

    spark.read
      .option("header", header.toString)
      .option("delimiter", delimiter)
      .csv(reportPath)
      .select(col(keyCol).as("key"), col(statusCol).as("status"))
      .filter(col("status") === ingestedValue)
      .select("key")
      .as[String]
      .collect()
      .toList
  }
}

package fr.ccf.job.ingest

trait TableInfoService {
  def extractTableInfo(rzBucket: String, key: String): Option[TableInfo]
}

// Adapter vers ton code actuel
final class DefaultTableInfoService extends TableInfoService {
  override def extractTableInfo(rzBucket: String, key: String): Option[TableInfo] =
    ObjectUtil.extractTableInfo(rzBucket, key) // <-- ton existant
}


package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame, SparkSession}

trait DataFrameLoader {
  def loadCsv(key: String): DataFrame
}

final class SparkUtilDataFrameLoader(implicit spark: SparkSession) extends DataFrameLoader {
  override def loadCsv(key: String): DataFrame =
    SparkUtil.readCsvAsDf(key) // <-- ton existant
}

package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame}
import org.apache.spark.sql.functions.col

trait DataFrameTransformer {
  def prepare(df: DataFrame): DataFrame
}

final class CastColumnsToStringTransformer(cols: Seq[String]) extends DataFrameTransformer {
  override def prepare(df: DataFrame): DataFrame =
    cols.foldLeft(df) { (acc, c) =>
      if (acc.columns.contains(c)) acc.withColumn(c, col(c).cast("string")) else acc
    }
}


package fr.ccf.job.ingest

import org.apache.spark.sql.DataFrame

trait TableWriter {
  def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit
}

final class IcebergTableWriter extends TableWriter {
  override def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit =
    TableCreator.createOrAppendTable(
      dbConf = dbConf,
      tableName = tableName,
      df = df,
      tableProperties = tableProperties
    )
}

package fr.ccf.job.ingest

import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession

final class IngestService(
  jobConf: JobConf,
  dbConf: DbConf,
  reportReader: ReportReader,
  tableInfoService: TableInfoService,
  loader: DataFrameLoader,
  transformer: DataFrameTransformer,
  writer: TableWriter,
  config: IngestionConfig
)(implicit spark: SparkSession) extends Logging {

  private val tableProperties: Map[String, String] = Map(
    "format-version" -> config.formatVersion,
    "write.target-file-size-bytes" -> config.targetFileSizeBytes.toString
  )

  def ingestFromReport(reportPath: String): IngestResult = {
    val keys = reportReader.listIngestedKeys(reportPath)

    val (ok, ko) = keys.foldLeft((List.empty[String], Map.empty[String, String])) {
      case ((successAcc, failedAcc), key) =>
        ingestOneKey(key) match {
          case Right(_)  => (key :: successAcc, failedAcc)
          case Left(err) => (successAcc, failedAcc + (key -> err))
        }
    }

    IngestResult(ok.reverse, ko)
  }

  def ingestOneKey(key: String): Either[String, Unit] = {
    tableInfoService.extractTableInfo(jobConf.rzBucket, key) match {
      case None =>
        val msg = s"TableInfo introuvable pour key=$key"
        logWarning(msg)
        Left(msg)

      case Some(ti) =>
        try {
          logInfo(s"Ingesting table=${ti.tableName}, key=${ti.keyInRz}")
          val rawDf = loader.loadCsv(ti.keyInRz)
          val finalDf = transformer.prepare(rawDf)

          writer.createOrAppend(dbConf, ti.tableName, finalDf, tableProperties)
          Right(())
        } catch {
          case e: Throwable =>
            val msg = s"Echec ingestion key=$key : ${e.getMessage}"
            logError(msg, e)
            Left(msg)
        }
    }
  }
}

package fr.ccf.job.ingest

import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession

object IngestRunner extends Logging {

  def run(jobConf: JobConf, dbConf: DbConf, reportPath: String)(implicit spark: SparkSession): IngestResult = {
    val config = IngestionConfig()

    val service = new IngestService(
      jobConf = jobConf,
      dbConf = dbConf,
      reportReader = new SparkCsvReportReader(),
      tableInfoService = new DefaultTableInfoService(),
      loader = new SparkUtilDataFrameLoader(),
      transformer = new CastColumnsToStringTransformer(config.castColumnsToString),
      writer = new IcebergTableWriter(),
      config = config
    )

    service.ingestFromReport(reportPath)
  }
}


package fr.ccf.job.ingest

import org.apache.spark.sql.SparkSession
import org.scalatest.BeforeAndAfterAll
import org.scalatest.funsuite.AnyFunSuite

trait SparkTestSession extends AnyFunSuite with BeforeAndAfterAll {
  implicit var spark: SparkSession = _

  override def beforeAll(): Unit = {
    spark = SparkSession.builder()
      .appName("ingest-tests")
      .master("local[2]")
      .config("spark.ui.enabled", "false")
      .getOrCreate()
  }

  override def afterAll(): Unit = {
    if (spark != null) spark.stop()
  }
}


package fr.ccf.job.ingest

import java.nio.file.Files

final class ReportReaderSpec extends SparkTestSession {

  test("listIngestedKeys retourne uniquement les keys en status=Ingested") {
    val dir = Files.createTempDirectory("report-test")
    val report = dir.resolve("report.csv")

    val content =
      """key_in_raw_zone,status
        |a.csv,Ingested
        |b.csv,Error
        |c.csv,Ingested
        |""".stripMargin

    Files.write(report, content.getBytes("UTF-8"))

    val reader = new SparkCsvReportReader()
    val keys = reader.listIngestedKeys(report.toString)

    assert(keys.sorted == List("a.csv", "c.csv"))
  }
}

package fr.ccf.job.ingest

import org.apache.spark.sql.types.StringType

final class DataFrameTransformerSpec extends SparkTestSession {

  test("CastColumnsToStringTransformer caste seulement les colonnes existantes") {
    import spark.implicits._

    val df = Seq((2025, 12, 18, "x")).toDF("year", "month", "day", "payload")

    val transformer = new CastColumnsToStringTransformer(Seq("year", "month", "day", "missing_col"))
    val out = transformer.prepare(df)

    assert(out.schema("year").dataType == StringType)
    assert(out.schema("month").dataType == StringType)
    assert(out.schema("day").dataType == StringType)
    assert(out.schema("payload").dataType != StringType) // payload reste tel quel
  }
}

package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame}

final class IngestServiceSpec extends SparkTestSession {

  test("ingestOneKey appelle le writer avec les bonnes propriétés") {
    val jobConf = JobConf(rzBucket = "rz-bucket")
    val dbConf = DbConf()

    val reportReader = new ReportReader {
      override def listIngestedKeys(reportPath: String): List[String] = List("k1")
    }

    val tableInfoService = new TableInfoService {
      override def extractTableInfo(rzBucket: String, key: String): Option[TableInfo] =
        Some(TableInfo("my_table", "s3://rz/path/file.csv"))
    }

    val loader = new DataFrameLoader {
      override def loadCsv(key: String): DataFrame = {
        import spark.implicits._
        Seq((1, 2, 3)).toDF("year", "month", "day")
      }
    }

    val transformer = new CastColumnsToStringTransformer(Seq("year", "month", "day"))

    final class CapturingWriter extends TableWriter {
      var called = false
      var props: Map[String, String] = Map.empty
      override def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit = {
        called = true
        props = tableProperties
        assert(tableName == "my_table")
      }
    }

    val writer = new CapturingWriter
    val config = IngestionConfig()

    val service = new IngestService(
      jobConf, dbConf, reportReader, tableInfoService, loader, transformer, writer, config
    )

    val res = service.ingestOneKey("k1")
    assert(res.isRight)
    assert(writer.called)
    assert(writer.props("format-version") == "2")
    assert(writer.props("write.target-file-size-bytes") == "536870912")
  }
}
-------------------------------------------

package fr.ccf.job.app

import fr.ccf.job.ingest.{DbConf, IngestRunner, JobConf}
import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession

object App extends Logging {

  def main(sysArgs: Array[String]): Unit = {
    val argsMap = GlueArgsResolver.resolve(sysArgs)

    val config = AppConfigLoader.fromArgs(argsMap) match {
      case Left(err) =>
        logError(err.message)
        System.exit(2)
        return
      case Right(c) => c
    }

    implicit val spark: SparkSession =
      SessionCreator.createSparkSession(
        config.jobConf.appName,
        config.warehousePath,
        config.dbConf.catalog,
        isLocal = config.isLocal
      )

    try {
      val objectsToIngest: List[String] =
        config.objectsToIngest.getOrElse {
          // fallback report obligatoire (sinon on ingère rien => erreur explicite)
          val reportPath = config.reportPath.getOrElse {
            throw new IllegalArgumentException(
              "Ni --objects-to-ingest ni --report-path fournis. Impossible de déterminer quoi ingérer."
            )
          }
          IngestRunner.listObjectFromRZ(reportPath) // ta méthode existante (ou à ajouter)
        }

      IngestRunner.run(config.jobConf, config.dbConf, objectsToIngest)
    } finally {
      spark.stop()
    }
  }
}

package fr.ccf.job.app

object GlueArgsResolver {

  private val Required: Array[String] =
    Array("context", "app-name", "rz-bucket", "sz-bucket", "glue-db-name")

  private val Optional: Seq[String] =
    Seq("objects-to-ingest", "report-path", "is-local")

  def resolve(sysArgs: Array[String]): Map[String, String] = {
    val optionalFound = Optional.filter(k => sysArgs.contains(s"--$k"))
    val keys = Required ++ optionalFound

    val raw = GlueArgParser.getResolvedOptions(sysArgs, keys) // ton util existant

    // Normalisation: on garde une seule forme interne en snake_case
    raw.map { case (k, v) => normalizeKey(k) -> v }.toMap
  }

  private def normalizeKey(k: String): String =
    k.trim.toLowerCase.replace("-", "_")
}


package fr.ccf.job.app

import fr.ccf.job.ingest.{DbConf, JobConf}

final case class AppConfig(
  jobConf: JobConf,
  dbConf: DbConf,
  isLocal: Boolean,
  warehousePath: String,
  objectsToIngest: Option[List[String]],
  reportPath: Option[String]
)

final case class ConfigError(message: String)




package fr.ccf.job.app

import fr.ccf.job.ingest.{DbConf, JobConf}

object AppConfigLoader {

  def fromArgs(args: Map[String, String]): Either[ConfigError, AppConfig] = {
    def required(key: String): Either[ConfigError, String] =
      args.get(key).map(_.trim).filter(_.nonEmpty)
        .toRight(ConfigError(s"Argument requis manquant: --${key.replace("_", "-")}"))

    def optional(key: String): Option[String] =
      args.get(key).map(_.trim).filter(_.nonEmpty)

    def optionalBoolean(key: String, default: Boolean = false): Boolean =
      args.get(key).exists(v => v.trim.equalsIgnoreCase("true") || v.trim == "1")

    val contextE   = required("context")
    val appNameE   = required("app_name")
    val rzBucketE  = required("rz_bucket")
    val szBucketE  = required("sz_bucket")
    val glueDbE    = required("glue_db_name")

    for {
      context  <- contextE
      appName  <- appNameE
      rzBucket <- rzBucketE
      szBucket <- szBucketE
      glueDb   <- glueDbE
    } yield {
      val jobConf = JobConf(
        context = context,
        appName = appName,
        rzBucket = rzBucket,
        szBucket = szBucket
      )

      val isLocal = optionalBoolean("is_local", default = false)
      val dbConf = DbConf("glue", glueDb)

      val warehousePath =
        WarehousePathBuilder.build(szBucket = jobConf.szBucket, appName = jobConf.appName)

      val objectsToIngest = optional("objects_to_ingest").map(ObjectsParser.parse)

      AppConfig(
        jobConf = jobConf,
        dbConf = dbConf,
        isLocal = isLocal,
        warehousePath = warehousePath,
        objectsToIngest = objectsToIngest.filter(_.nonEmpty),
        reportPath = optional("report_path")
      )
    }
  }
}


package fr.ccf.job.app

object ObjectsParser {
  /** Supporte: "a|b|c" ou "a,b,c" ou "a; b ; c" */
  def parse(raw: String): List[String] = {
    raw
      .split("[|,;]")
      .map(_.trim)
      .filter(_.nonEmpty)
      .toList
  }
}


package fr.ccf.job.app

object WarehousePathBuilder {
  def build(szBucket: String, appName: String): String =
    s"s3a://$szBucket/type=projected/daap/$appName/warehouse/"
}



package fr.ccf.job.app

import org.scalatest.funsuite.AnyFunSuite

final class ObjectsParserSpec extends AnyFunSuite {

  test("parse supporte pipe") {
    assert(ObjectsParser.parse("a|b|c") == List("a","b","c"))
  }

  test("parse supporte virgule et trim") {
    assert(ObjectsParser.parse(" a,  b ,c ") == List("a","b","c"))
  }

  test("parse ignore vides") {
    assert(ObjectsParser.parse("a|| |b; ;c") == List("a","b","c"))
  }
}


package fr.ccf.job.app

import org.scalatest.funsuite.AnyFunSuite

final class AppConfigLoaderSpec extends AnyFunSuite {

  test("fromArgs construit une config valide + warehouse + isLocal") {
    val args = Map(
      "context" -> "dev",
      "app_name" -> "ingestion",
      "rz_bucket" -> "rz",
      "sz_bucket" -> "sz",
      "glue_db_name" -> "db",
      "is_local" -> "true",
      "objects_to_ingest" -> "o1|o2"
    )

    val cfg = AppConfigLoader.fromArgs(args).toOption.get
    assert(cfg.isLocal)
    assert(cfg.warehousePath.contains("s3a://sz"))
    assert(cfg.objectsToIngest.contains(List("o1", "o2")))
  }

  test("fromArgs retourne une erreur si args requis manquants") {
    val args = Map("context" -> "dev")
    val err = AppConfigLoader.fromArgs(args).left.toOption.get
    assert(err.message.contains("Argument requis manquant"))
  }
}
