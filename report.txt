import org.scalatest.funsuite.AnyFunSuite
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

class DataRejectionServiceTest extends AnyFunSuite {

  val spark: SparkSession = SparkSession.builder()
    .appName("UnitTest")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  test("getDataRejectionsFromReport should return correct rejections map") {
    // Sample data that simulates the CSV
    val schema = StructType(List(
      StructField("CRE_ID", StringType, nullable = true),
      StructField("SEVERITY", StringType, nullable = true)
    ))

    val testData = Seq(
      Row("CRE_001", "FATAL"),
      Row("CRE_002", "INFO"),
      Row("CRE_001", "WARNING"),
      Row("CRE_003", "FATAL")
    )

    val df = spark.createDataFrame(spark.sparkContext.parallelize(testData), schema)

    // Mock SparkUtility
    object MockSparkUtility {
      def loadCsvFromS3(reportKey: String, addRowIdColumn: Boolean, delimiter: String): DataFrame = df
    }

    // Mock isCreRejected logic
    def isCreRejected(fatalDf: DataFrame, key: String): Boolean = {
      fatalDf.filter($"CRE_ID" === key).count() > 0
    }

    // Method under test
    def getDataRejectionsFromReport(dataKeys: List[String], reportKey: String): Map[String, Boolean] = {
      val allErrorsDf = MockSparkUtility.loadCsvFromS3(reportKey, false, ";")
      val fatalErrorsDf = allErrorsDf.filter($"SEVERITY" === "FATAL")

      dataKeys.map(key => key -> isCreRejected(fatalErrorsDf, key)).toMap
    }

    // Run test
    val result = getDataRejectionsFromReport(List("CRE_001", "CRE_002", "CRE_003", "CRE_004"), "mockKey")

    assert(result == Map(
      "CRE_001" -> true,
      "CRE_002" -> false,
      "CRE_003" -> true,
      "CRE_004" -> false
    ))
  }
}
