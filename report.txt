import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("Compare with First Row")
  .master("local[*]")  // Remplace par le mode cluster si nécessaire
  .getOrCreate()

import spark.implicits._

// 📌 1️⃣ Création du DataFrame avec des colonnes _c0 à _c6
val df = Seq(
  ("A", "X", "1", "P", "Red", "100", "Yes"),  // 🔹 Référence (1ère ligne)
  ("A", "X", "2", "Q", "Blue", "200", "No"),  // ❌ Différences dans _c2, _c3, _c4, _c5, _c6
  ("A", "Y", "1", "P", "Red", "100", "Yes"),  // ❌ Différence dans _c1
  ("C", "Z", "3", "P", "Red", "100", "Yes"),  // ❌ Différences dans _c0, _c1, _c2
  ("A", "X", "1", "P", "Red", "100", "Yes")   // ✅ Identique à la 1ère ligne
).toDF("_c0", "_c1", "_c2", "_c3", "_c4", "_c5", "_c6")

// 📌 2️⃣ Appliquer `first()` sur chaque colonne pour récupérer la première valeur
val firstValuesExprs = df.columns.map(c => first(col(c)).over() as s"${c}_first")

// 📌 3️⃣ Sélectionner toutes les colonnes originales + leurs valeurs de référence
val dfWithFirstValues = df.select(
  df.columns.map(col) ++ firstValuesExprs: _*
)

// 📌 4️⃣ Comparer chaque colonne avec sa première valeur
val diffCondition = df.columns.map(c => col(c) =!= col(s"${c}_first")).reduce(_ || _)

// 📌 5️⃣ Filtrer uniquement les lignes différentes de la première
val diffDf = dfWithFirstValues.filter(diffCondition).select(df.columns.map(col): _*)

// 📌 6️⃣ Afficher les lignes qui diffèrent de la première ligne
diffDf.show(false)
