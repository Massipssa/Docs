from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import boto3

class S3HookWithSTS(S3Hook):
    """
    Custom S3 Hook that assumes an IAM role using STS before accessing S3.
    
    :param role_arn: IAM Role ARN to assume
    :param region_name: AWS Region
    """

    def __init__(self, role_arn: str, region_name: str = "us-east-1", *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.role_arn = role_arn
        self.region_name = region_name

    def _get_credentials(self):
        """Override the default method to use STS for assuming a role."""
        self.log.info(f"Assuming IAM role: {self.role_arn}")

        sts_client = boto3.client("sts", region_name=self.region_name)
        assumed_role = sts_client.assume_role(
            RoleArn=self.role_arn,
            RoleSessionName="AirflowS3HookSession"
        )

        credentials = assumed_role["Credentials"]
        return {
            "aws_access_key_id": credentials["AccessKeyId"],
            "aws_secret_access_key": credentials["SecretAccessKey"],
            "aws_session_token": credentials["SessionToken"],
        }

    def get_s3_client(self):
        """Returns an S3 client using the assumed role credentials."""
        credentials = self._get_credentials()

        return boto3.client(
            "s3",
            aws_access_key_id=credentials["aws_access_key_id"],
            aws_secret_access_key=credentials["aws_secret_access_key"],
            aws_session_token=credentials["aws_session_token"],
            region_name=self.region_name,
        )

from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from custom_hooks.S3HookWithSTS import S3HookWithSTS

class S3KeySensorWithSTS(S3KeySensor):
    """
    Custom S3 Sensor that uses an STS-enabled S3 Hook to check for a file in S3.
    
    :param role_arn: IAM Role ARN to assume
    :param aws_region: AWS region
    """

    def __init__(self, role_arn, aws_region="us-east-1", *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.role_arn = role_arn
        self.aws_region = aws_region

    @cached_property
    def hook(self) -> S3HookWithSTS:
        """Overrides the default hook to use S3HookWithSTS instead of S3Hook."""
        return S3HookWithSTS(role_arn=self.role_arn, region_name=self.aws_region)


from airflow import DAG
from datetime import datetime, timedelta
from custom_sensors.S3KeySensorWithSTS import S3KeySensorWithSTS

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 2, 8),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    's3_sensor_with_sts',
    default_args=default_args,
    description='DAG using S3 Sensor with STS authentication',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

s3_sensor_task = S3KeySensorWithSTS(
    task_id='check_s3_file',
    bucket_name='my-s3-bucket',
    bucket_key='data/file.csv',
    role_arn='arn:aws:iam::123456789012:role/MyAirflowRole',
    aws_region='us-east-1',
    poke_interval=60,  # Check every 60 seconds
    timeout=600,  # Timeout after 10 minutes
    dag=dag,
)
