import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

case class StructureInterfaceDto(
  famille: String,
  identifiant: String,
  libelleLong: String,
  nomDonnee: String,
  obligatoire: String,
  propagation: String,
  definition: String,
  position: Int
)

case class InterfaceDto(
  codeEvenement: String,
  codeBoite: String,
  libelleLong: String,
  dateMaj: String,
  profondeur: String,
  description: String,
  structureInterface: Seq[StructureInterfaceDto] // Ensure it's a Seq/List
)

case class MetadataLineDto(
  creId: String,
  env: String,
  interfaceDto: InterfaceDto
)

object MetadataParser {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("MetadataLineDto Parser")
      .master("local[*]") // Run locally
      .getOrCreate()

    import spark.implicits._

    // Load the text file using read.text()
    val rawData = spark.read.text("path/to/your/file.txt")

    // Split each line into three parts: creId, env, JSON Data
    val jsonData = rawData.withColumn("splitData", split(col("value"), ",", 3))
      .select(
        col("splitData").getItem(0).alias("creId"),
        col("splitData").getItem(1).alias("env"),
        col("splitData").getItem(2).alias("jsonData")
      )

    // Define schema for InterfaceDto
    val interfaceDtoSchema = Encoders.product[InterfaceDto].schema

    // ✅ Convert jsonData (String) into StructType (InterfaceDto)
    val parsedDF = jsonData.withColumn(
      "interfaceDto",
      from_json(col("jsonData"), interfaceDtoSchema) // Ensures correct structure
    ).drop("jsonData") // Drop the raw JSON string after conversion

    // ✅ Convert DataFrame to Dataset[MetadataLineDto]
    val metadataDataset = parsedDF.as[MetadataLineDto]

    // Show parsed data
    metadataDataset.show(truncate = false)

    // Stop Spark session
    spark.stop()
  }
}
