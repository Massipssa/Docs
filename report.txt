import com.amazonaws.services.s3.AmazonS3
import com.amazonaws.services.s3.model._
import com.fasterxml.jackson.databind.{ObjectMapper, SerializationFeature}
import com.fasterxml.jackson.module.scala.{DefaultScalaModule, ScalaObjectMapper}

import java.io.{ByteArrayInputStream, InputStream}
import java.nio.charset.StandardCharsets
import java.time.Instant
import scala.jdk.CollectionConverters._

// =====================================================
// 1) DOMAIN (BatchStatus = Stage + Artifact)
// =====================================================

sealed trait BatchStatus { def order: Int }
object BatchStatus {
  case object DECRYPTED         extends BatchStatus { val order = 0 }
  case object DECOMPRESSED      extends BatchStatus { val order = 5 }
  case object DATA_CHECK        extends BatchStatus { val order = 10 }
  case object GESPARAM_TRANSFORM extends BatchStatus { val order = 15 }
  case object DATA_TRANSFORM    extends BatchStatus { val order = 20 }
  case object DATA_TRANSFER     extends BatchStatus { val order = 30 }
  case object DONE              extends BatchStatus { val order = 100 }
  case object FAILED            extends BatchStatus { val order = -1 }
}

sealed trait ArtifactType
object ArtifactType {
  case object DATA        extends ArtifactType          // CRE
  case object PARAMETRAGE extends ArtifactType          // fichier source paramétrage (1 seul)
  case object GESPARAM    extends ArtifactType          // artifacts dérivés du paramétrage
  case object METADATA    extends ArtifactType          // sans état
  case object MONITORING  extends ArtifactType          // sans état
  case object AUTRES      extends ArtifactType          // avec état
  case object UNKNOWN     extends ArtifactType          // ignoré
}

sealed trait ArtifactStatus
object ArtifactStatus {
  case object IN_PROGRESS extends ArtifactStatus
  case object DONE        extends ArtifactStatus
  case object FAILED      extends ArtifactStatus
  case object IGNORED     extends ArtifactStatus
}

// CRE: kind + frequency depuis référentiel
sealed trait CreKind
object CreKind { case object FLUX extends CreKind; case object STOCK extends CreKind }

sealed trait CreFrequency
object CreFrequency { case object DAILY extends CreFrequency; case object WEEKLY extends CreFrequency; case object MONTHLY extends CreFrequency }

final case class CreDefinition(creCode: String, kind: CreKind, frequency: CreFrequency)

final case class BatchPaths(
  bucket: String,
  fromFlowPrefix: String,       // group=.../zone=from_flow/...
  tarredPrefix: String,         // group=.../zone=tarred/batch=XXX/
  untarredPrefix: String,       // group=.../zone=untarred/batch=XXX/
  transferablePrefix: String,   // group=.../zone=transferable/batch=XXX/
  statePrefix: String           // group=.../zone=state/batch=XXX/
)

final case class Artifact(
  name: String,
  artifactType: ArtifactType,
  relativePath: String,             // ex: "DATA/file.csv" ou "METADATA/meta.json" (relatif à untarredPrefix)
  status: Option[ArtifactStatus],   // None => fichiers sans état (metadata/monitoring)
  error: Option[String] = None,

  // enrichissement CRE
  creCode: Option[String] = None,
  creKind: Option[CreKind] = None,
  creFrequency: Option[CreFrequency] = None
)

final case class Batch(
  batchId: String,
  source: String,
  status: BatchStatus,       // ✅ sert de stage
  createdAt: String,
  updatedAt: String,
  paths: BatchPaths,

  // CRE (DATA)
  cre: List[Artifact],

  // Paramétrage: 1 fichier source + liste GESPARAM dérivée
  parametrageFile: Option[Artifact], // ArtifactType.PARAMETRAGE (avec état)
  gesparam: List[Artifact],          // ArtifactType.GESPARAM (avec état)

  // Autres types
  autres: List[Artifact],            // avec état
  metadata: List[Artifact],          // sans état
  monitoring: List[Artifact]         // sans état
)

object Time { def nowIso: String = Instant.now().toString }

// =====================================================
// 2) PATH RESOLVER (anti-duplication)
// =====================================================

object PathResolver {
  def stateKey(batch: Batch): String =
    s"${batch.paths.statePrefix}state.json"

  def untarredKey(batch: Batch, a: Artifact): String =
    s"${batch.paths.untarredPrefix}${a.relativePath}"

  def transferableKey(batch: Batch, fileName: String): String =
    s"${batch.paths.transferablePrefix}$fileName"
}

// =====================================================
// 3) S3 + JSON (AWS SDK v1)
// =====================================================

trait S3Service {
  def listKeys(bucket: String, prefix: String): List[String]
  def read(bucket: String, key: String): S3ObjectInputStream
  def putJson(bucket: String, key: String, json: String): Unit
}

final class DefaultS3Service(s3: AmazonS3) extends S3Service {

  override def listKeys(bucket: String, prefix: String): List[String] = {
    var token: String = null
    val out = scala.collection.mutable.ListBuffer.empty[String]

    do {
      val req = new ListObjectsV2Request()
        .withBucketName(bucket)
        .withPrefix(prefix)
        .withContinuationToken(token)

      val res = s3.listObjectsV2(req)
      res.getObjectSummaries.asScala
        .map(_.getKey)
        .filterNot(_.endsWith("/"))
        .foreach(out += _)

      token = res.getNextContinuationToken
    } while (token != null)

    out.toList
  }

  override def read(bucket: String, key: String): S3ObjectInputStream =
    s3.getObject(bucket, key).getObjectContent

  override def putJson(bucket: String, key: String, json: String): Unit = {
    val bytes = json.getBytes(StandardCharsets.UTF_8)
    val meta = new ObjectMetadata()
    meta.setContentType("application/json")
    meta.setContentLength(bytes.length.toLong)
    s3.putObject(bucket, key, new ByteArrayInputStream(bytes), meta)
  }
}

final class JsonCodec {
  private val mapper = new ObjectMapper() with ScalaObjectMapper
  mapper.registerModule(DefaultScalaModule)
  mapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS)

  def toJson[A](a: A): String =
    mapper.writerWithDefaultPrettyPrinter().writeValueAsString(a)

  def fromJson[A: Manifest](is: InputStream): A =
    mapper.readValue[A](is)
}

trait StateStore {
  def load(bucket: String, stateKey: String): Batch
  def save(bucket: String, stateKey: String, batch: Batch): Unit
}

final class S3StateStore(s3: S3Service, json: JsonCodec) extends StateStore {
  override def load(bucket: String, stateKey: String): Batch = {
    val is = s3.read(bucket, stateKey)
    try json.fromJson[Batch](is)
    finally is.close()
  }

  override def save(bucket: String, stateKey: String, batch: Batch): Unit = {
    val content = json.toJson(batch)
    s3.putJson(bucket, stateKey, content)
  }
}

// =====================================================
// 4) HELPERS: mapping folder->ArtifactType + update ciblé
// =====================================================

object ArtifactTyping {

  // pattern match demandé
  def artifactTypeFromKey(key: String): ArtifactType = {
    key.toUpperCase match {
      case k if k.contains("/DATA/")        => ArtifactType.DATA
      case k if k.contains("/PARAMETRAGE/") => ArtifactType.PARAMETRAGE
      case k if k.contains("/METADATA/")    => ArtifactType.METADATA
      case k if k.contains("/MONITORING/")  => ArtifactType.MONITORING
      case k if k.contains("/AUTRES/")      => ArtifactType.AUTRES
      case _                                => ArtifactType.UNKNOWN
    }
  }

  def fileName(key: String): String =
    key.split("/").lastOption.getOrElse(key)

  /** key complet -> relativePath (relatif au untarredPrefix) */
  def toRelative(untarredPrefixKey: String, fullKey: String): String =
    fullKey.stripPrefix(untarredPrefixKey)
}

object BatchUpdater {

  private def patch(list: List[Artifact], pred: Artifact => Boolean, f: Artifact => Artifact): List[Artifact] =
    list.map(a => if (pred(a)) f(a) else a)

  /** met à jour uniquement les artifacts concernés (par name + type) dans les collections adéquates */
  def updateArtifact(batch: Batch, tpe: ArtifactType, name: String)(f: Artifact => Artifact): Batch = {
    def same(a: Artifact) = a.artifactType == tpe && a.name == name && a.status.isDefined

    batch.copy(
      cre = patch(batch.cre, same, f),
      gesparam = patch(batch.gesparam, same, f),
      autres = patch(batch.autres, same, f),
      parametrageFile = batch.parametrageFile.map(p => if (same(p)) f(p) else p),
      updatedAt = Time.nowIso
    )
  }
}

// =====================================================
// 5) REFERENTIAL + CRE NAME RULES
// =====================================================

trait CreReferential {
  def get(creCode: String): Option[CreDefinition]
}

object CreNameRules {
  // Exemple : CRE_<CODE>_<YYYYMMDD>.csv (à adapter)
  private val Pattern = """^CRE_([A-Z0-9_]+)_[0-9]{8}\.(csv|dat|txt|gz)$""".r

  def extractCode(fileName: String): Option[String] =
    fileName.toUpperCase match {
      case Pattern(code, _) => Some(code)
      case _ => None
    }
}

// =====================================================
// 6) STEPS + RUNNER
// =====================================================

trait IngestionStep {
  def name: String
  def expectedStatus: BatchStatus
  def nextStatus: BatchStatus
  def process(batch: Batch): Batch
}

final class IngestionRunner(state: StateStore, s3Bucket: String) {

  /** Sécurité + idempotence */
  def run(step: IngestionStep, stateKey: String): Batch = {
    val current = state.load(s3Bucket, stateKey)

    // trop tôt
    if (current.status.order < step.expectedStatus.order)
      throw new IllegalStateException(
        s"Cannot run step=${step.name}. batchStatus=${current.status}, expected=${step.expectedStatus}"
      )

    // déjà passé (idempotent)
    if (current.status.order > step.expectedStatus.order)
      return current

    val next = step.process(current)
    state.save(s3Bucket, stateKey, next)
    next
  }
}

// ---------------------------
// Step 1: DECRYPT
// Lit from_flow -> écrit tarred
// Crée le state minimal à la fin
// ---------------------------
final class DecryptStep(
  buildInitialBatch: () => Batch
) extends IngestionStep {

  override val name = "DECRYPT"
  override val expectedStatus = BatchStatus.DECRYPTED        // première exécution: on part d'un batch "virtuel"
  override val nextStatus = BatchStatus.DECRYPTED

  override def process(batch: Batch): Batch = batch // non utilisé ici

  /** Variante pratique: exécuter decrypt et retourner le batch minimal à écrire */
  def runDecryptAndCreateState(): Batch = {
    // Ici tu fais ta vraie logique: lire from_flow, écrire en tarred
    // (IO réel à implémenter dans ton code)
    buildInitialBatch().copy(status = BatchStatus.DECRYPTED, updatedAt = Time.nowIso)
  }
}

// ---------------------------
// Step 2: DECOMPRESS
// Lit tarred -> écrit untarred
// Puis construit artifacts en listant untarred, et enrichit le state
// ---------------------------
final class DecompressStep(
  s3: S3Service
) extends IngestionStep {

  override val name = "DECOMPRESS"
  override val expectedStatus = BatchStatus.DECRYPTED
  override val nextStatus = BatchStatus.DECOMPRESSED

  override def process(batch: Batch): Batch = {

    // Ici tu fais ta logique réelle: lire tarred et écrire untarred
    // (décompression streaming dans ton code)
    // Puis on découvre les fichiers réellement présents dans untarred:

    val keys = s3.listKeys(batch.paths.bucket, batch.paths.untarredPrefix)

    val artifacts = keys.flatMap { k =>
      val tpe = ArtifactTyping.artifactTypeFromKey(k)
      if (tpe == ArtifactType.UNKNOWN) None // ✅ unknown ignoré
      else {
        val name = ArtifactTyping.fileName(k)
        val rel  = ArtifactTyping.toRelative(batch.paths.untarredPrefix, k)

        val withState = tpe match {
          case ArtifactType.METADATA | ArtifactType.MONITORING => None
          case _ => Some(ArtifactStatus.IN_PROGRESS)
        }

        Some(Artifact(name, tpe, rel, withState))
      }
    }

    // Paramétrage: au départ 1 fichier source dans PARAMETRAGE
    val paramFile = artifacts.find(_.artifactType == ArtifactType.PARAMETRAGE)

    batch.copy(
      status = nextStatus,
      updatedAt = Time.nowIso,

      cre = artifacts.filter(_.artifactType == ArtifactType.DATA),
      parametrageFile = paramFile,
      gesparam = Nil,

      autres = artifacts.filter(_.artifactType == ArtifactType.AUTRES),
      metadata = artifacts.filter(_.artifactType == ArtifactType.METADATA),
      monitoring = artifacts.filter(_.artifactType == ArtifactType.MONITORING)
    )
  }
}

// ---------------------------
// Step 3: DATA_CHECK
// Valide nom CRE + présence référentiel
// Invalid => IGNORED (et reste ignoré pour les étapes suivantes)
// Enrichit CRE: code, kind, frequency
// ---------------------------
final class DataCheckStep(ref: CreReferential) extends IngestionStep {

  override val name = "DATA_CHECK"
  override val expectedStatus = BatchStatus.DECOMPRESSED
  override val nextStatus = BatchStatus.DATA_CHECK

  override def process(batch: Batch): Batch = {
    val updated = batch.cre.foldLeft(batch.copy(status = nextStatus, updatedAt = Time.nowIso)) { (b, cre) =>
      if (!cre.status.contains(ArtifactStatus.IN_PROGRESS)) b
      else {
        CreNameRules.extractCode(cre.name) match {
          case None =>
            BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name) { a =>
              a.copy(status = Some(ArtifactStatus.IGNORED), error = Some("invalid CRE name pattern"))
            }

          case Some(code) =>
            ref.get(code) match {
              case None =>
                BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name) { a =>
                  a.copy(status = Some(ArtifactStatus.IGNORED), error = Some("CRE not found in referential"))
                }
              case Some(defn) =>
                BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name) { a =>
                  a.copy(
                    creCode = Some(defn.creCode),
                    creKind = Some(defn.kind),
                    creFrequency = Some(defn.frequency)
                  )
                }
            }
        }
      }
    }

    // après ce step, on passe au step suivant (gesparam transform)
    updated.copy(status = BatchStatus.GESPARAM_TRANSFORM, updatedAt = Time.nowIso)
  }
}

// ---------------------------
// Step 4: GESPARAM_TRANSFORM
// Au départ: 1 fichier parametrageFile
// On le parse et on génère batch.gesparam = List[Artifact] (type GESPARAM)
// ---------------------------
trait ParametrageParser {
  def parseParamArtifacts(paramFileContent: Iterator[String]): List[String] // retourne des noms logiques
}

final class GesparamTransformStep(s3: S3Service, parser: ParametrageParser) extends IngestionStep {

  override val name = "GESPARAM_TRANSFORM"
  override val expectedStatus = BatchStatus.GESPARAM_TRANSFORM
  override val nextStatus = BatchStatus.DATA_TRANSFORM

  override def process(batch: Batch): Batch = {
    val srcOpt = batch.parametrageFile.filter(_.status.contains(ArtifactStatus.IN_PROGRESS))

    val (newGesparam, updatedParamFile) = srcOpt match {
      case None => (batch.gesparam, batch.parametrageFile) // pas de param => pas bloquant
      case Some(src) =>
        val key = PathResolver.untarredKey(batch, src)
        val is = s3.read(batch.paths.bucket, key)
        try {
          val lines = scala.io.Source.fromInputStream(is, "UTF-8").getLines()
          val names = parser.parseParamArtifacts(lines)

          val artifacts = names.map { n =>
            Artifact(
              name = n,
              artifactType = ArtifactType.GESPARAM,
              relativePath = s"PARAMETRAGE/derived/$n.json",
              status = Some(ArtifactStatus.IN_PROGRESS)
            )
          }

          (artifacts, Some(src.copy(status = Some(ArtifactStatus.DONE))))
        } catch {
          case e: Throwable =>
            (Nil, Some(src.copy(status = Some(ArtifactStatus.FAILED), error = Some(e.getMessage))))
        } finally is.close()
    }

    batch.copy(
      gesparam = newGesparam,
      parametrageFile = updatedParamFile,
      status = nextStatus,
      updatedAt = Time.nowIso
    )
  }
}

// ---------------------------
// Step 5: DATA_TRANSFORM
// Transforme seulement les CRE IN_PROGRESS (ignores restent ignorés)
// ---------------------------
trait DataTransformer {
  def transform(inputS3: String, outputS3: String): Either[String, Unit] // Left=err, Right=ok
}

final class DataTransformStep(transformer: DataTransformer) extends IngestionStep {
  override val name = "DATA_TRANSFORM"
  override val expectedStatus = BatchStatus.DATA_TRANSFORM
  override val nextStatus = BatchStatus.DATA_TRANSFER

  override def process(batch: Batch): Batch = {
    val updated = batch.cre.foldLeft(batch) { (b, cre) =>
      if (!cre.status.contains(ArtifactStatus.IN_PROGRESS)) b
      else {
        val inKey = PathResolver.untarredKey(b, cre)
        val inS3  = s"s3://${b.paths.bucket}/$inKey"
        val outS3 = inS3.replace("/zone=untarred/", "/zone=untarred/transformed/") // exemple

        transformer.transform(inS3, outS3) match {
          case Right(_) =>
            BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name)(_.copy(status = Some(ArtifactStatus.DONE)))
          case Left(err) =>
            BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name)(_.copy(status = Some(ArtifactStatus.FAILED), error = Some(err)))
        }
      }
    }

    updated.copy(status = nextStatus, updatedAt = Time.nowIso)
  }
}

// ---------------------------
// Step 6: DATA_TRANSFER
// Lit untarred -> écrit transferable
// ---------------------------
trait TransferService {
  def transfer(inputS3: String, outputS3: String): Either[String, Unit]
}

final class DataTransferStep(transfer: TransferService) extends IngestionStep {
  override val name = "DATA_TRANSFER"
  override val expectedStatus = BatchStatus.DATA_TRANSFER
  override val nextStatus = BatchStatus.DONE

  override def process(batch: Batch): Batch = {
    val updated = batch.cre.foldLeft(batch) { (b, cre) =>
      if (!cre.status.contains(ArtifactStatus.DONE)) b // on transfère seulement les transformés OK (à adapter)
      else {
        val inKey = PathResolver.untarredKey(b, cre)
        val inS3  = s"s3://${b.paths.bucket}/$inKey"
        val outKey = PathResolver.transferableKey(b, cre.name)
        val outS3  = s"s3://${b.paths.bucket}/$outKey"

        transfer.transfer(inS3, outS3) match {
          case Right(_) => b
          case Left(err) =>
            // si tu veux marquer le CRE en FAILED au transfert :
            BatchUpdater.updateArtifact(b, ArtifactType.DATA, cre.name)(_.copy(status = Some(ArtifactStatus.FAILED), error = Some(err)))
        }
      }
    }

    updated.copy(status = nextStatus, updatedAt = Time.nowIso)
  }
}

// =====================================================
// 7) MAIN ORCHESTRATION (exécution complète)
// =====================================================

object PipelineMainExample {

  def runAll(
    s3Client: AmazonS3,
    bucket: String,
    batchId: String,
    fromFlowPrefix: String,
    tarredPrefix: String,
    untarredPrefix: String,
    transferablePrefix: String,
    statePrefix: String,
    ref: CreReferential,
    parser: ParametrageParser,
    transformer: DataTransformer,
    transfer: TransferService
  ): Unit = {

    val s3 = new DefaultS3Service(s3Client)
    val json = new JsonCodec
    val store = new S3StateStore(s3, json)
    val runner = new IngestionRunner(store, bucket)

    // 1) DECRYPT -> crée state minimal
    val initialBatch = Batch(
      batchId = batchId,
      source = "ARKEA",
      status = BatchStatus.DECRYPTED,
      createdAt = Time.nowIso,
      updatedAt = Time.nowIso,
      paths = BatchPaths(bucket, fromFlowPrefix, tarredPrefix, untarredPrefix, transferablePrefix, statePrefix),
      cre = Nil,
      parametrageFile = None,
      gesparam = Nil,
      autres = Nil,
      metadata = Nil,
      monitoring = Nil
    )

    val stateKey = PathResolver.stateKey(initialBatch)
    store.save(bucket, stateKey, initialBatch) // ✅ state créé dès decrypt (après ton IO decrypt réel)

    // 2) DECOMPRESS -> enrichit batch (découvre les fichiers)
    runner.run(new DecompressStep(s3), stateKey)

    // 3) DATA_CHECK -> valide nom + référentiel, enrichit (kind/frequency)
    runner.run(new DataCheckStep(ref), stateKey)

    // 4) GESPARAM_TRANSFORM -> transforme le fichier paramétrage en liste gesparam
    runner.run(new GesparamTransformStep(s3, parser), stateKey)

    // 5) DATA_TRANSFORM
    runner.run(new DataTransformStep(transformer), stateKey)

    // 6) DATA_TRANSFER
    runner.run(new DataTransferStep(transfer), stateKey)
  }
}
