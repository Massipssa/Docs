import org.apache.commons.compress.archivers.tar._
import java.io._
import scala.collection.parallel.CollectionConverters._
import software.amazon.awssdk.services.s3.S3Client
import software.amazon.awssdk.core.sync.RequestBody
import software.amazon.awssdk.services.s3.model.PutObjectRequest

val s3Client = S3Client.builder().build()

// Function to stream tar file from S3
def extractTar(bucket: String, tarKey: String): List[(String, Array[Byte])] = {
  val s3Obj = s3Client.getObjectAsBytes(builder => builder.bucket(bucket).key(tarKey))
  val tarInput = new ByteArrayInputStream(s3Obj.asByteArray())
  val tarArchive = new TarArchiveInputStream(new BufferedInputStream(tarInput))

  Iterator.continually(tarArchive.getNextTarEntry)
    .takeWhile(_ != null)
    .filter(!_.isDirectory)
    .map { entry =>
      val byteArray = new ByteArrayOutputStream()
      val buffer = new Array  // Read 8KB chunks
      var bytesRead = tarArchive.read(buffer)
      while (bytesRead != -1) {
        byteArray.write(buffer, 0, bytesRead)
        bytesRead = tarArchive.read(buffer)
      }
      (entry.getName, byteArray.toByteArray)
    }
    .toList
}

// Function to upload extracted files to S3 in parallel
def uploadToS3(file: (String, Array[Byte])): Unit = {
  val (fileName, content) = file
  val uploadRequest = PutObjectRequest.builder()
    .bucket("your-bucket")
    .key(s"untarred/$fileName")
    .build()
  
  s3Client.putObject(uploadRequest, RequestBody.fromBytes(content))
}

// Processing the tar file
val bucketName = "your-bucket"
val tarFileKey = "path/to/your_tar_file.tar"

val extractedFiles = extractTar(bucketName, tarFileKey)

// Convert to Parallel Collection and Upload in Parallel
extractedFiles.par.foreach(uploadToS3)

println(s"Successfully processed ${extractedFiles.length} files in parallel!")
