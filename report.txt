import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("Data Quality Check")
  .master("local[*]")
  .getOrCreate()

import spark.implicits._

// 📌 1️⃣ Simuler `metadataDf` avec les colonnes et positions
val metadataDf = Seq(
  ("familleA", "colonne1"),
  ("familleB", "colonne2"),
  ("familleX", "colonne3") // Famille inexistante dans config
).toDF("famille", "colonne")

// 📌 2️⃣ Simuler `dataFileDf` avec des valeurs à vérifier
val dataFileDf = Seq(
  ("2024-02-21", "ABC123", "dummy"),  // ✅ Correct
  ("2024/02/21", "XYZ-456", "dummy"), // ❌ Mauvais format
  ("2019-07-15", "DEF789", "dummy")   // ✅ Correct
).toDF("colonne1", "colonne2", "colonne3")

// 📌 3️⃣ Configuration regex (Familles et leurs règles de validation)
val configFamilleAttributRegex: Map[String, String] = Map(
  "familleA" -> """\d{4}-\d{2}-\d{2}""", // Date format YYYY-MM-DD
  "familleB" -> """[A-Z]{3}\d+""" // Code alphanumérique
)

// 📌 4️⃣ Convertir `configFamilleAttributRegex` en DataFrame pour la jointure
val configDf = configFamilleAttributRegex.toSeq.toDF("famille", "regex")

// 📌 5️⃣ Joindre `metadataDf` avec `configDf` pour récupérer les regex associées
val metadataWithConfigDf = metadataDf
  .join(configDf, Seq("famille"), "left") // Associer regex à chaque colonne
  .filter(col("regex").isNotNull) // Ne garder que les familles avec une regex

// 📌 6️⃣ Appliquer les validations avec `rlike` et filtrer immédiatement les lignes invalides
val checkedDf = metadataWithConfigDf.collect().foldLeft(dataFileDf) { (df, row) =>
  val columnName = row.getAs[String]("colonne")
  val regex = row.getAs[String]("regex")

  df.withColumn(s"${columnName}_check", when(col(columnName).rlike(regex), lit(null)).otherwise(lit(s"Format invalide: $columnName")))
}
.filter(
  metadataWithConfigDf.collect().map(row => col(s"${row.getAs[String]("colonne")}_check").isNotNull).reduce(_ || _)
)

// 📌 7️⃣ Afficher uniquement les lignes avec erreurs
checkedDf.show(false)
