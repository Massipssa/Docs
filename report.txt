import org.apache.spark.sql.{SparkSession, Encoders}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

case class GesparamDto(
  nomTable: String,
  codeBoite: String,
  dateGeneration: String,
  environnement: String,
  donneesTable: List[Map[String, String]]
)

object GesparamParser {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("GesparamDto Parser")
      .master("local[*]") // Run locally
      .getOrCreate()

    import spark.implicits._

    // ✅ Load the text file (Modify path)
    val rawData = spark.read.text("path/to/your/file.txt")

    // ✅ Extract JSON part after first comma
    val jsonData = rawData.withColumn("splitData", split(col("value"), ",", 2))
      .select(
        col("splitData").getItem(1).alias("jsonData")
      )

    // ✅ Define Schema for GesparamDto
    val gesparamSchema = Encoders.product[GesparamDto].schema

    // ✅ Convert JSON string to structured GesparamDto
    val parsedDF = jsonData.withColumn(
      "parsedData",
      from_json(col("jsonData"), gesparamSchema)
    ).select("parsedData.*")

    // ✅ Convert DataFrame to Dataset[GesparamDto]
    val gesparamDataset = parsedDF.as[GesparamDto]

    // ✅ Show parsed data
    gesparamDataset.show(truncate = false)

    // ✅ Stop Spark session
    spark.stop()
  }
}
