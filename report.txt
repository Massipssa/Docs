package fr.ccf.job.contract

import org.apache.spark.sql.{Column, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import com.typesafe.scalalogging.LazyLogging

object DomainEnforcer extends LazyLogging {

  def enforce(
      df: DataFrame,
      domainRules: Seq[DomainType],
      protectedCols: Set[String] = Set("event_day")
  ): DataFrame = {

    val protectedLower = protectedCols.map(_.toLowerCase)

    val newCols: Seq[Column] = df.schema.fields.map { f =>
      val colName = f.name
      val actualType = f.dataType

      // 0) Colonnes protégées -> on garde tel quel (pas de cast)
      if (protectedLower.contains(colName.toLowerCase)) {
        logger.info(s"[DOMAIN] $colName is protected -> no cast")
        col(colName)

      } else {
        val domainOpt = DomainTypeReader.detectDomainType(colName, domainRules)
        val expectedTypeOpt = domainOpt.flatMap(d => DomainTypeReader.parseSparkType(d.sparkType))

        expectedTypeOpt match {
          case None =>
            col(colName)

          case Some(expectedType) if equalsType(actualType, expectedType) =>
            logger.info(s"[DOMAIN] $colName no cast (actual=$actualType expected=$expectedType domain=${domainOpt.map(_.sparkType)})")
            col(colName)

          case Some(expectedType) =>
            logger.info(s"[DOMAIN] $colName cast from $actualType to $expectedType (domain=${domainOpt.map(_.sparkType)})")
            col(colName).cast(expectedType).as(colName)
        }
      }
    }

    df.select(newCols: _*)
  }

  private def equalsType(a: DataType, b: DataType): Boolean =
    a.catalogString.equalsIgnoreCase(b.catalogString)
}
