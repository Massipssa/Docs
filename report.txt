import org.apache.spark.sql.{SparkSession, Encoders}
import org.apache.spark.sql.functions._

case class StructureInterfaceDto(
  famille: String,
  identifiant: String,
  libelleLong: String,
  nomDonnee: String,
  obligatoire: String,
  propagation: String,
  definition: String,
  position: Int
)

case class InterfaceDto(
  codeEvenement: String,
  codeBoite: String,
  libelleLong: String,
  dateMaj: String,
  profondeur: String,
  description: String,
  structureInterface: List[StructureInterfaceDto]
)

case class MetadataLineDto(
  creId: String,
  env: String,
  interfaceDto: InterfaceDto
)

object MetadataParser {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("MetadataLineDto Parser")
      .master("local[*]") // Run locally
      .getOrCreate()

    import spark.implicits._

    // Load the text file using read.text()
    val rawData = spark.read.text("path/to/your/file.txt")

    // Split each line into three parts: creId, env, JSON Data
    val jsonData = rawData.withColumn("splitData", split(col("value"), ",", 3))
      .select(
        col("splitData").getItem(0).alias("creId"),
        col("splitData").getItem(1).alias("env"),
        col("splitData").getItem(2).alias("jsonData")
      )

    // Parse JSON column into `InterfaceDto`
    val interfaceDataset = jsonData.select(
      col("creId"),
      col("env"),
      from_json(col("jsonData"), Encoders.product[InterfaceDto].schema).as("interfaceDto")
    )

    // Convert to `Dataset[MetadataLineDto]`
    val metadataDataset = interfaceDataset.map(row =>
      MetadataLineDto(
        creId = row.getAs[String]("creId"),
        env = row.getAs[String]("env"),
        interfaceDto = row.getAs[InterfaceDto]("interfaceDto")
      )
    )

    // Show parsed data
    metadataDataset.show(truncate = false)

    // Stop Spark session
    spark.stop()
  }
}
