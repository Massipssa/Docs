import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.amazonaws.services.s3.model._
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}

import java.io._
import java.util.UUID
import scala.collection.mutable.ListBuffer
import scala.concurrent.{ExecutionContext, Future}
import scala.util.{Failure, Success}
import scala.jdk.CollectionConverters._

object S3TarBz2ParallelUpload {

  val bucketName = "your-bucket-name"
  val inputKey = "input-folder/archive.tar.bz2"
  val outputPrefix = "extracted-folder/"
  val partSize = 100 * 1024 * 1024 // 100MB par partie

  // CrÃ©er un contexte d'exÃ©cution pour l'upload parallÃ¨le (ThreadPool)
  implicit val ec: ExecutionContext = ExecutionContext.global

  def extractAndUploadParallel(s3Client: AmazonS3): Unit = {
    // 1ï¸âƒ£ Lire le fichier `.tar.bz2` en streaming depuis S3
    val s3Object = s3Client.getObject(new GetObjectRequest(bucketName, inputKey))
    val s3InputStream = s3Object.getObjectContent

    // 2ï¸âƒ£ DÃ©compresser le fichier en mÃ©moire
    val bz2Stream = new BZip2CompressorInputStream(s3InputStream)
    val tarInput = new TarArchiveInputStream(bz2Stream)

    var entry: TarArchiveEntry = tarInput.getNextTarEntry

    val uploadFutures = ListBuffer[Future[Unit]]()

    while (entry != null) {
      if (!entry.isDirectory) {
        val fileName = entry.getName
        val s3TargetKey = outputPrefix + fileName

        println(s"Extraction et Upload en parallÃ¨le: $fileName -> $s3TargetKey")

        // Lire le fichier extrait en mÃ©moire
        val outputStream = new ByteArrayOutputStream()
        val buffer = new Array[Byte](16 * 1024) // 16KB buffer
        var bytesRead = 0

        while ({ bytesRead = tarInput.read(buffer); bytesRead != -1 }) {
          outputStream.write(buffer, 0, bytesRead)
        }

        val fileData = outputStream.toByteArray
        outputStream.close()

        // 3ï¸âƒ£ Lancer l'upload en parallÃ¨le avec Future
        val futureUpload = Future {
          multipartUpload(s3Client, bucketName, s3TargetKey, fileData)
        }

        uploadFutures.append(futureUpload)
      }
      entry = tarInput.getNextTarEntry
    }

    // 4ï¸âƒ£ Fermer les flux
    tarInput.close()
    bz2Stream.close()
    s3InputStream.close()

    // 5ï¸âƒ£ Attendre la fin de tous les uploads
    Future.sequence(uploadFutures).onComplete {
      case Success(_) => println("Tous les fichiers ont Ã©tÃ© uploadÃ©s avec succÃ¨s ðŸš€")
      case Failure(ex) => println(s"Erreur lors de l'upload: ${ex.getMessage}")
    }
  }

  def multipartUpload(s3Client: AmazonS3, bucket: String, key: String, data: Array[Byte]): Unit = {
    val uploadId = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(bucket, key)).getUploadId
    val partETags = ListBuffer[PartETag]()
    val totalSize = data.length
    var start = 0
    var partNumber = 1

    while (start < totalSize) {
      val end = Math.min(start + partSize, totalSize)
      val partData = data.slice(start, end)

      val inputStream = new ByteArrayInputStream(partData)
      val uploadPartRequest = new UploadPartRequest()
        .withBucketName(bucket)
        .withKey(key)
        .withUploadId(uploadId)
        .withPartNumber(partNumber)
        .withInputStream(inputStream)
        .withPartSize(partData.length)

      val partResult = s3Client.uploadPart(uploadPartRequest)
      partETags.append(partResult.getPartETag)

      println(s"Partie $partNumber uploadÃ©e: ${partData.length} bytes")
      start = end
      partNumber += 1
    }

    // âœ… Convertir Scala ListBuffer en Java List
    s3Client.completeMultipartUpload(new CompleteMultipartUploadRequest(bucket, key, uploadId, partETags.asJava))
    println(s"Upload terminÃ© pour $key âœ…")
  }

  def main(args: Array[String]): Unit = {
    val s3Client = AmazonS3ClientBuilder.standard().build()
    extractAndUploadParallel(s3Client)
  }
}
