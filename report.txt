def appendDataFrameToS3(
    df: DataFrame,
    csvPrefix: String,
    csvDelimiter: String = GeneralConstants.CSV_DELIMITER,
    showHeader: Boolean = true,
    oneCsvForAllPartitions: Boolean = true
)(implicit spark: SparkSession): String = {

  try {
    val dataFrameToWrite = if (oneCsvForAllPartitions) df.coalesce(1) else df

    dataFrameToWrite.write
      .option("header", showHeader)
      .option("sep", csvDelimiter)
      .mode(SaveMode.Append)
      .csv(csvPrefix)

    csvPrefix
  } catch {
    case ex: Throwable =>
      logger.warn(s"Failed to write DataFrame to S3 with prefix: $csvPrefix", ex)
      throw new RuntimeException(s"Unable to write to S3 path: $csvPrefix", ex)
  }
}


import org.scalatest.funsuite.AnyFunSuite
import org.apache.spark.sql.{SaveMode, SparkSession}
import java.nio.file.{Files, Paths}
import java.io.File

class SparkUtilityTest extends AnyFunSuite with SparkSessionTestWrapper {

  test("appendDataFrameToS3 should write CSV to given prefix") {
    import spark.implicits._

    val df = Seq(
      ("Alice", 30),
      ("Bob", 25)
    ).toDF("name", "age")

    val tmpDir = Files.createTempDirectory("test_csv_output").toString

    val resultPrefix = SparkUtility.appendDataFrameToS3(
      df,
      tmpDir,
      csvDelimiter = ",",
      showHeader = true,
      oneCsvForAllPartitions = true
    )(spark)

    assert(resultPrefix == tmpDir)
    val files = new File(tmpDir).listFiles().filter(_.getName.endsWith(".csv"))
    assert(files.nonEmpty)
  }

  test("appendDataFrameToS3 should throw exception on invalid path") {
    import spark.implicits._

    val df = Seq(("test", 1)).toDF("col1", "col2")

    assertThrows[RuntimeException] {
      SparkUtility.appendDataFrameToS3(df, "/invalid-path/output.csv")(spark)
    }
  }
}
