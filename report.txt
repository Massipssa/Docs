import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
import org.scalatest.funsuite.AnyFunSuite
import fr.ccf.job.parser.MonitoringDataParser

class MonitoringDataParserTest extends AnyFunSuite {

  val spark: SparkSession = SparkSession.builder()
    .master("local[*]")
    .appName("MonitoringDataParserTest")
    .getOrCreate()

  import spark.implicits._

  test("parseMonitoringDataFile should parse JSON lines correctly") {
    val tempFile = java.io.File.createTempFile("monitoring", ".json")
    tempFile.deleteOnExit()

    val sampleJson =
      """
        |{"value": "{\"CRE\": \"1234\", \"nbLigne\": 5, \"dateTraitement\": \"2024-05-01\", \"vacation\": \"night\"}"}
        |""".stripMargin

    java.nio.file.Files.write(tempFile.toPath, sampleJson.getBytes)

    val resultDF = MonitoringDataParser.parseMonitoringDataFile(tempFile.getAbsolutePath, spark)

    val expectedSchema = StructType(Seq(
      StructField("CRE", StringType, nullable = true),
      StructField("nbLigne", IntegerType, nullable = true),
      StructField("dateTraitement", StringType, nullable = true),
      StructField("vacation", StringType, nullable = true)
    ))

    val expectedRow = Row("1234", 5, "2024-05-01", "night")

    assert(resultDF.schema === expectedSchema)
    assert(resultDF.collect().head === expectedRow)
  }
}
