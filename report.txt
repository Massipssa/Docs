import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.amazonaws.services.s3.model._
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
import org.apache.commons.compress.archivers.tar.{TarArchiveEntry, TarArchiveInputStream}

import java.io._
import scala.collection.parallel.CollectionConverters._
import scala.collection.mutable.ListBuffer
import scala.collection.JavaConverters._

object S3TarBz2ParallelUpload {

  val bucketName = "your-bucket-name"
  val inputKey = "input-folder/archive.tar.bz2"
  val outputPrefix = "extracted-folder/"
  val partSize = 100 * 1024 * 1024 // 100MB par partie

  def extractAndUploadParallel(s3Client: AmazonS3): Unit = {
    // 1️⃣ Lire le fichier `.tar.bz2` en streaming depuis S3
    val s3Object = s3Client.getObject(new GetObjectRequest(bucketName, inputKey))
    val s3InputStream = s3Object.getObjectContent

    // 2️⃣ Décompresser le fichier en mémoire
    val bz2Stream = new BZip2CompressorInputStream(s3InputStream)
    val tarInput = new TarArchiveInputStream(bz2Stream)

    var entry: TarArchiveEntry = tarInput.getNextTarEntry
    val extractedFiles = ListBuffer[(String, Array[Byte])]()

    while (entry != null) {
      if (!entry.isDirectory) {
        val fileName = entry.getName
        val s3TargetKey = outputPrefix + fileName

        println(s"Extraction: $fileName -> $s3TargetKey")

        // Lire le fichier extrait en mémoire
        val outputStream = new ByteArrayOutputStream()
        val buffer = new Array[Byte](16 * 1024) // 16KB buffer
        var bytesRead = 0

        while ({ bytesRead = tarInput.read(buffer); bytesRead != -1 }) {
          outputStream.write(buffer, 0, bytesRead)
        }

        extractedFiles.append((s3TargetKey, outputStream.toByteArray))
        outputStream.close()
      }
      entry = tarInput.getNextTarEntry
    }

    // 3️⃣ Fermer les flux
    tarInput.close()
    bz2Stream.close()
    s3InputStream.close()

    println(s"Nombre de fichiers extraits: ${extractedFiles.size}, début de l'upload parallèle...")

    // 4️⃣ Transformer la liste en collection parallèle et lancer les uploads en parallèle
    extractedFiles.par.foreach { case (s3Key, fileData) =>
      multipartUpload(s3Client, bucketName, s3Key, fileData)
    }

    println("✅ Tous les fichiers ont été uploadés avec succès 🚀")
  }

  def multipartUpload(s3Client: AmazonS3, bucket: String, key: String, data: Array[Byte]): Unit = {
    val uploadId = s3Client.initiateMultipartUpload(new InitiateMultipartUploadRequest(bucket, key)).getUploadId
    val partETags = ListBuffer[PartETag]()
    val totalSize = data.length
    var start = 0
    var partNumber = 1

    while (start < totalSize) {
      val end = Math.min(start + partSize, totalSize)
      val partData = data.slice(start, end)

      val inputStream = new ByteArrayInputStream(partData)
      val uploadPartRequest = new UploadPartRequest()
        .withBucketName(bucket)
        .withKey(key)
        .withUploadId(uploadId)
        .withPartNumber(partNumber)
        .withInputStream(inputStream)
        .withPartSize(partData.length)

      val partResult = s3Client.uploadPart(uploadPartRequest)
      partETags.append(partResult.getPartETag)

      println(s"Partie $partNumber uploadée : ${partData.length} bytes")
      start = end
      partNumber += 1
    }

    // ✅ Convertir Scala ListBuffer en Java List (compatible Scala 2.12)
    s3Client.completeMultipartUpload(new CompleteMultipartUploadRequest(bucket, key, uploadId, partETags.asJava))
    println(s"✅ Upload terminé pour $key")
  }

  def main(args: Array[String]): Unit = {
    val s3Client = AmazonS3ClientBuilder.standard().build()
    extractAndUploadParallel(s3Client)
  }
}
