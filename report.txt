import org.scalatest.funsuite.AnyFunSuite
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

class RequiredColumnsCheckTest extends AnyFunSuite {

  implicit val spark: SparkSession = SparkSession.builder()
    .appName("RequiredColumnsCheckTest")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  test("should detect empty required columns and return CheckErrorDto") {
    // Simuler un creDF avec une colonne vide
    val creDF = Seq(
      Row("1", null, "fam1", "id1", "data1")
    )

    val creSchema = StructType(Seq(
      StructField("c0", StringType),
      StructField("c1", StringType),
      StructField("famille", StringType),
      StructField("identifiant", StringType),
      StructField("nomDonnee", StringType)
    ))

    val creDf = spark.createDataFrame(
      spark.sparkContext.parallelize(creDF),
      creSchema
    )

    // Simuler metadataDF
    val metadata = Seq(
      ("eventX", true, 2)
    ).toDF("codeEvenement", "obligatoire", "position")
      .withColumn("interfaceDto", struct(
        col("codeEvenement").as("codeEvenement"),
        array(struct(
          col("obligatoire").as("obligatoire"),
          col("position").as("position")
        )).as("structureInterfaces")
      )).drop("codeEvenement", "obligatoire", "position")

    val creCsvFilenameDto = new CreCsvFilenameDto {
      override def getCodeEvenement: String = "eventX"
    }

    val errors = RequiredColumnsCheck.check(creDf, metadata, creCsvFilenameDto)
    assert(errors.nonEmpty)
    assert(errors.head.description.get.contains("Not empty column value"))
  }
}
