import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

/**
 * Objectif (sans cast explicite) :
 * 1) Construire un sch√©ma StructType √† partir des domainRules
 * 2) √Ä la 1√®re ex√©cution : cr√©er la table Iceberg avec CE sch√©ma (DDL), puis append
 * 3) Aux ex√©cutions suivantes : r√©cup√©rer le sch√©ma depuis Iceberg et LIRE les donn√©es d‚Äôentr√©e avec ce sch√©ma
 *    (donc pas de cast ensuite : si la donn√©e ne respecte pas le sch√©ma -> √©chec/NULL selon le parse du reader)
 *
 * IMPORTANT : Sans cast, la seule fa√ßon ‚Äúpropre‚Äù d‚Äôavoir les bons types est de LIRE d√®s le d√©part avec le bon sch√©ma.
 *             Si tu lis tout en string puis tu √©cris, tu seras oblig√© de caster ou √ßa √©chouera.
 */
object IcebergDomainSchemaWriter {

  // ----------------------------
  // 1) Build schema from domain rules
  // ----------------------------

  /** Construit un sch√©ma cible √† partir des colonnes du DF (noms + ordre) et des domainRules. */
  def buildSchemaFromDomainRules(dfColumnsInOrder: Seq[String], domainRules: Seq[DomainFamiliesType]): StructType = {
    val fields = dfColumnsInOrder.map { colName =>
      val domainOpt = DomainTypeReader.detectDomainType(colName, domainRules)

      // ‚ö†Ô∏è adapte ici si ton mod√®le a `sparkType` au lieu de `description`
      val dtOpt: Option[DataType] = domainOpt.flatMap(d => DomainTypeReader.parseSparkType(d.description))

      // Si pas de r√®gle -> StringType par d√©faut (ou ce que tu veux)
      // üëâ Tu peux aussi choisir "ne pas cr√©er la colonne" si pas de domaine, mais g√©n√©ralement on la garde.
      StructField(colName, dtOpt.getOrElse(StringType), nullable = true)
    }
    StructType(fields)
  }

  // ----------------------------
  // 2) Create Iceberg table with the target schema (first run)
  // ----------------------------

  /**
   * Cr√©e une table Iceberg via DDL avec un sch√©ma exact.
   * partitionSpecSql : ex "years(event_date), months(event_date)" ou "event_day" etc.
   * tableProperties  : TBLPROPERTIES('key'='value', ...)
   */
  def createIcebergTableIfMissing(
    spark: SparkSession,
    fullTableName: String,                 // ex: "catalog.db.table"
    targetSchema: StructType,
    partitionSpecSql: Option[String],      // ex: Some("years(event_date), months(event_date)")
    tableProperties: Map[String, String],
    tableExists: => Boolean
  ): Unit = {
    if (tableExists) return

    val colsSql = targetSchema.fields.map { f =>
      s"`${f.name}` ${toSqlType(f.dataType)}"
    }.mkString(",\n  ")

    val partitionSql = partitionSpecSql.map(p => s"\nPARTITIONED BY ($p)").getOrElse("")

    val propsSql =
      if (tableProperties.isEmpty) ""
      else {
        val kv = tableProperties.map { case (k, v) => s"'$k'='$v'" }.mkString(", ")
        s"\nTBLPROPERTIES ($kv)"
      }

    val ddl =
      s"""
         |CREATE TABLE $fullTableName (
         |  $colsSql
         |)
         |USING iceberg$partitionSql$propsSql
         |""".stripMargin

    spark.sql(ddl)
  }

  private def toSqlType(dt: DataType): String = dt match {
    case StringType            => "string"
    case IntegerType           => "int"
    case LongType              => "bigint"
    case ShortType             => "smallint"
    case ByteType              => "tinyint"
    case BooleanType           => "boolean"
    case DoubleType            => "double"
    case FloatType             => "float"
    case DateType              => "date"
    case TimestampType         => "timestamp"
    case d: DecimalType        => s"decimal(${d.precision},${d.scale})"
    case BinaryType            => "binary"
    // si tu as struct/array/map, tu peux √©tendre ici
    case other                 => other.sql
  }

  // ----------------------------
  // 3) Read input using the table schema (next runs)
  // ----------------------------

  /** R√©cup√®re le sch√©ma depuis la table Iceberg existante (source de v√©rit√©). */
  def getIcebergTableSchema(spark: SparkSession, fullTableName: String): StructType =
    spark.table(fullTableName).schema

  /**
   * Lit l‚Äôentr√©e AVEC un sch√©ma (pas de cast ensuite).
   * Exemple CSV/JSON/Parquet : tu fournis format + options + path(s)
   */
  def readInputWithSchema(
    spark: SparkSession,
    format: String,
    paths: Seq[String],
    schema: StructType,
    options: Map[String, String] = Map.empty
  ): DataFrame = {
    val reader = spark.read.format(format).schema(schema)
    options.foreach { case (k, v) => reader.option(k, v) }
    reader.load(paths: _*)
  }

  // ----------------------------
  // 4) End-to-end writer
  // ----------------------------

  /**
   * Ecriture ‚Äúsans cast explicite‚Äù :
   * - 1√®re fois : build schema depuis domainRules => CREATE TABLE (schema) => read input avec schema => append
   * - ensuite : schema vient de Iceberg => read input avec schema => append
   */
  def writeWithDomainSchema(
    spark: SparkSession,
    fullTableName: String,
    inputFormat: String,
    inputPaths: Seq[String],
    inputOptions: Map[String, String],
    domainRules: Seq[DomainFamiliesType],

    // tu peux construire √ßa depuis ton TableInfo/PartitionParts
    partitionSpecSql: Option[String],
    tableProperties: Map[String, String],

    tableExists: => Boolean
  ): Unit = {

    // A) D√©terminer le sch√©ma √† utiliser
    val schemaToUse: StructType =
      if (tableExists) {
        // source de v√©rit√© : Iceberg
        getIcebergTableSchema(spark, fullTableName)
      } else {
        // 1√®re fois : construit depuis domainRules
        // üëâ IMPORTANT : il nous faut l‚Äôordre des colonnes attendues.
        //    Id√©alement tu l‚Äôas dans ton data contract / metadata.
        //    Ici, on d√©duit depuis l‚Äôen-t√™te de lecture "sans schema" (juste pour r√©cup√©rer les noms).
        //    Si tu as d√©j√† la liste des colonnes, remplace cette partie.
        val headerCols: Seq[String] = {
          val tmp = spark.read.format(inputFormat)
          inputOptions.foreach { case (k, v) => tmp.option(k, v) }
          val df0 = tmp.load(inputPaths: _*)
          df0.columns.toSeq
        }
        buildSchemaFromDomainRules(headerCols, domainRules)
      }

    // B) Si table absente : cr√©er la table avec ce sch√©ma
    createIcebergTableIfMissing(
      spark = spark,
      fullTableName = fullTableName,
      targetSchema = schemaToUse,
      partitionSpecSql = partitionSpecSql,
      tableProperties = tableProperties,
      tableExists = tableExists
    )

    // C) Lire l‚Äôentr√©e avec le sch√©ma ‚Äúsource de v√©rit√©‚Äù (domainRules √† la 1√®re fois, sinon Iceberg)
    val dfTyped = readInputWithSchema(
      spark = spark,
      format = inputFormat,
      paths = inputPaths,
      schema = schemaToUse,
      options = inputOptions
    )

    // D) √âcrire : Iceberg impose son sch√©ma ; pas de cast explicite ici
    dfTyped.writeTo(fullTableName).append()
  }
}
