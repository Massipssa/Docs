src/main/scala/fr/ccf/job/ingest/
  IngestRunner.scala
  IngestService.scala
  ReportReader.scala
  TableInfoService.scala
  DataFrameLoader.scala
  DataFrameTransformer.scala
  TableWriter.scala
  model.scala

src/test/scala/fr/ccf/job/ingest/
  SparkTestSession.scala
  ReportReaderSpec.scala
  DataFrameTransformerSpec.scala
  IngestServiceSpec.scala

package fr.ccf.job.ingest

final case class JobConf(rzBucket: String)
final case class DbConf(/* ... */)

final case class TableInfo(tableName: String, keyInRz: String)

final case class IngestionConfig(
  formatVersion: String = "2",
  targetFileSizeBytes: Long = 536870912L, // 512MB
  castColumnsToString: Seq[String] = Seq("year", "month", "day")
)

final case class IngestResult(success: List[String], failed: Map[String, String])


package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions.col

trait ReportReader {
  def listIngestedKeys(reportPath: String): List[String]
}

final class SparkCsvReportReader(
  implicit spark: SparkSession,
  keyCol: String = "key_in_raw_zone",
  statusCol: String = "status",
  ingestedValue: String = "Ingested",
  delimiter: String = ",",
  header: Boolean = true
) extends ReportReader {

  override def listIngestedKeys(reportPath: String): List[String] = {
    import spark.implicits._

    spark.read
      .option("header", header.toString)
      .option("delimiter", delimiter)
      .csv(reportPath)
      .select(col(keyCol).as("key"), col(statusCol).as("status"))
      .filter(col("status") === ingestedValue)
      .select("key")
      .as[String]
      .collect()
      .toList
  }
}

package fr.ccf.job.ingest

trait TableInfoService {
  def extractTableInfo(rzBucket: String, key: String): Option[TableInfo]
}

// Adapter vers ton code actuel
final class DefaultTableInfoService extends TableInfoService {
  override def extractTableInfo(rzBucket: String, key: String): Option[TableInfo] =
    ObjectUtil.extractTableInfo(rzBucket, key) // <-- ton existant
}


package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame, SparkSession}

trait DataFrameLoader {
  def loadCsv(key: String): DataFrame
}

final class SparkUtilDataFrameLoader(implicit spark: SparkSession) extends DataFrameLoader {
  override def loadCsv(key: String): DataFrame =
    SparkUtil.readCsvAsDf(key) // <-- ton existant
}

package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame}
import org.apache.spark.sql.functions.col

trait DataFrameTransformer {
  def prepare(df: DataFrame): DataFrame
}

final class CastColumnsToStringTransformer(cols: Seq[String]) extends DataFrameTransformer {
  override def prepare(df: DataFrame): DataFrame =
    cols.foldLeft(df) { (acc, c) =>
      if (acc.columns.contains(c)) acc.withColumn(c, col(c).cast("string")) else acc
    }
}


package fr.ccf.job.ingest

import org.apache.spark.sql.DataFrame

trait TableWriter {
  def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit
}

final class IcebergTableWriter extends TableWriter {
  override def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit =
    TableCreator.createOrAppendTable(
      dbConf = dbConf,
      tableName = tableName,
      df = df,
      tableProperties = tableProperties
    )
}

package fr.ccf.job.ingest

import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession

final class IngestService(
  jobConf: JobConf,
  dbConf: DbConf,
  reportReader: ReportReader,
  tableInfoService: TableInfoService,
  loader: DataFrameLoader,
  transformer: DataFrameTransformer,
  writer: TableWriter,
  config: IngestionConfig
)(implicit spark: SparkSession) extends Logging {

  private val tableProperties: Map[String, String] = Map(
    "format-version" -> config.formatVersion,
    "write.target-file-size-bytes" -> config.targetFileSizeBytes.toString
  )

  def ingestFromReport(reportPath: String): IngestResult = {
    val keys = reportReader.listIngestedKeys(reportPath)

    val (ok, ko) = keys.foldLeft((List.empty[String], Map.empty[String, String])) {
      case ((successAcc, failedAcc), key) =>
        ingestOneKey(key) match {
          case Right(_)  => (key :: successAcc, failedAcc)
          case Left(err) => (successAcc, failedAcc + (key -> err))
        }
    }

    IngestResult(ok.reverse, ko)
  }

  def ingestOneKey(key: String): Either[String, Unit] = {
    tableInfoService.extractTableInfo(jobConf.rzBucket, key) match {
      case None =>
        val msg = s"TableInfo introuvable pour key=$key"
        logWarning(msg)
        Left(msg)

      case Some(ti) =>
        try {
          logInfo(s"Ingesting table=${ti.tableName}, key=${ti.keyInRz}")
          val rawDf = loader.loadCsv(ti.keyInRz)
          val finalDf = transformer.prepare(rawDf)

          writer.createOrAppend(dbConf, ti.tableName, finalDf, tableProperties)
          Right(())
        } catch {
          case e: Throwable =>
            val msg = s"Echec ingestion key=$key : ${e.getMessage}"
            logError(msg, e)
            Left(msg)
        }
    }
  }
}

package fr.ccf.job.ingest

import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession

object IngestRunner extends Logging {

  def run(jobConf: JobConf, dbConf: DbConf, reportPath: String)(implicit spark: SparkSession): IngestResult = {
    val config = IngestionConfig()

    val service = new IngestService(
      jobConf = jobConf,
      dbConf = dbConf,
      reportReader = new SparkCsvReportReader(),
      tableInfoService = new DefaultTableInfoService(),
      loader = new SparkUtilDataFrameLoader(),
      transformer = new CastColumnsToStringTransformer(config.castColumnsToString),
      writer = new IcebergTableWriter(),
      config = config
    )

    service.ingestFromReport(reportPath)
  }
}


package fr.ccf.job.ingest

import org.apache.spark.sql.SparkSession
import org.scalatest.BeforeAndAfterAll
import org.scalatest.funsuite.AnyFunSuite

trait SparkTestSession extends AnyFunSuite with BeforeAndAfterAll {
  implicit var spark: SparkSession = _

  override def beforeAll(): Unit = {
    spark = SparkSession.builder()
      .appName("ingest-tests")
      .master("local[2]")
      .config("spark.ui.enabled", "false")
      .getOrCreate()
  }

  override def afterAll(): Unit = {
    if (spark != null) spark.stop()
  }
}


package fr.ccf.job.ingest

import java.nio.file.Files

final class ReportReaderSpec extends SparkTestSession {

  test("listIngestedKeys retourne uniquement les keys en status=Ingested") {
    val dir = Files.createTempDirectory("report-test")
    val report = dir.resolve("report.csv")

    val content =
      """key_in_raw_zone,status
        |a.csv,Ingested
        |b.csv,Error
        |c.csv,Ingested
        |""".stripMargin

    Files.write(report, content.getBytes("UTF-8"))

    val reader = new SparkCsvReportReader()
    val keys = reader.listIngestedKeys(report.toString)

    assert(keys.sorted == List("a.csv", "c.csv"))
  }
}

package fr.ccf.job.ingest

import org.apache.spark.sql.types.StringType

final class DataFrameTransformerSpec extends SparkTestSession {

  test("CastColumnsToStringTransformer caste seulement les colonnes existantes") {
    import spark.implicits._

    val df = Seq((2025, 12, 18, "x")).toDF("year", "month", "day", "payload")

    val transformer = new CastColumnsToStringTransformer(Seq("year", "month", "day", "missing_col"))
    val out = transformer.prepare(df)

    assert(out.schema("year").dataType == StringType)
    assert(out.schema("month").dataType == StringType)
    assert(out.schema("day").dataType == StringType)
    assert(out.schema("payload").dataType != StringType) // payload reste tel quel
  }
}

package fr.ccf.job.ingest

import org.apache.spark.sql.{DataFrame}

final class IngestServiceSpec extends SparkTestSession {

  test("ingestOneKey appelle le writer avec les bonnes propriétés") {
    val jobConf = JobConf(rzBucket = "rz-bucket")
    val dbConf = DbConf()

    val reportReader = new ReportReader {
      override def listIngestedKeys(reportPath: String): List[String] = List("k1")
    }

    val tableInfoService = new TableInfoService {
      override def extractTableInfo(rzBucket: String, key: String): Option[TableInfo] =
        Some(TableInfo("my_table", "s3://rz/path/file.csv"))
    }

    val loader = new DataFrameLoader {
      override def loadCsv(key: String): DataFrame = {
        import spark.implicits._
        Seq((1, 2, 3)).toDF("year", "month", "day")
      }
    }

    val transformer = new CastColumnsToStringTransformer(Seq("year", "month", "day"))

    final class CapturingWriter extends TableWriter {
      var called = false
      var props: Map[String, String] = Map.empty
      override def createOrAppend(dbConf: DbConf, tableName: String, df: DataFrame, tableProperties: Map[String, String]): Unit = {
        called = true
        props = tableProperties
        assert(tableName == "my_table")
      }
    }

    val writer = new CapturingWriter
    val config = IngestionConfig()

    val service = new IngestService(
      jobConf, dbConf, reportReader, tableInfoService, loader, transformer, writer, config
    )

    val res = service.ingestOneKey("k1")
    assert(res.isRight)
    assert(writer.called)
    assert(writer.props("format-version") == "2")
    assert(writer.props("write.target-file-size-bytes") == "536870912")
  }
}

