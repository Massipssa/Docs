import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("Data Quality Check")
  .master("local[*]")
  .getOrCreate()

import spark.implicits._

// ğŸ“Œ 1ï¸âƒ£ Simuler `metadataDf` avec les colonnes et positions
val metadataDf = Seq(
  ("familleA", "colonne1"),
  ("familleB", "colonne2"),
  ("familleX", "colonne3") // Famille inexistante dans config
).toDF("famille", "colonne")

// ğŸ“Œ 2ï¸âƒ£ Simuler `dataFileDf` avec des valeurs Ã  vÃ©rifier
val dataFileDf = Seq(
  ("2024-02-21", "ABC123", "dummy"),  // âœ… Correct
  ("2024/02/21", "XYZ-456", "dummy"), // âŒ Mauvais format
  ("2019-07-15", "DEF789", "dummy")   // âœ… Correct
).toDF("colonne1", "colonne2", "colonne3")

// ğŸ“Œ 3ï¸âƒ£ Configuration regex (Familles et leurs rÃ¨gles de validation)
val configFamilleAttributRegex: Map[String, String] = Map(
  "familleA" -> """\d{4}-\d{2}-\d{2}""", // Date format YYYY-MM-DD
  "familleB" -> """[A-Z]{3}\d+""" // Code alphanumÃ©rique
)

// ğŸ“Œ 4ï¸âƒ£ Convertir `configFamilleAttributRegex` en DataFrame pour la jointure
val configDf = configFamilleAttributRegex.toSeq.toDF("famille", "regex")

// ğŸ“Œ 5ï¸âƒ£ Joindre `metadataDf` avec `configDf` pour rÃ©cupÃ©rer les regex associÃ©es
val metadataWithConfigDf = metadataDf
  .join(configDf, Seq("famille"), "left") // Associer regex Ã  chaque colonne
  .filter(col("regex").isNotNull) // Ne garder que les familles avec une regex

// ğŸ“Œ 6ï¸âƒ£ Appliquer les validations avec `rlike` et filtrer immÃ©diatement les lignes invalides
val checkedDf = metadataWithConfigDf.collect().foldLeft(dataFileDf) { (df, row) =>
  val columnName = row.getAs[String]("colonne")
  val regex = row.getAs[String]("regex")

  df.withColumn(s"${columnName}_check", when(col(columnName).rlike(regex), lit(null)).otherwise(lit(s"Format invalide: $columnName")))
}
.filter(
  metadataWithConfigDf.collect().map(row => col(s"${row.getAs[String]("colonne")}_check").isNotNull).reduce(_ || _)
)

// ğŸ“Œ 7ï¸âƒ£ Afficher uniquement les lignes avec erreurs
checkedDf.show(false)
