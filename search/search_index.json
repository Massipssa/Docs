{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to The Best Data Engineer Your all-in-one learning hub for mastering the craft of data engineering.</p> <p>Note</p> <p>Whether you're just starting your journey or refining your skills, this platform guides you through the key pillars  of a modern data engineer:</p> <ul> <li>Programming for automation and ETL scripting</li> <li>Big Data for scalable processing with Spark, Kafka, and beyond</li> <li>Cloud for building reliable, serverless pipelines</li> <li>DevOps to operationalize and monitor your workflows</li> </ul> <p>Explore curated content, real-world examples, and tooling best practices that bridge software engineering, infrastructure, and data processing \u2014 all in one place.</p> <p></p> <p>Let's build better data pipelines, together.</p>"},{"location":"big-data/architecture/","title":"Architecture","text":"<ol> <li>How to choose storage tool</li> <li>Managing risks</li> <li>SQL and NOSQL</li> </ol>"},{"location":"big-data/architecture/#how-to-choose-storage-tool","title":"How to choose storage tool","text":"<ul> <li>Data model<ul> <li>Key/value</li> <li>Semi-structured</li> <li>Column-oriented</li> <li>Document-oriented</li> </ul> </li> <li>Storage model<ul> <li>In-memory </li> <li>Persistent</li> </ul> </li> <li>Consistency model<ul> <li>Strictly</li> <li>Eventually consistent</li> </ul> </li> <li>Physical model <ul> <li>Distributed</li> <li>Single machine</li> </ul> </li> <li>Read/write performance<ul> <li>Read / write equals ?  equal ?</li> <li>Does it support range scans or is it better suited doing random reads</li> </ul> </li> <li> <p>Secondary indexes</p> <ul> <li>Does it support secondary indexes ?</li> <li>How strong it supports secondary indexes</li> </ul> </li> <li> <p>Failure handling</p> <ul> <li>How it handles servers failure ?</li> <li>It is able to continue operating ?</li> <li>How it backups data in server replacement ? </li> <li>How it performs server decommissioning</li> </ul> </li> <li> <p>Compression</p> <ul> <li>Is the compression method pluggable? </li> <li>What types are available?</li> </ul> </li> <li> <p>Load balancing</p> <ul> <li>Does it accept load balancing </li> <li>How it performs load balancing in case of high throughput</li> </ul> </li> <li> <p>Atomic read-modify-write</p> </li> <li> <p>Locking, waits and dead-locks</p> </li> <li>Can it be free for wait and therefore deadlocks ?</li> <li>Security<ul> <li>??  </li> </ul> </li> </ul>"},{"location":"big-data/architecture/#questions-to-ask","title":"Questions to ask","text":"<ul> <li>Types of data to be stored</li> <li>How the data will be transformed</li> <li>Who should access the data</li> <li>What are the typical access patterns</li> </ul>"},{"location":"big-data/architecture/#steps-to-mount-cluster","title":"Steps to mount cluster","text":"<ul> <li>Chose OS</li> <li>Select the FS to use with disk<ul> <li>ext3</li> <li>ext4</li> <li>XFS</li> </ul> </li> </ul>"},{"location":"big-data/architecture/#managing-risk","title":"Managing risk","text":""},{"location":"big-data/architecture/#categories-of-risks","title":"Categories of risks","text":"<ul> <li>Technology risk<ul> <li>Individual component risk </li> <li>Interation between components </li> <li>Unfamiliarity with na technology used in designing the system</li> </ul> </li> <li>Team risk <ul> <li>Knowledge level and strength of team</li> <li>Dependency on external team</li> <li>Potentially disruptive team member  </li> </ul> </li> <li>Requirements</li> <li>Poorly defined requirements or poorly defined problem</li> <li>New technologies on which team member didn't worked on</li> </ul>"},{"location":"big-data/architecture/#how-to-manage-risks","title":"How to manage risks","text":"<ul> <li>Categorize risk in the architecture: break the architecture into pieces. Allows the risk of each component to be contained within that component.</li> <li>Data ingest</li> <li>Data serving </li> <li>Data processing (compute)</li> <li>Access pattern (JDBC, ODBC, REST, ...)</li> <li>Data storage </li> <li>Assign team members to work on each component</li> </ul>"},{"location":"big-data/architecture/#sql-and-nosql","title":"SQL and NOSQL","text":"<ul> <li>CAP</li> <li>Consistency: data source should provide the most recent data in every copy after successfully write </li> <li>Availability: no downtime and data system is available every time</li> <li>Partition: system should continue to serve data even system is down in some partitions</li> </ul>"},{"location":"big-data/datalake-archi/","title":"Datalake archi","text":""},{"location":"big-data/datalake-archi/#best-practice","title":"Best practice","text":"<ul> <li>Use Event-Sourcing to facilitate backups (store now, analyze later)</li> <li>Layer data based on user\u2019s skills (data analytic, engendering, ...)</li> <li>Keep the Datalake open (avoiding vendor lock-in, or overbalance on a single tool or database)</li> <li>Plan for performance</li> <li>See the link</li> </ul>"},{"location":"big-data/datalake-archi/#architecture-patterns","title":"Architecture (patterns)","text":"<ul> <li>Datalake</li> <li>Pros:<ul> <li>Store raw data </li> <li>Store different type of data (structured, semi and non-structured)</li> <li>Allows flexibly</li> </ul> </li> <li> <p>Cons: </p> <ul> <li>Multiples issues of data quality</li> </ul> </li> <li> <p>Lambda</p> </li> <li>Batch layer</li> <li>Realtime layer </li> <li> <p>Serving layer</p> </li> <li> <p>Kappa</p> </li> <li> <p>Simplify lamda architecture by fusionionning batch realtime layers </p> </li> <li> <p>Partitioning</p> </li> <li>Enables efficient data filtering and retrieval, as queries can skip irrelevant partitions during processing,     which is also referred to as data pruning.</li> <li>Choose frequently used columns </li> <li>Two types<ul> <li>Static </li> <li>Dynamic </li> </ul> </li> <li> <p>Partition size should be balanced to avoid data skew </p> </li> <li> <p>Bucketing</p> </li> <li>Bucketing improves query performance by grouping similar data together and reducing the number of files      to scan during processing </li> <li>Reduce the number of files to scan and improves data locality </li> </ul>"},{"location":"big-data/datalake-archi/#drawbacks","title":"Drawbacks","text":"<ul> <li>Do not enforce data quality </li> <li>Do not support transactions</li> <li>Lake of consistency and isolation</li> </ul>"},{"location":"big-data/datalake-archi/#example-of-data-ingestion-in-uber","title":"Example of data ingestion in Uber","text":"<ul> <li>Decouple storage from query layer, each can be scaled independently</li> <li> <p>Two dataset types</p> <ul> <li>Append-only</li> <li>Append-plus-update</li> </ul> </li> <li> <p>Challenges</p> <ul> <li>Not read the hole dataset</li> <li>Spread across multiple files within partitions to leverage high parallelism during writes and in case of files updates, limiting write footprint only to the files containing these updates</li> <li> <p>For efficient updates: lookup tool (lookup table) for location of the data with the Big data platform </p> </li> <li> <p>The indexing system that maybe used HBase or Cassandra </p> </li> <li>Hbase vs Cassandra</li> <li>HBase permits consistency on reads and writes. No need to tweak consistency parameter as is done in cassandra</li> <li>HBase provides automatic re-balancing of tables within a cluster</li> </ul> </li> </ul> <p></p> <ul> <li>See the link</li> </ul>"},{"location":"big-data/deltalake/","title":"Deltalake","text":"<ul> <li>Layers</li> <li>Consumption: BI tools, Visualization and other API</li> <li>Compute: where we compute queries</li> <li>Storage: where data is stored</li> </ul>"},{"location":"big-data/deltalake/#storage","title":"Storage","text":"<ul> <li>Datalake</li> <li>Openness </li> <li>Flexibility</li> <li>Because of the flexibility we have in Datalake this leads to a lot of Data quality issues </li> <li> <p>Datalake operates in file level  </p> </li> <li> <p>Data-warehouse</p> </li> <li>Closed, propriety format</li> <li>Offers a good data quality </li> <li>Cons<ul> <li>Limited to structured data</li> <li>Performance issues </li> </ul> </li> <li>Deltalake</li> <li>Get version data</li> </ul>"},{"location":"big-data/file-fomats/","title":"File fomats","text":""},{"location":"big-data/file-fomats/#parquet","title":"Parquet","text":"<ul> <li>Columnar oriented</li> <li>Self-describing: metadata including schema and structure is embedded within a file</li> <li>Advantages:</li> <li>Compression</li> <li>Encoding</li> <li>Optimize I/O</li> </ul>"},{"location":"big-data/file-fomats/#avro","title":"Avro","text":"<ul> <li>Row based</li> <li>Schema-based serialization library</li> <li>Uses JSON to specify data format</li> <li>Keeps data along with a schema in its metadata section</li> <li>Supports two type of serialization</li> <li>JSON format</li> <li>Binary format</li> </ul>"},{"location":"big-data/hdfs/","title":"Hdfs","text":"<ul> <li>Namenode <ul> <li>Open, Close, Rename, Delete operations on files and directories </li> <li>Determines the mapping between blocks and databnode</li> </ul> </li> <li> <p>Datanode</p> <ul> <li>Read, Write</li> <li>Block creation, replication and deletion</li> </ul> </li> <li> <p>Replica selection: in order to optimize bandwith and latency hdfs always read from the replica closest to reader </p> <ul> <li>If replica exist in the same rack read it </li> <li>if cluster is spans  on multiple data center read replica from local data center</li> </ul> </li> <li> <p>SafeMode: at start up Name node enter in safemode state (No replication is accepted at this state) </p> <ul> <li>Datanode sends Block report (list of data blocks that it holds) </li> <li>Block is said in safemode if the minimum block of replicas is checked by Namenode</li> <li>Once all block are check Name Node exist the safemode state </li> <li>If any block doesn't satisfy the nombre of replica, Name node will replicate </li> </ul> </li> <li> <p>EditLog </p> <ul> <li>Any action in FS is stored in edit log (delete a file, change file replica number, ...) </li> </ul> </li> <li> <p>FsImage: it stores</p> <ul> <li>All FS namespace </li> <li>Mapping of blocks to files </li> <li>FS properties</li> </ul> </li> <li> <p>Checkpoint: </p> <ul> <li>Are occurred <ul> <li>After certain time period (dfs.namenode.checkpoint.period) </li> <li>Give number of the transactions on FS have accumulated (dfs.namenode.checkpoint.txns) </li> </ul> </li> </ul> </li> <li> <p>Checksum</p> <ul> <li>When data is written or read</li> <li>The datanodes are responsible for verifying the data they receive before storing the data and its checksum</li> <li>HDFS run DataBlockScanner (n a background thread that periodically verifies all the blocks stored on the datanode)   This is to guard against corruption due to bit rot in the physical storage media </li> </ul> </li> <li> <p>Re-replication: it may occur when</p> <ul> <li>Datanode become unavailable </li> <li>Replica is corrupted </li> <li>Datanode hard disk fail </li> <li>Replication factor is decreased </li> </ul> </li> <li> <p>Trash </p> <ul> <li>When files are deleted they are moved to <code>/user/&lt;username&gt;/.Trash</code></li> <li>How to configure it ???</li> </ul> </li> <li> <p>Backup and checkpoint nodes </p> <ul> <li>dfs.namenode.backup.address</li> <li>dfs.namenode.backup.http-address</li> </ul> </li> </ul>"},{"location":"big-data/hdfs/#block","title":"Block","text":""},{"location":"big-data/hdfs/#utility-of-blocks","title":"Utility of blocks","text":"<ul> <li>Estimate the size easily</li> <li>Separate metadata from file itself</li> <li>Allows replication for providing fault-tolerant and availability</li> <li>Insure against corrupted blocks and disk and machine failures</li> </ul>"},{"location":"big-data/hdfs/#block-caching","title":"Block caching","text":"<ul> <li>cache directive</li> <li>cache pool </li> </ul>"},{"location":"big-data/hdfs/#ha","title":"HA","text":"<ul> <li>Solve Single Point Of Failure (SPOF) </li> <li>Two or more Namenodes<ul> <li>Active</li> <li>StandBy</li> </ul> </li> <li>To keep the state of the cluster synchronized, Namenodes use separate group of daemons called JournalNodes   to read all edit logs from active Namenode</li> <li>To set HA add options in hdfs-site.xml</li> <li>The number of JournalNodes must be at least 3  </li> <li>Minimum number of HA nodes is 2. It's suggested to not exceed 5 - with a recommended 3 NameNodes - due to communication overheads  </li> </ul>"},{"location":"big-data/hdfs/#hdfs-federation","title":"HDFS Federation","text":"<ul> <li>Uses multiple Namenodes/namespaces</li> <li>Namenodes are independents and don't need coordination</li> <li>Datanodes are used as common storage for blocks by all Namenodes </li> </ul>"},{"location":"big-data/hdfs/#common-problems","title":"Common problems","text":"<ul> <li>A massive amount of small files in hdfs =&gt; More pressure on Namenode =&gt; Slow latency</li> <li>Split brain scenario  </li> <li>Hadoop and parquet don't support updates</li> </ul>"},{"location":"big-data/to-read/","title":"To read","text":"<ul> <li>https://eng.uber.com/uber-big-data-platform</li> <li>https://eng.uber.com/presto</li> </ul>"},{"location":"big-data/airflow/","title":"Hello word example in Apache Airflow","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\n# [1. Define default args]\ndefault_args = {\n    'owner': 'massi',\n    'depends_on_past': False,\n    'start_date': datetime(2019, 10, 26),\n    'email': ['kerrache.massipssa@gmail.com'],\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1),\n}\n\n# [2. Define dag args]\nwith DAG(\n    'hello_dag',\n    default_args=default_args,\n    schedule_interval=\"2 * * * *\"\n) as dag:\n\n    # [3. Define Dag's tasks]\n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='echo \"Hello World from Task 1\"',\n        dag=dag)\n\n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='echo \"Hello World from Task 2\"',\n        dag=dag)\n\n    # [4. Set task dependencies]\n    task_1 &gt;&gt; task_2\n</code></pre>"},{"location":"big-data/airflow/#validate-a-dag","title":"Validate a DAG","text":"<p>Before running a DAG, execute your file to check if it contains errors.  <pre><code>python helloworld_dag.py\n</code></pre></p>"},{"location":"big-data/airflow/#test-task","title":"Test task","text":"<pre><code>airflow test helloworld_dag task_1 2019-10-26\n</code></pre>"},{"location":"big-data/airflow/#airflow-in-distributed-mode","title":"Airflow in distributed mode","text":"<ol> <li>Scheduler creates new task instance in metadata store with status SCHEDULED  </li> <li>Scheduler uses Celery Executor to send a task to messages broker (RabbitMQ) </li> <li>The Cerely worker receives the command from the queue</li> <li>The Cerely worker updates the status of task in metadata store to RUNNING</li> <li>The Cerely worker executes task command</li> <li>Once the task is finished, the celery worker updates the metadata to set the status of the task instance to SUCCESS</li> </ol> <p>For more info see  How Apache Airflow Distributes Jobs on Celery workers</p> <ul> <li>Celery: task queue </li> </ul>"},{"location":"big-data/airflow/concepts/","title":"Concepts","text":"<ul> <li>Best practices</li> <li>Task should be atomic (do only one thing and independent from other tasks)</li> <li></li> </ul>"},{"location":"big-data/beam/concepts/","title":"Concepts","text":"<ul> <li>Is unified programming model for batching and streaming</li> <li>Provide a portable programming layer</li> <li>Separates pipeline structure from execution platform (Spark, Dataflow, Flink, ...)</li> </ul>"},{"location":"big-data/beam/concepts/#components","title":"Components","text":"<ul> <li>PCollection dataset of bounded or unbounded items </li> <li>Has a schema</li> <li>Immutable (like RDD in spark)</li> <li>Each item is assigned a timestamp by a source that creates it</li> <li>PTransformation</li> <li>Acts on PCollection, create zero or many new PCollections</li> <li>Pipeline </li> <li>Acyclic graph of PCollection and PTransformation</li> <li>Different pipeline cannot share a PCollection</li> <li>PipelineRunner </li> </ul>"},{"location":"big-data/beam/concepts/#beam-model","title":"Beam Model","text":"<ul> <li>What is beign computed ?</li> <li>Here we compute Sum per key:     <code>PCollection&lt;KV&lt;String, Integer&gt;&gt; scores = input.apply(Sum.integersPerKeyy())</code></li> <li>Where in event time ?</li> <li>Window each two minutes we receive (we compute) an event  --&gt;      <pre><code>  PCollection&lt;KV&lt;String, Integer&gt;&gt; scores = input\n             .apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))\n             .apply(Sum.integersPerKeyy())\n</code></pre></li> <li>Where in processing time ? </li> <li>How refinement relate ? </li> </ul>"},{"location":"big-data/beam/concepts/#windowing","title":"Windowing","text":"<ul> <li>Each element of PCollection may belong to one or more windows</li> <li>Each individual window contains an infinite elements</li> <li>Watermark </li> <li>Event-time: where event occured (timestamp inside event)</li> <li>Proccessing-time: where event was processed by the pipeline</li> </ul>"},{"location":"big-data/beam/concepts/#triggers","title":"Triggers","text":"<ul> <li>Emitting after a certain amount of time elapses, or after a certain number of elements arrives </li> <li>Processing of late data by triggering after the event time watermark passes the end of the window</li> </ul>"},{"location":"big-data/flink/concepts/","title":"Flink","text":"<ul> <li>Job </li> <li>Job Graph </li> <li>Operator: transform event stream</li> <li>Operator operations: </li> <li>Parallel </li> <li>Forward </li> <li>Repartition </li> <li>Rebalance</li> </ul>"},{"location":"big-data/iceberg/architecture/","title":"Apache Iceberg Architechture","text":""},{"location":"big-data/iceberg/architecture/#layers","title":"Layers","text":"<p>Apache iceberg is made up by a three layer</p>"},{"location":"big-data/iceberg/architecture/#catalog-layer","title":"Catalog Layer","text":"<p>The data layer of an Apache Iceberg table stores the actual table data, consisting mainly of \\textcolor{blue}{data files}  and \\textcolor{blue}{delete files}. It is responsible for supplying query results, except in certain cases where metadata can answer queries (e.g., max value lookups). In Iceberg\u2019s tree structure, the data layer forms the leaves.</p>"},{"location":"big-data/iceberg/architecture/#metadata-layer","title":"Metadata Layer","text":"<p>It's a layer that contains all metadata files for an table in Iceberg. It's a tree that tracks the datafiles and metadata about them as well as the oprations that resulted in the their creation. It's made up of three file types: manifest files, manifest lists and metadata files.\u00a0 This layer is essential for effenciently managing large datasets and enable core features such as time travel and schema evolution.\u00a0 Manifest file: keep track of file in the data layer (datafiles and delete files) as well as some aditionnal details and statistics about each files such as minimum and maximum values of datafiles's columns. Note: separate subsebt of manifest files are used to track datafiles and delete file but the schema is identical same for both.</p> <p>Information from the manifest files, such as upper and lower bounds for a specific column, null value counts, and partition-specific data, is used by the engine for file pruning. Manifest list: is a snapshot of Iceberg table at a given point in time. It contains a list of manifest file, including the location, the partitions it belongs to, and the upper and lower bounds for partition columns for the datafiles it tracks.\u00a0</p> <p>Query engines interact with the manifest lists to get information about partition specifications that help them skip the nonrequired manifest files for faster performance. Metadata file: it tracks manifest list files. It includes table's schema, partition information, snapshots, and which snapshot is current one. Each time a change is made to an Iceberg table, a metadata file is created and is registred as last version of the metadata file, it helps during scenarios such as:</p> <p>Concurrent writes (i.e., multiple engines writing data simultaneously). Also, during read operations, engines will always see the latest version of the table.</p>"},{"location":"big-data/iceberg/architecture/#data-layer","title":"Data Layer","text":"<p>The layer where the data that be quyered is stored be the user. It's made up by the datafiles and deleted files (data deleted) and is backed by a distibuted system such as Hadoop, Amazon Simple Storage (S3), Google Cloud Storage (GCS) and Azure Data Lake Storage (ADLS), etc.\u00a0 It's file agnostic and at time of this writing three formats are supported by Apache Iceberg, Apache Paraquet, Apache Avro and Apache ORC.\u00a0 Deleted files: it's a best practice to keep data in data lake immutable, so when you can't update rows in file in place. Instead you need to write a new file. Two ways are available in Apache Iceberg Copy-On-Write (COW): the new file is a copy of the old file with the changfes reflected in a new copy of it Merge-On-Read (MOR): a new file that only has the changes written, which engines reading the data then coalesce</p> <p>MOR delete methods Positional delete file Eqaulity delete file</p>"},{"location":"big-data/iceberg/optimization/","title":"Optimization","text":""},{"location":"big-data/iceberg/optimization/#compaction","title":"Compaction","text":"<p>Each operation on file genererate metadata file this can lead to small files problem, espcially when delaing with stream.  The solution is to periodically rewirte sa small files into large one, this process is called cwcxompation. </p>"},{"location":"big-data/iceberg/read-write/","title":"Read-Write","text":""},{"location":"big-data/iceberg/read-write/#writing-query-in-iceberg","title":"Writing Query in Iceberg","text":"<p>Writing process in Iceberg goes through a series of steps that help query engines to efficiently insert and update data.</p> <ol> <li>Send the query to the engine </li> <li>Get the latest metadata file </li> <li>Write data to datafiles </li> <li>Create Manifest file </li> <li>Create Manifest list </li> <li>Write metadata file </li> <li>Update latest metadata file</li> </ol>"},{"location":"big-data/iceberg/read-write/#reading-query-iniceberg","title":"Reading Query inIceberg","text":"<p>The query engine interacts with the catalog to get the current metadata file.</p> <p>It then gets the current-snapshot-id (S2 in this case) and the manifest list location for that snapshot. The manifest file path is then retrieved from the manifest list. The engine determines the datafile path based on the partition filter from the manifest file. The matching data from the required datafile is then returned to the user.</p>"},{"location":"big-data/kafka/Tools/","title":"Tools","text":""},{"location":"big-data/kafka/Tools/#kafka-tools","title":"Kafka tools","text":"<ul> <li> <p>kafka-topics.sh : create, modify, delete and list the topics </p> </li> <li> <p>create</p> </li> </ul> <pre><code>kafka-topics.sh --zookeeper host.zookeeper:2181 \\\n    --create \\\n    --topic topic-name \\\n    --replication-factor 3 \\\n    --partitions 8 \\\n    --if-not-exists \\ # to avoid warn if topic exists\n</code></pre> <ol> <li>alter (increase partitions number)</li> </ol> <pre><code>kafka-topics.sh --zookeeper host.zookeeper:2181 \\\n    --alter \\\n    --topic topic-name \\\n    --partitions 16 \\\n</code></pre> <ol> <li>delete</li> </ol> <p>The option delete.topics.enable must be set to true in broker properties</p> <pre><code>kafka-topics.sh --zookeeper host.zookeeper:2181 \\\n    --delete \\\n    --topic topic-name\n</code></pre> <ul> <li> <p>kafka-consumer-groups.sh: lists and describes consumer groups</p> </li> <li> <p>For old groups, zookeeper is used to store them</p> </li> <li>Newer topics are stored in the Broker</li> </ul> <pre><code>kafka-consumer-groups.sh --list --bootstrap-server kafka-server:9092\n</code></pre> <ul> <li> <p>kafka-acls.sh: add, remove or list ACLs</p> </li> <li> <p>kafka-configs.sh: set and override topic's configurations</p> </li> <li> <p>Topic config:</p> </li> </ul> <pre><code>kafka-config.sh --zookeeper localhost:2181 \\\n    --alter \\\n    --entity-type topics \\\n    --entity-name &lt;topic-name&gt; \\\n    --add-config &lt;key&gt;=&lt;value&gt; [,&lt;key&gt;=&lt;value&gt;]\n</code></pre>"},{"location":"big-data/kafka/acls/","title":"ACLs","text":"<ul> <li>Kafka uses Zookeeper to store ACLs </li> <li>If no ACL is defined no user is authozided to access resource, except super user</li> <li>AdminClient API can be used to manage ACLs </li> <li>ACLs are stored in Zookeeper and then propaged to broker there may be delay before take effect  </li> </ul> <p>--transactional-id ??  --idempotent ?? </p> <ul> <li>Principal (Allow/Denied) Operation From Host on Resource matches ResourcesPattern </li> </ul>"},{"location":"big-data/kafka/concepts/","title":"Concepts","text":"<ul> <li>Partitions of topic can be stored in different servers (scaled horizontally)</li> <li>Consumer read a message in order they arrive</li> </ul>"},{"location":"big-data/kafka/concepts/#broker","title":"Broker","text":"<ul> <li>Knows all about topics and partitions metadata </li> <li>Is bootstrap broker </li> <li>Stores some topics and partitions </li> <li>Receive messages from Producer </li> <li>Assign offsets to messages </li> <li>Commit messages to disk </li> <li>Responds to consumers with messages</li> </ul>"},{"location":"big-data/kafka/concepts/#controller","title":"Controller","text":"<ul> <li>Elected automatically</li> <li>Responsible for administrative operations (Assign partitions to brokers and monitoring for broker failures)</li> </ul>"},{"location":"big-data/kafka/concepts/#leader","title":"Leader","text":"<ul> <li>The owner of partition</li> </ul>"},{"location":"big-data/kafka/concepts/#partition","title":"Partition","text":"<ul> <li>Can not be deleted once is created</li> <li>Order is guaranteed within the partition, and once data is wrote to partition is immutable</li> <li>Key used:</li> <li>As additional information that gets stored with the message</li> <li>To decide to which one of the topic partitions the message will be writen to. All the message with the same key    will go to same partition</li> </ul>"},{"location":"big-data/kafka/concepts/#retention","title":"Retention","text":"<ul> <li>Can be the number of days</li> <li>Or when topic reaches certain size in bytes  </li> <li>Individual topic can have its own retention</li> <li>Topic can also be configured with log compacted that means Kafka will retain only the last messages with some keys</li> </ul>"},{"location":"big-data/kafka/concepts/#consumer","title":"Consumer","text":"<ul> <li>The way to scale data consumption is by adding more consumer to consumer group</li> <li>Coordinator group: broker responsible to manage consumer of consumer group</li> <li>To join coordinator group, the consumer sends JoinGroup request</li> <li>The first consumer who joined the group become the leader of the group</li> <li>The leader is responsible to assign partitions to consumer</li> <li>Leader uses Range or RoundRobin policy to assign partition to the consumer</li> <li>KafkaConsumer is not thread safe, it needs to be executed in one thread</li> </ul>"},{"location":"big-data/kafka/concepts/#partition-reassignment","title":"Partition reassignment","text":"<ul> <li>It happens when</li> <li>New consumer is added to consumer group</li> <li>Consumer shuts down or crashes</li> <li>The topics the consumers are consuming is modified (e.g. administrator add new partition to topic)</li> <li>During re-balance a consumer cannot consume, they should wait until re-balance is finished</li> </ul>"},{"location":"big-data/kafka/concepts/#configuration-parameters","title":"Configuration parameters","text":"<ul> <li>auto.offset.reset</li> <li>latest: is the default (records written after consumer were started)</li> <li>earliest: read the data starting from the very beginning</li> <li>none</li> <li>enable.auto.commit: can be true or false. Allows Kafka to commit automatically the offset it had consumed</li> </ul>"},{"location":"big-data/kafka/concepts/#offset-commit","title":"Offset commit","text":"<ul> <li>Kafka does not track acknowledgments from consumers the way many JMS queues do</li> <li>It allows consumers to use Kafka to track their position (offset) in each partition</li> <li> <p>Consumer commit offset by producing a message to Kafka to topic __consumer_offsets</p> </li> <li> <p>Commit type</p> </li> <li>Auto commit: <code>enable.auto.commit=true</code></li> <li>Sync commit:<ul> <li><code>commitSync()</code> API commit the last offset return by the <code>poll()</code> and throws an exception <code>CommitedFailException</code> if the commit fails for some reason</li> <li>Drawback:</li> <li>It blocks until the broker responds to the commit request</li> <li>Will retry until the commit succeeded or encounter  non-retriable error</li> </ul> </li> <li> <p>Async commit:</p> <ul> <li><code>commitSync()</code> API  sends request and continue, don't wait for the broker response</li> <li>It does not retry</li> </ul> </li> <li> <p><code>subscribe()</code>: leverage consumer group mechanism</p> </li> <li><code>assign()</code>: used to assign partitions manually to consumer</li> <li>subscribe and assign can't  be called by the same confuser</li> <li><code>close()</code>: on consumer immediately triggers a partition re-balance as the consumer will not be available anymore</li> </ul>"},{"location":"big-data/kafka/concepts/#producer","title":"Producer","text":"<ul> <li>KafkaProducer is thread safe </li> </ul>"},{"location":"big-data/kafka/concepts/#consuming-strategies","title":"Consuming strategies","text":"<ul> <li>At-most-once: never duplicate messages, but can miss ones</li> <li>At-last-once: can duplicate messages but can't miss ones</li> <li>Exactly-once: deliver all messages without duplication</li> </ul>"},{"location":"big-data/kafka/concepts/#links","title":"Links","text":"<p>http://lahotisolutions.blogspot.com/2019/03/apache-kafka-notes.html</p> <p>livelock : application did not crash but fails to make progress for some reason</p>"},{"location":"big-data/kafka/cross-cluster/","title":"Cross cluster","text":"<ul> <li>Mirror: copying data between Kafka clusters</li> <li>MirrorMaker: built-in-cross-cluster replicator in Kafka</li> <li>Useful: </li> <li>Multiple data centers</li> <li>Redundancy</li> <li> <p>Cloud migrations</p> </li> <li> <p>Three difficulties should be considered when we have to deal with cross-cluster </p> </li> <li>High latencies</li> <li>Limited bandwidth</li> <li>Higer costs</li> </ul>"},{"location":"big-data/kafka/install-kafka-cluster/","title":"Install kafka cluster","text":"<ol> <li>Download zookeeper</li> </ol> <p>wget https://downloads.apache.org/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz</p>"},{"location":"big-data/kafka/internals/","title":"Internals","text":""},{"location":"big-data/kafka/internals/#record","title":"Record","text":"<p>It consists of three parts:</p> <ul> <li>Key</li> <li>Value</li> <li>Timestamp</li> </ul> <p>Default size is 1 MB</p>"},{"location":"big-data/kafka/internals/#broker","title":"Broker","text":"<ul> <li>Every broker has a unique id (set in config file or auto generated)</li> <li>Broker's metadata are stored within zookeeper in the znode /brokers/ids/broker-id</li> <li>If the broker is gone its ID will still exist in other data structure. Like this, if we replace the broker   with another one having the same ID, it'll immediately join the cluster and with same partitions and topics assigned to it</li> </ul>"},{"location":"big-data/kafka/internals/#controller","title":"Controller","text":"<ul> <li>Is responsible to elect partitions leader</li> <li>The first broker that starts in cluster becomes the controller</li> <li>It creates ephemeral node /controller in zookeeper</li> </ul>"},{"location":"big-data/kafka/internals/#replication","title":"Replication","text":"<ul> <li>Leader replica: single replica with leader node</li> <li>Producers and consumers use this replica for read and write operations</li> <li> <p>Responsible for knowing which followers are up-to-date with the leader</p> </li> <li> <p>Follower replica:</p> </li> <li>Used in case the leader craches</li> <li>Replicate messages from the leader</li> <li>Stay up-to-date with the recent message</li> <li>It may be in sync or out of sync</li> <li>Only in sync followers are eligible to become leader in case of crach</li> <li> <p>replica.lag.time.max.ms the amout of time after which the follower is considered in out of sync</p> </li> <li> <p>Preferred Leader</p> </li> <li>The replica that was the leader of the partition when the topic was originally created</li> <li>auto.leader.rebalance.enable=true will check if the current leader is not the preferred leader but it's in sync and trigger the leader election to make the preferred leader the current leader</li> </ul>"},{"location":"big-data/kafka/internals/#request-processing","title":"Request processing","text":"<ul> <li>Metadata request</li> <li>Sent by client with the list of topics in which is interested</li> <li>Can be sent to any broker</li> <li>Server will respond with:<ul> <li>List of partitions the topic contains</li> <li>List of replicas for each partition</li> <li>Leader replica for each partition</li> </ul> </li> <li>Client will cache the information</li> <li> <p>metadata.cache.max.ms refresh interval of the metadata in the cache</p> </li> <li> <p>Produce request After receiving the request, the lead replica will:</p> </li> <li> <p>Check if the user who trying to write has the required privileges</p> </li> <li>The acks is valid (0, 1 or all)</li> <li>If acks=all, is there enough in sync replicas</li> </ul>"},{"location":"big-data/kafka/internals/#fetch-request","title":"Fetch request","text":"<ul> <li>It's important to set the limit of the data the consumer can handle per partition, because large amount of data can lead to out of memory problems</li> <li>zero copy send data directly from the filesystem to network channel without any intermediate buffers</li> <li>Set upper and lower bound of data to read, is greate way to reduce CPU and network utilization</li> </ul>"},{"location":"big-data/kafka/internals/#physical-storage","title":"Physical storage","text":"<ul> <li>JBOD, RAID</li> <li>Unit storage is partition replica</li> <li>Partition cannot be split between disk</li> <li>Partition's size is limited by available size in disk</li> <li>log.dirs: file where partitions are stored</li> <li>Retention</li> <li>Time or size</li> </ul>"},{"location":"big-data/kafka/internals/#file-format","title":"File format","text":"<ul> <li>The format is the same used by producer &lt;-&gt; borker and broker &lt;-&gt; consumer. This avoid any compression and decompression and allows the zero copy optimization</li> <li>File contains</li> <li>Key, value and offset of the message</li> <li>Message size</li> <li>Magic byte:  indicates the version of the message format</li> <li>Codec compression (Snappy, GZip, LZ4)</li> <li>Tool DumpLogSegment : allows to examine partition segement in filesystem     <code>./bin/kafka-run-class.sh kafka.tools.DumpLogSegments</code></li> </ul>"},{"location":"big-data/kafka/internals/#index","title":"Index","text":"<ul> <li>Maps offset to segment files and position within the file</li> <li>They are broken into segments , they can be deleted when messages are purged</li> <li>Partition has one segment and two indexes files</li> </ul>"},{"location":"big-data/kafka/internals/#compaction","title":"Compaction","text":"<ul> <li>Keep only the latest message value</li> <li>If the key of message is null the compaction will fail</li> <li>Message having null values are deleted</li> </ul>"},{"location":"big-data/kafka/ksql/","title":"KSQL","text":"<ul> <li>KsqlDB: streaming database</li> <li>Components:</li> <li>ksqldb</li> <li>ksqldb-server</li> <li> <p>ksql-cli</p> </li> <li> <p>Default port 8088</p> </li> <li> <p>Run ksql-cli in docker <code>docker exec -ti ksql-cli ksql http://ksql-server:8088</code></p> </li> <li> <p>The result of an aggregate query is always a Table</p> </li> <li>Supports exactly-once processing</li> <li>Accepts: SSL, SASL, ACLs managed by Kafka, Schema Registry</li> <li> <p>Uses Admin Client, if topic auto creation is disableed and don't exist, Ksql will create the topic  </p> </li> <li> <p>Metadata</p> </li> <li>They are stroed internal topic</li> <li>To secure them, we must secure the topic</li> <li>Data Definition Langue (DDL):</li> <li>They write data to metedata</li> <li>Data Manipulation Language (DML): arn't available</li> </ul>"},{"location":"big-data/kafka/questions/","title":"Questions","text":"<ul> <li> <p>min.insync.replicas ?</p> </li> <li> <p>is KSQL ANSI SQL compliant ?</p> </li> <li> <p>peek transformation ?</p> </li> <li> <p>unclean.leader.election.enable ?</p> </li> </ul> <p>Setting unclean.leader.election.enable to true means we allow out-of-sync replicas to become leaders, we will lose messages when this occurs, effectively losing credit card payments and making our customers very angry</p> <ul> <li> <p>output of KStream-KTable join ?</p> </li> <li> <p>max.tasks in kafka connect ?</p> </li> <li> <p>exactly once guarantee ?</p> </li> <li>Using idempotence and the transaction from producer side</li> <li> <p>isolation level from consumer side</p> </li> <li> <p>max.in.flight.requests.per.connection ?</p> </li> </ul>"},{"location":"big-data/kafka/questions/#stream","title":"Stream","text":"<ul> <li> <p>num.streams.threads ?</p> </li> <li> <p>cleanup.policy ?</p> </li> <li> <p>num.standby.replicas ?</p> </li> </ul> <p>See the full list of errors and their \"retriable\" status here: https://kafka.apache.org/protocol#protocol_error_codes</p> <ul> <li>Why Kafka ?</li> <li>No serialization and deserialization </li> <li> <p>Zero copy</p> </li> <li> <p>Kafka is more than messaging system </p> </li> <li>Publish / Subscribe mechanism </li> <li>Storage in distributed way (replication and data recovery)</li> <li>Data processing </li> </ul>"},{"location":"big-data/kafka/rest-proxy/","title":"REST Proxy","text":"<p>Allows to: - Produce and consume messages - View the state of the cluster - Perform administrative operations (without using Kafka protocol)</p>"},{"location":"big-data/kafka/rest-proxy/#apis-examples","title":"APIs examples","text":"<ul> <li>Get all topics <code>curl --silent  http://localhost:8082/topics | jq .</code></li> </ul>"},{"location":"big-data/kafka/schema-registry/","title":"Schema Registry","text":"<ul> <li> <p>The main values:</p> <ul> <li>Rgister schema</li> <li>Enable schema evolution</li> </ul> </li> <li> <p>Schema are managed in subject</p> <ul> <li>Ex: <code>http://localhost:8081/subjects/&lt;topicname-value&gt;</code></li> </ul> </li> <li> <p>Producer only contacts Schema Registry on the first schema write</p> </li> <li> <p>Consumer only contacts Schema Registry on the first schema id read</p> </li> <li> <p>Client applications have the ability to registry the schema autmatically when they write to topic, this is convinient for developpement but to be disabled for production</p> <ul> <li><code>auto.register.schemas=false</code></li> </ul> </li> </ul>"},{"location":"big-data/kafka/schema-registry/#compatibility-mode","title":"Compatibility Mode","text":"<ul> <li>Backward</li> <li>Backward_transitive</li> <li>Forward</li> <li>Forward_transitive</li> <li>Full</li> <li>Full_transitive</li> <li>None</li> </ul>"},{"location":"big-data/kafka/schema-registry/#apis","title":"APIs:","text":"<ul> <li> <p>Subject</p> <ul> <li>GET all subjects: <code>http://localhost:8081/subjects/</code></li> <li>GET all version of subject: <code>http://localhost:8081/subjects/&lt;topicname-value&gt;/versions</code></li> <li>GET specfic version of subject: <code>http://localhost:8081/subjects/&lt;topicname-name&gt;/versions/&lt;version-id&gt;</code></li> </ul> </li> <li> <p>Config</p> <ul> <li><code>http://localhost:8081/config/&lt;topicname-value&gt;</code></li> </ul> </li> </ul>"},{"location":"big-data/kafka/security/","title":"Security","text":"<ul> <li>Data encyption in-flight with SSL</li> <li>Authentication using SSL or SASL</li> <li>Authorization using ACLs</li> </ul>"},{"location":"big-data/kafka/security/#security-models","title":"Security models","text":"<ul> <li> <p>PLAINTEXT</p> <ul> <li>Is the default model</li> <li>No authentication, no authorization and insecure channel</li> <li>To be used only for PoC (not for DEV/PPRDO or PROD)</li> </ul> </li> <li> <p>SSL</p> <ul> <li>Uses X.509 certificate</li> <li>No authentication</li> <li>Need to have keystore/truststore for broker and client</li> <li>Broker side:<ul> <li><code>listener=SSL://host_ip:6667</code></li> <li><code>inter.broker.protocol=SSL</code></li> </ul> </li> <li>Client side:<ul> <li><code>security.protocol=SSL</code></li> </ul> </li> </ul> </li> <li> <p>SASL_PLAINTEXT</p> <ul> <li>Uses:<ul> <li>Username / Password authentication</li> <li>SCRAM (Salted password)</li> <li>GSSAPI (Kerberos)</li> </ul> </li> <li>Supports User Authorization via Kafka ACLs or Apache Ranger</li> <li>Sends secrets and data over wire Plain format</li> <li>Broker side:<ul> <li><code>listener=SASL_PLAINTEXT://host_ip:6667</code></li> <li><code>inter.broker.protocol=SASL_PLAINTEXT</code></li> <li><code>sasl.enabled.mechanism=PLAIN | SCRAM | GSSAPI</code></li> </ul> </li> <li>Client side:<ul> <li><code>security.protocol=SASL_PLAINTEXT</code></li> <li><code>saslmechanism=PLAIN | GSSAPI | SCRAM-SHA-256 | SCRAM-SHA-512</code></li> </ul> </li> </ul> </li> <li>SASL_SSL<ul> <li>Same thing as SASL_PLAINTEXT but:<ul> <li>Sends secrets and data over wire Encrypted format</li> <li>Needs keystore/truststore for brokers and clients</li> </ul> </li> </ul> </li> </ul>"},{"location":"big-data/kafka/security/#authentication-with-sasl-simple-authorization-service-layer","title":"Authentication with SASL (Simple Authorization Service Layer)","text":"<p>export KAFKA_OPTS=\"-Djava.security.auth.login.config=/home/massi/auth/kafka_server_jaas.conf\"</p>"},{"location":"big-data/kafka/stream/","title":"Kafka streams","text":"<ul> <li>Record: is key-value pair</li> <li>Stream partitions</li> <li> <p>Tasks</p> </li> <li> <p>Consume -&gt; Process -&gt; Produce</p> </li> <li>Data stream: unbounded dataset (unfinite or ever growing)</li> <li>Envent streams features:<ul> <li>They are ordred (card ex : put money, send, recover...)</li> <li>Immutable data record (ex: fincancial transaction can not be modfied)</li> <li>Are relayable</li> <li>Fault-tolerant</li> </ul> </li> <li>Three paradigms:<ul> <li>Request-Response (online trasaction processing OLAP)</li> <li>Bash-processing</li> <li>Stream-processing</li> </ul> </li> </ul>"},{"location":"big-data/kafka/stream/#concepts","title":"Concepts","text":""},{"location":"big-data/kafka/stream/#time","title":"Time","text":"<ul> <li>Event time: time where event occured. Kafka add automatically the current time to producer record</li> <li>Log append time: time event arrived to Kafka borker</li> <li> <p>Processing time:</p> <ul> <li>Time at which sytem processing received data to perform some calculation</li> <li>Two threads of same application can have different processing time</li> <li>Higly unreliable and best avoided</li> <li>PS: when usign time, time zone should be standardized over the application</li> </ul> </li> <li> <p>Time windows: operations happen on slices of time</p> </li> </ul>"},{"location":"big-data/kafka/stream/#state","title":"State","text":"<ul> <li>Information stored between events</li> <li>Can be Local or internal state: accesible only by steaming application instance</li> <li> <p>External state (database, cache system,...)</p> </li> <li> <p>Design pattern</p> <ul> <li>Single event processing</li> </ul> </li> <li> <p>Two APIs</p> <ul> <li>Low level Processor</li> <li>High Level Stream DSL</li> </ul> </li> </ul> <p>1- StreamBuilder: create topology (DAG), which is a serie of transformations 2- KafkaStream: execute object from topology. will start multiple threads,each applying the processing topology to events in the stream</p> <ul> <li> <p>Stream:</p> <ul> <li>Uunbouded, countinous real-time flow of records</li> <li>Like topic, consists of one or many partitions</li> </ul> </li> <li> <p>Tolology: processing logic where</p> <ul> <li>Node: Stream processor</li> <li>Edge: stream</li> </ul> </li> <li> <p>State:</p> <ul> <li>Stateless:<ul> <li>Independent from the processing of the other message</li> <li>Don't require state store</li> </ul> </li> <li>Stateful:<ul> <li>Depends from other messages, its used when we need to join, aggregate or window input data</li> <li>It requires state store</li> </ul> </li> </ul> </li> <li> <p>State Store</p> <ul> <li>State store are fault-tolerent, kafka will restore all state store prior to resuming</li> <li>Used to store and query the data</li> </ul> </li> <li> <p>KStream:</p> <ul> <li>Insert only</li> </ul> </li> <li> <p>KTable:</p> <ul> <li>Upsert (insert and update it the record exists)</li> <li>Key when value are null, those key are deleted</li> <li>Read from a subset topic's partitions</li> </ul> </li> <li> <p>GlobalKTable</p> <ul> <li>Upsert mode</li> <li>Reads from all topic's partitions</li> </ul> </li> <li> <p>Stream application do not run inside the broker, it runs on the client JVM</p> </li> <li>We can run many instances of an application</li> <li>Processor topology: the strategy which allows to compute data (is graph)</li> </ul>"},{"location":"big-data/kafka/stream/#windowing","title":"Windowing","text":"<ul> <li>Grace perdiod: indicates when the window result is final</li> <li> <p>Out-of-order: if a record arrive after grace period (record.ts &gt; window.end.time + grace.period) it'll discared and not processed</p> </li> <li> <p>Window types </p> </li> <li>Tumbling</li> <li>Hopping</li> <li>Sliding</li> <li>Session</li> <li>Prime use area is user behavior analysis</li> </ul>"},{"location":"big-data/kafka/stream/#processing-grantee","title":"Processing grantee","text":"<ul> <li>At-least-once: records are nerver lost but may be delivred</li> <li>Exactly-once:: records are processed only once</li> </ul> <p>https://kafka-tutorials.confluent.io/changing-serialization-format/kstreams.html</p>"},{"location":"big-data/kafka/connect/connect/","title":"Kafka Connect","text":"<ul> <li>When building data pipelines:</li> <li>Timeliness</li> <li>Reliability</li> <li>Default port in distributed mode</li> </ul>"},{"location":"big-data/kafka/connect/connect/#components","title":"Components","text":"<ul> <li>Connector</li> <li>Defines how data will be copied</li> <li>They perform the copy of the data using jobs by breaking the job into a set of Tasks</li> <li>Two types of connectors:<ul> <li>Source connector: push data to Kafka topic</li> <li>Sink connector: pull data from kafka</li> </ul> </li> <li> <p>Is responsible for three things:</p> <ul> <li>How many tasks to run for the connector</li> <li>How to split data-copying between tasks</li> <li>Getting configurations of tasks from the workers and pass it along</li> </ul> </li> <li> <p>Tasks</p> <ul> <li>Responsible for getting data in and out of Kafka</li> <li>They are initialized by receiving a context from the connector (Source or Sink context)</li> <li>Task states are stored in special topics config.storage.topic and status.storage.topic and managed by the associated connector</li> </ul> </li> <li> <p>Workers</p> <ul> <li>They are the container process that execute connectors and tasks</li> <li>Responsible for<ul> <li>Handle HTTP request and their configurations</li> <li>Store connectors and tasks configurations</li> <li>Start connectors and thier tasks and passing the appropriate configurations along</li> <li>Commit offset for source and sink connectors</li> <li>Handle retries when task fails</li> </ul> </li> <li>When worker fails, tasks are rebalanced over active workers, but when tasks fail they are considered as an exception and no balance is triggered</li> <li>Two types:<ul> <li>Standalone Workers: single process is responsible for executing all tasks</li> <li>Distributed Workers: starts many process using <code>group.id</code></li> </ul> </li> </ul> </li> <li> <p>Converters: convert data from kafka to source system</p> <ul> <li>JSON converter: is part of Kafka</li> <li>Avro converter: provided by Confluent Schema Registry</li> </ul> </li> </ul>"},{"location":"big-data/kafka/connect/connect/#internal-topics","title":"Internal topics","text":"<ul> <li>connect-configs</li> <li>connect-offset</li> <li> <p>connect-status</p> </li> <li> <p>Tool</p> </li> <li> <p>Connect version: <code>curl http://localhost:8083/</code></p> </li> <li>Available connector pluging: <code>curl http://localhost:8083/connector-plugins</code></li> <li>All connectors: <code>curl http://localhost:8083/connectors</code></li> </ul>"},{"location":"big-data/kafka/connect/examples/","title":"File connector example","text":""},{"location":"big-data/kafka/connect/examples/#post-filesourceconnector","title":"Post FileSourceConnector","text":"<pre><code>curl -X POST http://localhost:8083/connectors --header \"content-Type:application/json\" -d '\n{\n    \"name\":\"source-file-config\",\n    \"config\":{\n        \"connector.class\":\"FileStreamSourceConnector\",\n        \"file\":\"/etc/kafka/server.properties\",\n        \"topic\":\"kafka-connect-topic\"\n    }\n}'\n</code></pre>"},{"location":"big-data/kafka/connect/examples/#post-filesinkconnector","title":"Post FileSinkConnector","text":"<pre><code>curl -X POST http://localhost:8083/connectors --header \"content-Type:application/json\" -d '\n{\n  \"name\": \"sink-file-config\",\n  \"config\": {\n    \"connector.class\": \"FileStreamSinkConnector\",\n    \"file\": \"/sink_kafka.txt\",\n    \"topics\": \"kafka-connect-topic\"\n  }\n}'\n</code></pre>"},{"location":"big-data/kafka/connect/examples/#get-avaiblable-connectors","title":"Get avaiblable connectors","text":"<p><code>curl http://localhost:8083/connectors</code></p>"},{"location":"big-data/kafka/connect/examples/#delete-connector","title":"Delete connector","text":"<p><code>curl -X DELETE http://localhost:8083/connectors/source-file-config</code> <code>curl -X DELETE http://localhost:8083/connectors/sink-file-config</code></p>"},{"location":"big-data/kafka/connect/examples/#common-erros-with-jdbc-connector","title":"Common erros with JDBC connector","text":"<ul> <li>Wrong connection string</li> <li>Diver not available in classpath</li> <li>Permissions to read the table</li> </ul>"},{"location":"big-data/kafka/monitor/monitoring/","title":"Monitoring","text":"<ul> <li>Add exporter to prometheus</li> </ul> <p>JAR_PATH=/home/massi/monitoring/jars/jmx_prometheus_javaagent-0.12.0.jar EXPORTER=/home/massi/monitoring/kafka.yml export KAFKA_OPTS=\"$KAFKA_OPTS -javaagent:$JAR_PATH=7071:$EXPORTER\"</p>"},{"location":"big-data/old/hbase/architecture/","title":"Architecture","text":""},{"location":"big-data/old/hbase/architecture/#components","title":"Components","text":"<ul> <li> <p>HMaster server</p> <ul> <li>Acts as NameNode in HDFS</li> <li>Handles and manage Region Server</li> <li>Performs DDL operations</li> <li>Assigns regions to Region Servers</li> <li>Performs recovery activities</li> </ul> </li> <li> <p>HBase Region server</p> <ul> <li>Responsible for handling, manging, executing reads and writes operations on the set of regions</li> <li>Region<ul> <li>Contains all the rows between the start key and end key</li> <li>Hbase table can be divided into a number of regions in such way that all the columns of family   is stored in one region</li> <li>Each region contains the rows in sorted order</li> <li>Many regions are assigned to a Region Server</li> <li>Default size of region is 256 MB which can be configured</li> <li>They are automatically scaled when they become large, or they can be also merged</li> <li>Recommendations<ul> <li>10 to 10000 regions per region server</li> <li>Between 1 GB to 2 GB per size for each region </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Zookeeper</p> <ul> <li>Acts as a coordinator inside HBase</li> <li>Helps in maintains servers states. Servers send heartbeat to Zookeeper</li> <li>Maintains .META server's path</li> <li>Region servers are created as ephemeral nodes, once they fail to send heartbeat the node are deleted</li> </ul> </li> <li> <p>Region Server</p> </li> </ul> <p></p> <pre><code>- **Block Cache**\n    - It stores in the memory the frequently accessed data  \n    - If the data in BlockCache is least recently used, then that data is removed from BlockCache\n- **WAL (Write Ahead Log)**\n    - Is a file attached to every Region Server\n    - Stores a new data hasn't been persisted or committed to permanent storage\n    - It is used in case of failure to recover data sets\n- **MemStore**\n    - It is write cache store\n    - It stores all the incoming data before committing it to the disk or permanent memory\n    - There is one MemStore for each column family\n    - The data is stored in lexicographical order before committing it to the disk\n- **Hfile**\n    - It stores data to cells on the disk\n    - Persistent, ordered and  immutable, it maps keys to values\n    - Default size **64 KB**  \n    - MemStore commit the data to Hfile when the size of MemStore exceeds\n</code></pre> <ul> <li>Read Mechanism</li> <li>Write Mechanism</li> </ul>"},{"location":"big-data/old/hbase/architecture/#data-model","title":"Data Model","text":"<ul> <li>Table: is made of set columns and rows</li> <li>Row: addressed uniquely by row key*</li> <li>Column family: set of column</li> <li>Column <ul> <li>Small unit in HBase</li> <li>May have multiple versions </li> <li>Version<ul> <li>Distinct the values contained in cells</li> <li>They are stored in decreasing timestamped order which helps to retrieve the newest value first</li> <li>User can specify the number of version to keep</li> </ul> </li> </ul> </li> </ul> <pre><code>SortedMap&lt;RowKey, // first map --&gt; is table (map row keo to list of CF)\n    List&lt;SortedMap&lt;Column, // first list --&gt; CF, second map --&gt; (map column to list of values) \n        List&lt;Value, Timestamp&gt;&gt;&gt;&gt; // second list --&gt; List of values and ts\n</code></pre>"},{"location":"big-data/old/hbase/architecture/#notes","title":"Notes","text":"<ul> <li>Sparse</li> <li>wWide tables </li> <li>Column-oriented </li> </ul> <p>Often eliminate the to normalize data, and the Join operations cost needed to aggregate data</p> <ul> <li> <p>Use of intelligent keys gives fine-grained control over how - and where data is stored.</p> </li> <li> <p>Set large memory for region servers, could sometimes be dangerous, because of GC</p> </li> </ul>"},{"location":"big-data/old/hbase/basics/","title":"Basics","text":""},{"location":"big-data/old/hbase/basics/#data-definition-language-ddl","title":"Data Definition Language (DDL)","text":"<ul> <li>Create: <code>create 'table-name', 'CF1', 'CF2'</code></li> <li>Alter<ul> <li>Alter cell number: <code>alter 'table-name', NAME =&gt; 'CF', VERSIONS =&gt; number</code></li> <li>Delete column family: <code>alter 'table-name', METHOD  =&gt; 'CF'</code></li> <li>Make readonly: <code>alter 'table-name', READONLY(option)</code></li> <li>Table operators</li> </ul> </li> <li>Drop: <code>drop 'table-name'</code></li> <li>Drop all: <code>drop_all 'regex'</code></li> <li>List: <code>list</code></li> <li>Disable / Enable: <code>disable 'table-name'</code></li> <li>Is_disabled: <code>is_disabled 'table-name'</code></li> <li>Describe: <code>describe 'table-name'</code></li> <li>Exists: <code>exists 'table-name'</code></li> </ul> <pre><code>create 'customers','Address','Order'\nalter 'Customer', NAME =&gt; 'f1', VERSIONS =&gt; 4\nalter 'customer', 'delete' =&gt; 'f1'\ndrop 'customer'\ndrop 'c*'\ndescribe 'customer'\ndisable 'customer'\nenable 'customer'\nexists 'customer'\n</code></pre>"},{"location":"big-data/old/hbase/basics/#data-manipulation-language-dml","title":"Data Manipulation Language (DML)","text":"<ul> <li>Put : <code>put &lt;table-name&gt;, &lt;row-key&gt;, &lt;columnfamily:columnname&gt;, &lt;value&gt;</code></li> <li>Get with row key : <code>get &lt;table-name&gt;, &lt;row-key&gt;</code></li> <li>Get with row key and column family: <code>get &lt;table-name&gt;, &lt;row-key&gt;, &lt;column-family&gt;</code></li> <li>Scan table: <code>scan &lt;table-name&gt;</code></li> <li>Delete : <code>delete &lt;table-name&gt;, rowkey</code></li> <li>Count rows: <code>count &lt;table-name&gt;</code></li> <li>Count rows: <code>truncate &lt;table-name&gt;</code></li> </ul> <pre><code>put 'customer', '1', 'address:state', 'paris'\nput 'customer', '1','order:number','ORD-15'\nget 'customer', '1'\nget 'customer', '1', 'address'\nsacn 'customer'\ndelete 'customer', '1'\ncount 'customer'\ntruncate 'customer'\n</code></pre>"},{"location":"big-data/old/hive/DDL/","title":"DDL","text":""},{"location":"big-data/old/hive/DDL/#data-definition-language","title":"Data Definition Language","text":"<ul> <li>Create     <pre><code>Create table if not exists dbname.tablename(\n    id int Comment 'id comment',\n    name string comment 'name comment',\n    records array&lt;string&gt;,\n    deductions map&lt;string, float&gt;\n    adgress struct&lt;street: string, city: string, state: string, zip: int&gt; \n)\nComment 'this is comment about table'\nTBLPROPERTIES ('creator=user', 'created_at=2020-01-01')\nLOCATION '/user/hive/dbname.db/tablename;\n</code></pre><ul> <li>If not exists: if the new schema differ from the current hive will not warn you</li> <li>TBLPROPERTIES serves as additional info about table and in some cases can be used as table metadata</li> <li>Default warehouse location <code>/user/hive/warehouse</code></li> <li>Use external table avoid using default location </li> <li>Copy the schema of existing table     <pre><code>create table if not exists dbname.tablename2\nlike dbname.tablename1\n</code></pre></li> </ul> </li> <li> <p>Alter</p> <ul> <li>Modify only table metadata</li> </ul> </li> <li> <p>Drop</p> </li> <li> <p>Rename</p> </li> </ul>"},{"location":"big-data/old/hive/DML/","title":"DML","text":""},{"location":"big-data/old/hive/DML/#dml","title":"DML","text":"<ul> <li> <p>Load data</p> <p><code>LOAD DATA LOCAL INPATH /path/local/file OVERWRITE INTO TABLE tablename</code></p> </li> <li> <p>This command is useful if source table is partitioned</p> <p><pre><code>INSERT OVERWRITE TABLE tablename1\nSELECT * FROM tablename2 t\nWHERE t.col1 = 'US' and t.col2 = 'OR'\n</code></pre> - To avoid scanning table multiple time, use this syntax</p> <pre><code>FROM tablename2 t\nINSERT OVERWRITE TABLE tablename1\n    PARTITION (country = 'US', state = 'OR') \n    SELECT * WHERE t.cntry = 'US' and t.st = 'OR'\n</code></pre> </li> <li> <p>Hive support Dynamic Partition Inserts</p> </li> <li> <p>Create table and load in them in one query </p> <pre><code>CREATE TABLE test\nAS SELECT name, email \nFROM tablesource t\nWHERE t.col = 'toto'\n</code></pre> </li> <li> <p>Export data </p> <p><code>INSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees'</code></p> </li> <li> <p>Aggregation functions </p> <ul> <li>count, sum, avg, ...</li> <li>To improve aggregations queries performance <code>set hive.map.aggr=true;</code>. Doing this the aggr will be done in  map phase, but it'll require more memory</li> </ul> </li> <li> <p>Generating functions</p> <ul> <li>The inverse of aggregation function</li> <li>it takes one column and explode it to multiple columns or rows</li> <li>explode, json_tuple, ...</li> </ul> </li> <li> <p>Join query optimization</p> <ul> <li>Put the large table at the end of the query (Hive will put the ones (small) in buffer and stream the last one) </li> <li>Add <code>/*+STREAMTABLE(tablename)</code> to the query</li> <li>Map-side joins</li> </ul> </li> <li> <p>Compression</p> <ul> <li>Minimize space storage</li> <li> <p>Reduce the overhead of disk and network I/O</p> </li> <li> <p>Common techniques</p> <ul> <li>GZip</li> <li>Bzip2</li> <li>Snappy</li> <li>LZO</li> </ul> </li> <li> <p>GZip and BZip2</p> <ul> <li>Create small compressed output, but with the highest CPU overhead</li> <li>Useful if disk space, and I/O overhead are concerns    </li> </ul> </li> <li>Snappy and LZo<ul> <li>Create larger file, but they are faster</li> <li>Useful if space, and I/O overhead aren't more important than rapid decompression</li> </ul> </li> </ul> </li> </ul>"},{"location":"big-data/old/hive/concepts/","title":"Hive","text":"<ul> <li>Is data warehouse</li> <li>Uses HQL</li> </ul>"},{"location":"big-data/old/hive/concepts/#services","title":"Services","text":"<ol> <li>Hive</li> <li>Metastore<ul> <li>Stores metadata of tables and partitions  </li> <li>Hive in remote mode:<ul> <li>Metastore use its own JVM process and other services communite with it using Thrift API</li> </ul> </li> </ul> </li> <li>HiveServer2</li> </ol>"},{"location":"big-data/old/hive/concepts/#tuning","title":"Tuning","text":"<ul> <li>Memory configuration </li> <li>GC limits </li> <li>Partitions number limits </li> </ul>"},{"location":"big-data/old/hive/concepts/#ha","title":"HA","text":"<ul> <li>Configure load balancer for HiveServer2 </li> <li>Enable HA for metastore </li> </ul>"},{"location":"big-data/old/hive/concepts/#replication","title":"Replication","text":""},{"location":"big-data/old/hive/concepts/#security","title":"Security","text":"<ol> <li>Authentication: LDAP and Kerberos <ul> <li>If Kerberos is not enabled no need to provide password when connecting using Beeline CLI </li> </ul> </li> <li>Authorization: Ranger </li> <li>Encryption SASL QOP or SSL/TLS  </li> </ol>"},{"location":"big-data/old/hive/concepts/#about","title":"About","text":"<ul> <li>Hive do not enforce schema on write, but it</li> </ul>"},{"location":"big-data/old/hive/concepts/#hive-sitexml","title":"hive-site.xml","text":"<ul> <li>hive.metastore.warehouse.dir : default dir in hdfs where data is stored</li> <li>Metastore </li> <li>Driver: javax.jdo.option.ConnectionDriverName</li> <li>Password: javax.jdo.option.ConnectionPassword</li> <li>Url: javax.jdo.option.ConnectionURL</li> </ul> <p>### core-site.xml</p>"},{"location":"big-data/old/hive/concepts/#data-model","title":"Data Model","text":""},{"location":"big-data/old/hive/concepts/#table","title":"Table","text":"<ul> <li>Show<ul> <li><code>show tables in &lt;namespace&gt;</code></li> <li><code>show tables 'emp*'</code></li> </ul> </li> <li>Describe<ul> <li><code>describe extended &lt;tablename&gt;</code></li> <li><code>describe formatted &lt;tablename&gt;</code></li> </ul> </li> <li> <p>Describe column </p> <ul> <li><code>describe extended &lt;tablename.columnname&gt;</code></li> </ul> </li> <li> <p>Internal (managed)</p> <ul> <li>Data stored in default location</li> <li>The data is dropped when the table is dropped</li> </ul> </li> <li>External<ul> <li>Location is not the default one</li> <li>Dropping table will not drop data, although table's metadata will be dropped</li> </ul> </li> </ul>"},{"location":"big-data/old/hive/concepts/#partition","title":"Partition","text":"<ul> <li>strict: prohibit queries without where clause in partitioned tables </li> <li>nonstrict: allow running queries without where clause</li> </ul> <p><code>set hive.mapred.mode=strict;</code></p> <ul> <li>We can describe and show partitions<ul> <li><code>show partitions &lt;tablename&gt;</code></li> <li><code>show partitions &lt;tablename&gt; PARTITION(col_name='value')</code></li> </ul> </li> <li>We can alter table by adding partition</li> <li>no support for insert, update and delete</li> <li>doesn't support transactions</li> </ul> <p><code>sql     Create table if not exists mydb.employee(         name string,         salary float,         adgress struct&lt;street: string, city: string, state: string, zip: int&gt;      )     PARTITIONED BY (country string, state string);</code></p> <pre><code>- Load data from file to partitioned table\n```\nLOAD DATA LOCAL INPATH '${env:HOME}/path/to/dir'\nINTO TABLE employees\nPARTITION (col_part_name1 = 'value1', col_part_name2 = 'value2');\n```\n</code></pre>"},{"location":"big-data/old/hive/concepts/#bucket","title":"Bucket","text":"<ul> <li>Each bucket is stored as file in partition directory</li> </ul>"},{"location":"big-data/old/hive/concepts/#types","title":"Types","text":"<ul> <li>Java types (int, string, float, ...)</li> <li>Complex types: Array, Map, Struct</li> </ul>"},{"location":"big-data/old/hive/concepts/#internal-vs-external-table","title":"Internal vs external table","text":"<ul> <li>Hive manages interanl but non external</li> <li>Drop internal delete metadata and path, but drop external remove only metadata</li> <li>Internal support archive, unarchive, ACID/transactional and result caching ?? but none of there features are supported by an external</li> </ul>"},{"location":"big-data/old/yarn/concepts/","title":"Yarn","text":""},{"location":"big-data/old/yarn/concepts/#components","title":"Components","text":"<ul> <li>Resource Manger</li> <li>Scheduler<ul> <li>Schedule job based on resources available within the cluster.</li> </ul> </li> <li>Application Manager<ul> <li>Accepts job-submission.</li> <li>Negotiates the first container with scheduler in order to create Application Master.</li> <li>Provides the service for restarting the ApplicationMaster container on failure.</li> </ul> </li> <li> <p>ApplicationMaster: is per-application, and it has the responsibility of negotiating appropriate resource containers       from the Scheduler, tracking their status and monitoring for progress</p> </li> <li> <p>Node Manger</p> </li> <li>Responsible for containers, monitoring their resources usage (cpu, memory, disk, network)     for Resource Manager.</li> </ul>"},{"location":"big-data/old/yarn/queues/","title":"Queues","text":"<ul> <li>Queues </li> <li>FIFO: <ul> <li>Does not need any configuration. </li> <li>Runs applications in submission order.      </li> </ul> </li> <li>Capacity Scheduler</li> <li>Fair Scheduler</li> </ul>"},{"location":"big-data/presto/concepts/","title":"Presto","text":"<ul> <li>Architecture similar to MPP</li> <li>The main components</li> <li>Coordinator</li> <li>Workers</li> <li>Connectors</li> </ul>"},{"location":"big-data/redis/concepts/","title":"Concepts","text":""},{"location":"big-data/redis/concepts/#docker-image","title":"Docker image","text":"<p><code>docker run --name redis-server -d redis</code></p> <ul> <li>Master / Slave architecture </li> </ul>"},{"location":"big-data/redis/concepts/#basic-commands","title":"Basic commands:","text":"<ul> <li>Start cli <code>redis-cli</code></li> <li>Check connection <code>ping</code></li> <li>Quit connection <code>QUIT</code></li> <li>Set and get value:      <code>SET foo 100</code> <code>GET foo</code></li> <li>Increment or decrement <code>INCR / DECR foo</code> ==&gt; 101 / 100</li> <li>Check if exists <code>EXISTS foo</code></li> <li>Delete <code>DEL foo</code></li> <li>Clear all <code>FLUSHALL</code></li> <li>Set expire <code>EXPIRE foo 50</code> ==&gt; will expire in 50 seconds</li> <li>Remaing time to expire <code>TTL foo</code></li> <li>Set value with experation <code>SETEX  time key value</code></li> </ul>"},{"location":"big-data/spark/basics/","title":"Basics","text":""},{"location":"big-data/spark/basics/#why-spark","title":"Why spark ?","text":"<ul> <li>In-memory </li> <li>Lazy evaluation</li> <li>DAG for fault-tolerance and disaster recovery</li> </ul>"},{"location":"big-data/spark/basics/#transformations","title":"Transformations","text":"<ul> <li>Narrow transformations</li> <li>Large transformations</li> </ul>"},{"location":"big-data/spark/basics/#action","title":"Action","text":"<p>There are three type of actions in spark - Action to write to console (show, explain, ...) - Action to collect data (collect, count, ...) - Action to write to external data storage (write)</p>"},{"location":"big-data/spark/basics/#main-component","title":"Main component","text":"<ul> <li>Execution model</li> <li>The shuffle</li> <li>The cache</li> </ul>"},{"location":"big-data/spark/basics/#execution-steps","title":"Execution steps","text":"<p><code>User code ==&gt; Logical plan ==&gt; Physical plan ==&gt; Execution (RDD manipulations)</code></p> <ul> <li>Unresolved Logical Plan </li> <li>Verify syntactic field</li> <li> <p>Semantic analysis</p> </li> <li> <p>Logical Plan</p> </li> <li>Check data structure, schema and types</li> <li> <p>It resolves types using Catalog</p> </li> <li> <p>Optimized Logical Plan</p> </li> <li>Apply some rules to optimize the logical plan</li> <li> <p>Reorder the logical operations to perform some optimizations</p> </li> <li> <p>Physical Plan</p> </li> <li>Catalyst Optimizer will generate many Physical Plans base on execution time and resource cosumption.     Only one with the best cost performance will be chosen.  </li> </ul>"},{"location":"big-data/spark/basics/#notes","title":"Notes","text":"<ul> <li>For production, it's recommended to define a schema when we're dealing with untyped   data sources like CSV and JSON.</li> <li>The most flexible way when dealing with selects in df is to use <code>selectExpr</code>.  </li> <li>Query Pushdown: With JDBC Spark makes best-effort attempt to filter data in database itself before creating    the dataframe.</li> <li>Parquet with GZIP compression is recommended.   </li> </ul>"},{"location":"big-data/spark/concepts/","title":"Concepts","text":"<ul> <li>Solve the problem of iterative algorithm</li> <li>MR pb: repetead access to HDFS, no data caching in between iteration </li> <li>MPI: no support for fault tolerance</li> <li>What does spark</li> <li>Iterative algorithm </li> <li>Interactive data mining </li> <li>Use RDD</li> <li>Keep features of MR: fault tolerance, data locality, scalability </li> <li>Data Model</li> <li>RDD</li> <li>Logistic regression ? </li> </ul>"},{"location":"big-data/spark/join_shuffle/","title":"Join shuffle","text":""},{"location":"big-data/spark/join_shuffle/#shuffle","title":"Shuffle","text":"<ul> <li>Redistributes data among partitions</li> <li> <p>Hash keys into buckets</p> </li> <li> <p>Data no longer stay in memory</p> </li> <li> <p>Involves:</p> <ul> <li>Data partition: which may involves very expensive data sorting works</li> <li>Data ser/des: to enable data been transfer through network or across process</li> <li>Data compression: to reduce IO bandwidth</li> <li>DISK IO: probably multiple times on single data bloc<ul> <li>e.g Shuffle Spill, Merge combine</li> </ul> </li> </ul> </li> <li> <p>Tricks</p> <ul> <li>Avoid when it's possible</li> <li>Use partial aggregation to reduces data movement ?</li> </ul> </li> </ul>"},{"location":"big-data/spark/join_shuffle/#join","title":"Join","text":"<ul> <li> <p>Big-table to Big-table</p> <ul> <li>Shuffle join: =&gt; Shuffle join (Each node will talk to another node to get join keys)</li> <li>It'll induce  all-to-all communication</li> </ul> </li> <li> <p>Big-table to Small-table</p> <ul> <li>Broadcast join<ul> <li>Use broadcast join on the small table.</li> <li>Place a small table in each executor </li> <li>Induce per node computation strategy</li> <li><code>spark.sql.autoBroadcastJoinThreshold</code> is used to determine the size</li> </ul> </li> </ul> </li> <li> <p>Little-to-little table</p> </li> <li> <p>Let Spark to decide how to join them</p> </li> <li> <p>Sort join</p> </li> <li> <p>Bucket join</p> </li> </ul>"},{"location":"big-data/spark/memory/","title":"Memory","text":""},{"location":"big-data/spark/memory/#executor-memory","title":"Executor memory","text":""},{"location":"big-data/spark/memory/#on-heap","title":"On-heap","text":"<ul> <li>Spark Memory (spark.memory.fraction)</li> <li>Storage Memory (spark.memory.storageFraction) : used to store Spark cache data, such as RDD cache, Broadcast variable, Unroll data (process of deserializing a serialized data), and so on.</li> <li>Execution Memory:  used to store temporary data in the calculation process of Shuffle, Join, Sort, Aggregation, etc</li> <li>User Memory: It's mainly used to store the data needed for RDD conversion operations, such as the information for RDD dependency</li> <li>Reserved Memory (300MB): The memory is reserved for system and is used to store Spark's internal objects</li> <li>Spark fail if we don't give executor memory at least 1.5 * Reveserved Memory = 450MB</li> </ul>"},{"location":"big-data/spark/memory/#cache-and-repartition","title":"Cache and Repartition","text":"<ul> <li>Cache: use MEMORY_AND_DISK storage level.</li> <li>Persist: the storage level can be changed.</li> <li>Cache and persist are lazy operations (transformations)</li> <li>Spark drops persisted data if not used or by using least-recently-used (LRU) algorithm.</li> </ul>"},{"location":"big-data/spark/pres/","title":"Pres","text":""},{"location":"big-data/spark/pres/#why-spark","title":"Why Spark ?","text":"<ul> <li>In-memory Computation (100x times faster than MR in memory, 10x faster than on disk) ===&gt; Remark DS</li> <li>Resilient Distributed Datasets (RDD)</li> <li>Immuatable (Why ?) Because of fact that data is distruted + servers in data recovery</li> <li>Can be cached or persisted   </li> <li>Lazy Evaluation</li> <li>Batch and stream processing </li> </ul>"},{"location":"big-data/spark/pres/#spark-architecture","title":"Spark architecture","text":"<ul> <li>Driver</li> <li>Executors </li> </ul>"},{"location":"big-data/spark/pres/#how-can-be-executed-a-spark-app-execution-modes","title":"How can be executed a Spark App Execution modes","text":"<ul> <li>Standalone (local) </li> <li>Yarn</li> <li>Kubernetes</li> <li>Mesos</li> </ul>"},{"location":"big-data/spark/pres/#deploy-modes","title":"Deploy modes","text":"<ul> <li>Client </li> <li>Cluster (main difference: owner of resources ??) </li> </ul>"},{"location":"big-data/spark/pres/#transformation-vs-action","title":"Transformation vs action","text":"<ul> <li>Transformation always returns an RDD (wide and large --&gt; add schema)</li> <li>Action </li> </ul>"},{"location":"big-data/spark/pres/#how-does-spark-partition-the-data-add-an-image","title":"How does Spark partition the data ? ===&gt; add an image","text":"<ul> <li>Huge amount of data can't fit in one single node memory</li> <li>Leads to minimize IO (serialization and deserialization)</li> <li>Spark uses the princpe of data locality (read date from the nodes that are close) look in detail </li> <li>Creates a partition of size 128 MB in HDFS<ul> <li>The partition itself can be partited by HDFS (in hdfs way) </li> <li>Imagine it as a file that you write and every file writen to HDFS is partitionned</li> <li>Partiton of 256 MB =&gt; 2 blocks in HDFS (HDFS partition)</li> </ul> </li> <li>Having many partitions doesn't mean more performance<ul> <li>Partition &lt;=&gt; task, so, many partitions will the increase the execution time because it   requires a lot of time for creating, scheduling and manging task by spark scheduler</li> <li>A lot of partition leads to huge flow between driver and executor (Increase IO. Well known Small files issue in HDFS that saturates INodes tables)</li> <li>Empty partitions take time to compute </li> </ul> </li> <li>A few number of partition<ul> <li>Idle nodes</li> <li>Data skew issue  </li> </ul> </li> <li>Recommandation:<ul> <li>2x or 3x number of vcores </li> <li>100 ms to computes a partition (benchmark have been done on machines with average capacities)</li> </ul> </li> <li>Example:  file of 10KB in 20 partitions </li> </ul> <p><pre><code>  read.text(\"path/to/text.txt\")\n      .repartition(20)\n</code></pre>   -  repartition() or coalesce(): partition data in memory    -  partitionBy(): partition data in disk</p>"},{"location":"big-data/spark/pres/#spark-memory-types-add-image","title":"Spark memory types ? Add image","text":""},{"location":"big-data/spark/pres/#notion-of-app-stage-tasks-vcore","title":"Notion of APP -&gt; Stage -&gt; tasks (vcore)","text":""},{"location":"big-data/spark/pres/#how-handales-data-recovery-add-an-image","title":"How handales data recovery ==&gt;Add an image","text":"<ul> <li>Node crash (no heartbeat are received from the node) ??</li> <li>Lineage graph (execution plan) --&gt;  DAG<ul> <li>Applies the same execution plan in all nodes leads to recover the data</li> </ul> </li> <li> <p>Task fails ?</p> <ul> <li>Retry </li> <li>It no ==&gt; assign to new executor</li> </ul> </li> <li> <p>How spark reads config ? Order is important </p> </li> <li>Main</li> </ul> <p><pre><code>SparkSession spark = SparkSession\n      .master(\"local\")\n      .config(\"key1\", \"value1\")\n      ...\n      .getOrCreate() // Important ? \n</code></pre>   - Spark-submit    - Default config </p>"},{"location":"big-data/spark/pres/#to-present-later","title":"To present later","text":"<ul> <li>Spark plans</li> </ul>"},{"location":"big-data/spark/structuredStream/","title":"structuredStream","text":"<ul> <li>Input: Kafka, HDFS, S3, Socket, </li> <li> <p>Output:</p> <ul> <li>Kafka</li> <li>Files (json, parquet, csv, ...)</li> <li>Console (for testing)</li> <li>Memory (for debugin)</li> </ul> </li> <li> <p>Output mode (how output data):</p> <ul> <li>Append: add only new records</li> <li>Update: add updated records</li> <li>Complete: add all records</li> </ul> </li> <li> <p>Triggers (when output data):</p> </li> <li> <p>Event time processing</p> </li> <li> <p>Watermarks ?? </p> </li> </ul>"},{"location":"big-data/spark/structuredStream/#notes","title":"Notes","text":"<ul> <li>Infer schema is disabled by default in Streaming to enable it use : spark.sql.streaming.schemaInference=true</li> <li>To control how quickly Spark will read all of the files in folder, add the option maxFilesPerTrigger to Dataframe</li> <li>Spark Structured Streaming 2.2 do not support chained aggregations</li> </ul>"},{"location":"big-data/spark/tuning/","title":"Tuning","text":""},{"location":"big-data/spark/tuning/#in-driver","title":"In Driver","text":"<ul> <li>Dynamic Executor Allocation<ul> <li>Enables Spark jobs to add and remove executors on the fly</li> <li>Get how much resource you need not more </li> <li>Good for multi-tenant environment</li> </ul> </li> </ul>"},{"location":"big-data/spark/tuning/#in-executor","title":"In Executor","text":"<ol> <li> <p>Important ....</p> </li> <li> <p>spark.executor.cores: the number of virtual cores to assign for each executor</p> </li> <li>The number of threads per executor</li> <li>Large number of virtual cores leads to low number of executors =&gt; reduce the parallelism </li> <li>Low number of virtual cores leads to high number of executors =&gt; large amount of I/O operations</li> <li> <p>Recommended number based of benchmarks is: <code>spark.executor.cores=5</code></p> </li> <li> <p>spark.executor.memory: amount of memory per executor</p> <ul> <li> <p>Assign one virtual core to Hadoop daemons</p> <p><code>nb of executor per instance = (nb of total virtual cores - 1) / spark.executor.cores</code></p> </li> <li> <p>Assign 1 GB for Hadoop daemons</p> <p><code>Total executor memory = (total RAM - 1) / nb of executor per instance</code></p> </li> </ul> </li> <li> <p>spark.executor.instances (--num-executor):</p> <ul> <li>The number of tasks that can be run in parallel </li> </ul> </li> <li> <p>Off-Heap</p> <ul> <li>spark.memory.offHeap.enabled =&gt; true</li> <li>spark.memory.offHeap.size =&gt; e.g : 2g</li> </ul> </li> <li> <p>Garbage collector</p> </li> <li> <p>Eliminating disk I/O bottleneck</p> <ul> <li>Disk access is 10-100K time slower than memory</li> <li>spark.shuffle.file.buffer</li> <li>spark.io.compression.lz4.blockSize This will reduce the size of shuffles  <ul> <li>Default is 32k and is sub-optimal</li> <li>512k gives the best performance</li> </ul> </li> </ul> </li> </ol>"},{"location":"big-data/spark/tuning/#cache-persist","title":"Cache / persist","text":"<ul> <li>Use cache or persist when small data is accessed frequently (like lookup table)    or using iterative algorithm.</li> <li>Don't forget to un-cache or un-persist otherwise you'll see some spill to disk which will   increase pressure on GC.</li> </ul>"},{"location":"big-data/spark/tuning/#coalesce-repartition","title":"Coalesce / repartition","text":"<ul> <li>Use coalesce instead of repartition when you want to reduce partitions size. It'll avoid   shuffling the data. </li> </ul>"},{"location":"big-data/spark/tuning/#broadcast-join","title":"Broadcast join","text":""},{"location":"big-data/spark/tuning/#serialization","title":"Serialization","text":"<ul> <li>Convert object to stream of bytes or vice versa</li> <li>Help when<ul> <li>Saving data to disk</li> <li>Send data over network</li> </ul> </li> <li>Happen when things are shuffled around</li> <li> <p>Helps to</p> <ul> <li>Decrease memory usage</li> <li>Reduce network bottleneck</li> <li>Enhance performance tuning</li> </ul> </li> <li> <p>Two types of serialization</p> <ul> <li>Java serialization</li> <li>Kryo serialization</li> </ul> </li> <li> <p>Kryo serialization is exceptionally 10x faster and more compact than Java serialization</p> <ul> <li>spark.serializer =&gt; org.apache.spark.serializer.KryoSerializer</li> <li>spark.kryoserializer.buffer.mb =&gt; 24</li> </ul> </li> </ul>"},{"location":"big-data/spark/tuning/#partition-tuning","title":"Partition tuning","text":"<ul> <li>More issue    <ul> <li>Too few partitions<ul> <li>Less concurrency</li> <li>More susceptible to data skew</li> <li>Increased memory pressure for groupBy, reduceBy, ...</li> </ul> </li> <li>Too many partitions<ul> <li>It'll take many times to schedule task</li> </ul> </li> <li>Need reasonable number of partition</li> </ul> </li> <li>Lower bound:  At least ~3x number of cores in the cluster</li> <li>Upper bound: Ensure tasks take at least 100ms</li> </ul>"},{"location":"big-data/spark/tuning/#shuffle","title":"Shuffle","text":"<ul> <li>spark.shuffle.compress: whether to compress map output files (true by default).</li> <li>spark.shuffle.spill.compress: wheter to compress data spilled during the shuffles (true by default).</li> <li>spark.shuffle.manager: specify spark shuffle algorithm<ul> <li>Hash</li> <li>Sort</li> <li>Tungsten</li> </ul> </li> </ul>"},{"location":"big-data/spark/tuning/#data-locality","title":"Data locality","text":""},{"location":"big-data/spark/tuning/#example","title":"Example","text":"<ul> <li>6 nodes (number of nodes) </li> <li>16 cores per node (number of cores per node)</li> <li>One core should be reserved to Hadoop process, so <code>number of cores per node = 15</code></li> <li>64 GB RAM per node (memory per node)</li> <li> <p>The optimal value is 5 vcores per executor (number of cores per executor, --executor-cores)</p> <p><code>number of executor per node = number of cores per node / number of vcores per executor                                = 15 / 5                                = 3</code></p> </li> <li> <p>The total number of executors (--num-executors)</p> <ul> <li>One executor (JVM process) should be left to yarn (Application Master).</li> </ul> </li> </ul> <p><pre><code>total number of executors = (number of nodes * number of executors per node) - 1 \n                               = (6 * 3) - 1 \n                               = 17                           \n</code></pre> - Memory per executor</p> <p><pre><code>initial memory per executor = memory per node / number of executor per node \n                               = 64 / 3\n                               = 21 GB                           \n</code></pre>   - This isn't the final memory we should allocate small overhead memory needed by yarn process </p> <pre><code>```\noverhead memory = max(384, 0.07 * spark.executor.memory)\n                = max(384, 0.07 * 21)\n                = 1.47\n```\n\n``` \nmemory per executor = initial memory per executor -  overhead memory\n                        = 21 - 1.47\n                        ~= 19 GB\n```\n</code></pre>"},{"location":"big-data/spark/tuning/#links","title":"Links","text":"<ul> <li>Link-1 </li> </ul>"},{"location":"big-data/zookeeper/concepts/","title":"Zookeeper","text":"<ul> <li>Enables coordination in distributed systems</li> <li>Main cluster state and notify any changes</li> <li>Needed to:</li> <li>Manage configuration around the cluster</li> <li>Have leader to perform updates, writes, ...</li> <li>Lock resources</li> <li>Challenges of Distributed Applications</li> <li>Race condition: two machines try to perform a task, which should be performed by one machine</li> <li>Deadlock: two or more operations waiting other to complete</li> <li>Inconsistency: partial failure of data</li> </ul>"},{"location":"big-data/zookeeper/concepts/#components","title":"Components","text":"<ul> <li>Client: sends heartbeats to inform the server that is alive</li> <li>Server: sends acknowledgment  </li> <li>Zookeeper ensemble: set of servers. Minimum nodes required to form assemble is 3</li> <li>Leader: recover failed nodes</li> <li> <p>Follower: follows leader instructions</p> </li> <li> <p>Namespaces:</p> </li> <li>Znode: each node have name and sequence of path separated by (/)</li> <li>Config: centralized configuration</li> <li>Workers: used for naming</li> <li>Data model</li> <li>stat<ul> <li>Version number</li> <li>ACL</li> <li>Timestamp</li> <li>Data length</li> </ul> </li> </ul>"},{"location":"big-data/zookeeper/concepts/#znode","title":"Znode","text":"<ul> <li>Maintain a stats structure</li> <li>Can store data and have children at same time</li> <li>Stores information (last version, transaction Id of last transaction, ...)</li> <li>Have ACL (create, write, read, delete and admin)</li> <li>Support authentication username / password</li> <li> <p>Client can set watcher on znode to get notified when change occurs</p> <ul> <li>Data change</li> <li>Change an any znode</li> <li>Child znode created or deleted</li> </ul> </li> <li> <p>Types</p> </li> <li>Persistence: permanent node they are only deleted manually (default)  </li> <li>Ephemeral: delete by Zookeeper when (-e flag to create)           * Client who created it close the session           * No request from client after timeout session (use by Kafka to maintain brokers)</li> <li> <p>Sequential: same as ephemeral just Zoopkeeper assigns number of 10 digits during the creation and increment      this number when other sequential is created (-s flag to create)</p> </li> <li> <p>Session: create between client and zookeeper</p> <ul> <li>Client sends heartbeats</li> <li>Fails If no heartbeat was sent after certain period (timeout)</li> </ul> </li> <li> <p>Watchers: mechanism used by client to get changes in Zookeeper</p> </li> </ul>"},{"location":"big-data/zookeeper/concepts/#config","title":"Config","text":"<ul> <li>server.X=hostname:peerPort:leaderPort</li> <li>X: should be integer</li> <li>hostname: ip of the host</li> <li>peerPort: port on which ensemble communicates</li> <li>leaderPort: port used to elect leader</li> <li> <p>clientPort: port used to communicate with client  </p> </li> <li> <p>If leader goes down =&gt; followers store are notified and store the latest change in cluster data storage</p> </li> </ul>"},{"location":"big-data/zookeeper/concepts/#commands","title":"Commands","text":"<ul> <li>Create: <code>create /znode_test \"this is the data of the znode_test\"</code></li> <li>Get: <code>get /znode_test</code></li> <li>Create child: <code>create /znode_test/child_1 \"this is the data of chid_1\"</code></li> <li>Delete: <code>delete /znode_test/child_1</code></li> <li>Get ACL: <code>getAcl /znode_test</code></li> </ul>"},{"location":"big-data/zookeeper/concepts/#election","title":"Election","text":"<ul> <li>Herd effet</li> </ul>"},{"location":"big-data/zookeeper/concepts/#useful-links","title":"Useful links","text":"<ul> <li>Link-1</li> </ul>"},{"location":"cloud/Concepts/","title":"Cloud Important Concepts","text":""},{"location":"cloud/Concepts/#benefits","title":"Benefits","text":"<ul> <li>Cost-effective: you pay for what you consume. Pay-as-you-go (PAYG)</li> <li>Global: we can choose regions in every part in world</li> <li>Secure: cloud provider put a lot of effort to secure services</li> <li>Reliable<ul> <li>data backups</li> <li>disaster recovery</li> <li>data replication</li> <li>fault tolerance</li> </ul> </li> <li>Scalable: increase and decrease resources and services based on demand</li> <li>Elastic: automate scaling during the spikes and drop in demand</li> </ul>"},{"location":"cloud/Concepts/#cloud-services-type","title":"Cloud services type","text":"<p>Cloud services come in three primary types:</p> <ul> <li>SaaS (Software as a Service) --&gt; For customers</li> <li>PaaS (Platform as a Service) --&gt; For developers</li> <li>IaaS (Infrastructure as a Service) --&gt; For Admins</li> </ul>"},{"location":"cloud/Concepts/#cloud-types","title":"Cloud types","text":"<p>Cloud computing can be deployed in different ways depending on your business needs:</p> <ul> <li>Public Cloud: Services are delivered over the public internet and shared across multiple users (Known also as Cloud-Native)</li> <li>Private Cloud: Cloud infrastructure is used exclusively by one organization\u2014either hosted internally or by a third-party. Ideal for strict security or compliance needs (Known also as On-premise)</li> <li>Hybrid Cloud:  Combines public and private clouds to allow data and applications to move between them. Offers greater flexibility and optimization</li> </ul>"},{"location":"cloud/Concepts/#advantages","title":"Advantages","text":"<ul> <li>High Availability</li> <li>High Scalability<ul> <li>Scale up (vertical scaling)</li> <li>Scale out (horizontal scaling)</li> </ul> </li> <li>High Elasticity (scale automatically)</li> <li>High Durability<ul> <li>Be fast to restore in case of disaster</li> </ul> </li> </ul>"},{"location":"cloud/Concepts/#plan-supports","title":"Plan supports","text":"<ul> <li>Enterprise</li> <li>Business</li> <li>Developer </li> <li>Basic</li> </ul>"},{"location":"cloud/twelve-factor/","title":"Twelve Factors","text":"<ol> <li>Codebase<ul> <li>Git repo per application</li> <li>An app should have its own git repo and should not share this repo with any other apps</li> </ul> </li> <li>Dependencies<ul> <li>App should always explicitly declare all its dependencies (using tool like maven), it doesn't suppose that the dependencies will be available at system level </li> </ul> </li> <li>Configurations<ul> <li>App should externalize all such configurations that vary between deployments </li> <li>Storage of configuration in environment</li> </ul> </li> <li>Backing services<ul> <li>Treat all such backing services as attached resources</li> <li>No code change is needed when the service change </li> </ul> </li> <li>Build, Release, Run<ul> <li>Separate the process of converting the codebase into running application</li> <li>Build: compile the code and create docker image</li> <li>Release: push the image to registry</li> <li>Run: pull the image and run it in the container </li> </ul> </li> <li>Process - Stateless Apps<ul> <li>Any data needs to persist must be ub backing services like database in order to allow the scaling out of    the application </li> </ul> </li> <li>Port binding</li> <li>Concurrency</li> <li>Disposability<ul> <li>Maximize robustness with fast startup and graceful shutdown</li> <li>If the application is under increasing load we should be able to start and stop the application rapidly to scale up and scale down </li> </ul> </li> <li>Dev / Prod parity<ul> <li>Keep the gap between environments as minimal as possible</li> <li>A containerized application is expected to behave the same, no matter where we run it (Use docker) </li> </ul> </li> <li>Logs<ul> <li>Publish logs as event streams</li> </ul> </li> <li>Admin process<ul> <li>Keep admin scripts together with app codebase</li> </ul> </li> </ol>"},{"location":"cloud/aws/basics/","title":"Basics","text":"<ul> <li>Access Management basics</li> <li>Security Responsability Models</li> <li>Service Models</li> <li>High Availability (HA) and Fault Tolerance (FT)</li> <li>RPO and RTO</li> <li>Scaling</li> </ul>"},{"location":"cloud/aws/basics/#access-management-basics","title":"Access Management basics","text":"<ol> <li>Principal: an identity that makes a request to be authenticated</li> <li>Authentication: process to authenticate a Principal</li> <li>Identity: object that requires authentication and are authorized to access resources</li> <li>Authorization: process of checking and allowing or denying access to resource to an Identity</li> </ol>"},{"location":"cloud/aws/basics/#security-responsibility-models","title":"Security Responsibility Models","text":"<p>Two types of models</p> <ul> <li>Security IN cloud: is the customer responsibility</li> <li>Security OF the cloud: AWS responsibility</li> </ul>"},{"location":"cloud/aws/basics/#service-models","title":"Service Models","text":"<ul> <li>IaaS (Infrastructure as a Service): Infrastructure to manage (ex: EC2)</li> <li>PaaS (Platform as a Service): Data and Application to manage (ex: EMR)</li> <li>SaaS (Software as a Service): only Data needs to be managed (Ex: ??)</li> </ul>"},{"location":"cloud/aws/basics/#high-availability-ha-and-fault-tolerance-ft","title":"High Availability (HA) and Fault Tolerance (FT)","text":"<ul> <li>HA: recover quickly in the event of failure (minimize the downtime)</li> <li>FT: operate through the failure with no user impact. Is more expensive and hard to achieve</li> </ul>"},{"location":"cloud/aws/basics/#rpo-and-rto","title":"RPO and RTO","text":"<ul> <li>Recovery Point Objective</li> <li>How much a business can tolerate to lose, expressed in time</li> <li>The maximum time between failure and the last successful backup</li> <li>Recovery Time Objective</li> <li>Maximum amount of time a system can be down</li> <li>How long a solution takes to recover</li> </ul>"},{"location":"cloud/aws/basics/#scaling","title":"Scaling","text":"<ul> <li>Vertical</li> <li>Increase the server capacity</li> <li>Add more CPU and RAM</li> <li>Horizontal</li> <li>Add more servers (machines)</li> </ul>"},{"location":"cloud/aws/basics/#tiered-application","title":"Tiered Application","text":"<ul> <li>Monolithic</li> <li>Micro services</li> </ul>"},{"location":"cloud/aws/basics/#encryption","title":"Encryption","text":"<ul> <li>Encryption on fly (SSL)</li> <li>Data encrypted before sending and decrypted when received by the server</li> <li>To protect against the MIMD attack</li> <li>SSL certificates help to secure HTTPS</li> <li>Server side encryption at rest</li> <li>Client side encryption</li> <li>Data encrypted by the client and never decrypted by the server</li> </ul>"},{"location":"cloud/aws/basics/#aws-account","title":"AWS account","text":"<ul> <li>Has:</li> <li>Account id</li> <li>Sign in url (it can be customized by creating account alias)</li> </ul>"},{"location":"cloud/aws/concepts/","title":"Concepts","text":"<ul> <li>Services type</li> <li>Managed service: respo for infra an ha and scaling</li> <li>Fully managed: no access to infra but responsible for high availabilyt an scaling</li> <li>Serverless </li> </ul>"},{"location":"cloud/aws/fontend-mobile/","title":"Fontend mobile","text":""},{"location":"cloud/aws/fontend-mobile/#aws-amplify","title":"AWS Amplify","text":"<ul> <li>Offers tools for front-end web abd mobile developers to quickly build a full-stack application on AWS</li> <li>Two services</li> <li>Amplify Hosting<ul> <li>Frameworks: Single Page Application</li> <li>Environments: separate dev, prod</li> <li>SSR</li> </ul> </li> <li>Amplify Studio<ul> <li>Easy auth</li> <li>Simplified development</li> <li>Ready to use</li> </ul> </li> </ul>"},{"location":"cloud/aws/fontend-mobile/#aws-device-farm","title":"AWS Device Farm","text":"<ul> <li>An application testing service for testing and interacting with Android, iOS, and web apps</li> <li>Two primary testing methods</li> <li>Automated: upload scripts or use built-in tests fir automated parallel tests on mobile devices</li> <li>Remote access: swipe, gesture, and interact with devices in real time via web browsers</li> </ul>"},{"location":"cloud/aws/fontend-mobile/#amazon-pinpoint","title":"Amazon Pinpoint","text":"<ul> <li>Enables to engage with customers through a variety of different messaging channels</li> <li>It is geared towards less technical teams, like marketing teams, and simplifies engagement campaigns</li> <li>The service also offers the ability to leverage machine learning models to better understand customer interactions   for future engagements</li> </ul>"},{"location":"cloud/aws/machine-learning/","title":"Machine learning","text":""},{"location":"cloud/aws/machine-learning/#amazon-comprehend","title":"Amazon Comprehend","text":"<ul> <li>Uses Natural-Language-Processing (NLP) to help you understand the meaning and sentiment in your text</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-comprehend-medical","title":"Amazon Comprehend Medical","text":"<ul> <li>Detects and returns useful information in unstructured clinical text</li> <li>Uses NLP to detect Protected Health Information (PHI)</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-textract","title":"Amazon Textract","text":"<ul> <li>Uses ML to automatically extract text, handwriting, and data from scanned documents</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-kendra","title":"Amazon Kendra","text":"<ul> <li>Allows to create an intelligent search service powered by machine learning</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-forcast","title":"Amazon Forcast","text":"<ul> <li>Is a time-series forcasting service that uses machine learning and is build to give an important business insights</li> </ul>"},{"location":"cloud/aws/machine-learning/#fraud-detector","title":"Fraud Detector","text":"<ul> <li>An AI service that is built to detect fraud in your data </li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-transcribe","title":"Amazon Transcribe","text":"<ul> <li>Concert speech to text automatically</li> <li>You can use this service to generate subtitles on the fly</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-polly","title":"Amazon Polly","text":"<ul> <li>Turns text into lifelike speech and allows to create applications that talk to and interact with you using a variety   of languages and accents</li> <li>The opposite of Transcribe</li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-lex","title":"Amazon Lex","text":"<ul> <li>To build conversational interfaces in your applications using natural language models</li> </ul>"},{"location":"cloud/aws/machine-learning/#rekognition","title":"Rekognition","text":"<ul> <li>Recognition of images and videos using deep learning and neural network </li> <li>Find objects, people, text, scenes in images and videos using ML </li> </ul>"},{"location":"cloud/aws/machine-learning/#sagemaker","title":"SageMaker","text":"<ul> <li>Fully managed service for developers / data scientists to build ML models</li> <li>Components</li> <li>Ground truth </li> <li>Notebook </li> <li>Training </li> <li> <p>Inference </p> </li> <li> <p>Deployment types</p> </li> <li>Online usage </li> <li>Offline usage </li> <li>Stages </li> <li>Create a model </li> <li>Create an endpoint configuration </li> <li>Create an endpoint </li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-translate","title":"Amazon translate","text":"<ul> <li>Allows to translate from one language to another </li> </ul>"},{"location":"cloud/aws/machine-learning/#amazon-personalize","title":"Amazon Personalize","text":"<ul> <li>Fully managed ML service to build apps with real-time personalized recommendations</li> </ul>"},{"location":"cloud/aws/media/","title":"Media","text":""},{"location":"cloud/aws/media/#elastic-transcoder","title":"Elastic transcoder","text":"<ul> <li>Allows businesses and developers to convert (or transcode) media files from their original format into versions that   are optimized for various devices, such as smartphones, tablets, and PCs</li> </ul>"},{"location":"cloud/aws/media/#amazon-kinesis-video-streams","title":"Amazon Kinesis Video Streams","text":"<ul> <li>Stream media content from a large number of devices to AWS and then run analytics, machine learning (ML), playback,   and other processing</li> </ul>"},{"location":"cloud/aws/migration/","title":"Migration","text":""},{"location":"cloud/aws/migration/#snow-family","title":"Snow Family","text":"<ul> <li>Snowcone</li> <li>8 TB of storage, 4 GB of memory and 3 vCPUs</li> <li>IOT sensor migration</li> <li>Snowball Edge</li> <li>48 to 81 TB in storage</li> <li>Snowmobile</li> <li>100 PB of storage</li> </ul>"},{"location":"cloud/aws/migration/#storage-gateway","title":"Storage Gateway","text":"<ul> <li>Hybrid cloud storage service helps to merge on-premises resources with the cloud</li> <li>It can help with a one-time migration or a long-term pairing of your architecture with AWS</li> <li>Types</li> <li>S3 File Gateway<ul> <li>Configured S3 buckets are accessible using the NFS and SMB protocol</li> <li>Most recently used data is cached in the File gateway</li> <li>Bucket access using IAM roles for each File Gateway</li> <li>SMB Protocol has integration with Active Directory (AD) for user authentication</li> </ul> </li> <li>FSx File Gateway<ul> <li>Native access to Amazon FSx for Windows File Server</li> <li>Local cache for frequently accessed data</li> </ul> </li> <li>Volume Gateway<ul> <li>Block storage using iSCSI protocol backed by S3</li> <li>Two types</li> <li>Cached volumes</li> <li>Stored volumes</li> </ul> </li> <li>Tape Gateway<ul> <li>Some companies have backup process using physical tapes  </li> </ul> </li> </ul>"},{"location":"cloud/aws/migration/#aws-datasync","title":"AWS DataSync","text":"<ul> <li>Move large amount of data to and from</li> <li>On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API ...) - needs agent</li> <li>An agent-based solution for migrating on-premises storage to AWS</li> <li>It allows to easily move data between NFS and SMB shares and AWS storage solutions</li> </ul>"},{"location":"cloud/aws/migration/#aws-transfer-family","title":"AWS Transfer Family","text":"<ul> <li>Allows to easily move files in and out of S3 or EFS FTP Protocol</li> <li>It supports:</li> <li>File Transfer Protocol (FTP)</li> <li>File Transfer Protocol over SSL (FTPS)</li> <li>Secure Transfer Protocol (SFTP)</li> <li>Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)</li> </ul>"},{"location":"cloud/aws/migration/#migration-hub","title":"Migration Hub","text":"<ul> <li>Gives a single place to track the progress of your applications migration to AWS</li> <li>It integrates with Server Migration Service (SMS) and Database Migration Service (DMS)</li> </ul>"},{"location":"cloud/aws/migration/#aws-application-discovery-service","title":"AWS Application Discovery Service","text":"<ul> <li>Helps plan (track) migrations to AWS via collection of usage and configuration data from on-premise servers</li> <li>Two types:</li> <li>Agentless Discovery (AWS Agentless Discovery Connector)</li> <li>Agent-based Discovery (AWS Application Discovery Agent)</li> </ul>"},{"location":"cloud/aws/migration/#aws-application-migration-service","title":"AWS Application Migration Service","text":"<ul> <li>Is the primary migration service recommended for lift-and-shift migrations to AWS</li> <li>AWS encourages customers who are currently using AWS Elastic Disaster Recovery to switch to AWS MGN for future migrations</li> <li>AWS MGN enables organizations to move applications to AWS without having to make any changes to the applications, their architecture, or the migrated servers</li> </ul>"},{"location":"cloud/aws/migration/#aws-database-migration-service","title":"AWS Database Migration Service","text":"<ul> <li>Helps you migrate databases to AWS quickly and securely</li> <li>The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database</li> <li>Can migrate your data to and from the most widely used commercial and open-source databases</li> <li>Supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different    database platforms, such as Oracle or Microsoft SQL Server to  Amazon Aurora.</li> <li>Can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data    warehouse by streaming data to Amazon Redshift and Amazon S3</li> <li>AWS Schema Conversion Tool (SCT):</li> <li>Convert db schema from one engine to another</li> </ul>"},{"location":"cloud/aws/migration/#aws-backup","title":"AWS Backup","text":"<ul> <li>Fully managed service</li> <li>Centrally manage and automate backups across AWS services</li> <li>No need to create scripts and manual processes</li> <li>Supports cross-region backups</li> <li>Supports cross-account backups</li> </ul>"},{"location":"cloud/aws/tools/","title":"Tools","text":"<ul> <li>AWS DataSync</li> <li>Multipart upload</li> <li>Database copy aws ? </li> <li>AWS Import/Export</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/","title":"Identity Access Management (IAM)","text":""},{"location":"cloud/aws/00-Security/IAM/#root-account","title":"Root account","text":"<ul> <li>Has full access to all services</li> <li>The account must be secured (Use MFA to do it)</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#permission","title":"Permission","text":"<ul> <li>It's defined using JSON file</li> <li>Explicit deny overwrite Allow everywhere</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#user","title":"User","text":"<ul> <li>Identity to be identified in AWS account</li> <li>Tow types access:</li> <li>Programmatic</li> <li>AWS management access</li> <li>Once new user is created =&gt; new url to login into the account will be generated</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#group","title":"Group","text":"<ul> <li>Set of users</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#role","title":"Role","text":"<ul> <li>Is an IAM entity that defines a set of permissions for making AWS service request</li> <li>Ex: give access to EC2 full access to S3</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#policy","title":"Policy","text":"<ul> <li>To assign a permission to a users, group, or resource we create a policy, which is a document which explicitly lists permissions (the actions)</li> <li>Identity based policy: attached to IAM user, group or role</li> <li>Resource based policy: attached to a resource</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#policy-types","title":"Policy Types","text":"<ul> <li>Managed</li> <li>Created by AWS</li> <li>Read only</li> <li>Useful when we need to assign them to many users</li> <li>Customer</li> <li>Created by the user</li> <li>Are readable</li> <li>Useful when we need to assign them to many users</li> <li>Inline</li> <li>Attached directly to the user</li> <li> <p>Useful for exception permissions</p> </li> <li> <p>Principal: an entity that can perform action or access a service </p> </li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#credentials","title":"Credentials","text":"<ul> <li>Account can be accessed using: User/PWD or Access Key (from cmd)</li> <li>STS (Security Token Service): allows to give temporary access between services</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#access-keys","title":"Access Keys","text":"<ul> <li>Allows programmatically access to AWS</li> <li>Contains</li> <li>Access Key ID</li> <li>Secret Access Key</li> <li>A user can have only two access key</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<ul> <li>User to turn it on, administrator can not enable it</li> <li>Administrator can restrict access for example to user that only have MFA</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#monitoring","title":"Monitoring","text":"<ul> <li>Access Analyzer: identify unintended access</li> <li>Credential Report: for auditing and compliance</li> </ul>"},{"location":"cloud/aws/00-Security/IAM/#best-practices","title":"Best practices","text":"<ul> <li>Create a strong password for your AWS resources</li> <li>Use a group email alias with your AWS account</li> <li>Enable multi-factor authentication</li> <li>Set up AWS IAM users, groups, and roles for daily account access</li> <li>Delete your account\u2019s access keys</li> <li>Enable CloudTrail in all AWS regions</li> </ul>"},{"location":"cloud/aws/00-Security/cli/","title":"AWS CLI","text":"<ul> <li>Install with pip  <code>pip install --user awscli</code></li> <li>Configure<ul> <li>Add aws to path: <code>export PATH=$PATH:$HOME/.local/bin</code></li> <li>Add secret key: <code>aws configure</code></li> </ul> </li> <li>Credential will be added in the file <code>$HOME/.aws/credentials</code></li> </ul>"},{"location":"cloud/aws/00-Security/cloudTrail/","title":"CloudTrail","text":"<ul> <li>Provides governance, compliance and audit for you AWS Account</li> <li>Enabled by default</li> <li>Get a history of events / API calls made within your AWS Account by:</li> <li>Console</li> <li>SDK</li> <li>CLI</li> <li>AWS Services</li> <li>Can put logs from CloudTrail into CloudWatch Logs</li> <li> <p>If a resource is deleted in AWS, look into CloudTrail first</p> </li> <li> <p>Only the past 90 days of activity</p> </li> <li>Default show only Create, Modify or Delete events</li> <li>Can:</li> <li>Get a detailed list of all events you chose</li> <li>Ability to store these events in S3 for further analysis</li> <li>Can be region specific or global</li> <li>CloudTrail Logs have SSE-S3 encryption when placed into S3</li> <li>Control access to S3 using IAM, Bucket Policy, etc ...</li> <li>Near real-time intrusion detection</li> </ul>"},{"location":"cloud/aws/00-Security/cloudTrail/#best-practices","title":"Best practices","text":"<ul> <li>Configure CloudTrail in all AWS accounts and Regions</li> <li>Set up separate trails for different use cases</li> <li>Enable MFA-delete and versioning on the Amazon S3 Bucket storing log files</li> <li>Enable CloudTrail log file integrity validation</li> <li>Encrypt CloudTrail log files at rest</li> <li>Use advanced event selectors with data events</li> <li>Integrate CloudTrail with Amazon CloudWatch Logs</li> <li>Use CloudTrail Insights to monitor anomalous API activity</li> <li>Use AWS Config rules to meet CloudTrail standards for the CIS AWS Foundations Benchmark controls</li> </ul> <p>See https://aws.amazon.com/fr/blogs/mt/aws-cloudtrail-best-practices/</p>"},{"location":"cloud/aws/00-Security/cognito/","title":"Cognito","text":"<ul> <li>Decentralized managed way for authentication</li> <li>Sign-in, sing-up for you app</li> <li>Social identity provider Facebook,..</li> </ul>"},{"location":"cloud/aws/00-Security/cognito/#components","title":"Components","text":"<ul> <li>User pools</li> <li>Decentralized repository of users</li> <li> <p>Handles the actions</p> <ul> <li>Sign-in</li> <li>Sign-up</li> <li>Account recovery</li> <li>Account confirmation</li> </ul> </li> <li> <p>Identity pools</p> </li> <li>Provides temporary AWS credentials to access services eg. S3, DynamoDB, ...</li> <li>It is generated using SDK</li> </ul>"},{"location":"cloud/aws/00-Security/cognito/#cognito-workflow","title":"Cognito Workflow","text":"<ol> <li>Authenticate and get tokens</li> <li>Exchange tokens and get AWS credentials</li> <li>Access AWS service using credentials</li> </ol> <ul> <li>Web Identity Provider: exchange identity and security information between IdP and Application</li> <li>Identity Provider: Facebook, GitHub,...</li> <li>Types</li> <li>SAML is of type Single Sign On (SSO)</li> <li>OpenID</li> <li>OAuth</li> </ul> <ul> <li>Cognito Sync</li> <li>Sync user data  and preferences across devices with one line of code</li> <li> <p>Uses:</p> <ul> <li>Push synchronization to push updates and synchronize data</li> <li>Simple Notification Service (SNS) to send notifications to all user devices when data in cloud changes</li> </ul> <p></p> </li> </ul>"},{"location":"cloud/aws/00-Security/cognito/#conclusion","title":"Conclusion","text":""},{"location":"cloud/aws/00-Security/common-security-tools/","title":"Security Services","text":""},{"location":"cloud/aws/00-Security/common-security-tools/#aws-shield","title":"AWS Shield","text":"<ul> <li>Network protection service</li> <li>Offer free DDOS protection</li> <li>Protect against layer 3 and layer 4 attacks only</li> <li>Protects all AWS customers on ELB, CloudFront and Route S3</li> <li>Protects against SYN/UDP floods, reflection attacks, and other Layer 3 and Layer 4 attacks</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#ssm-parameter-store","title":"SSM Parameter Store","text":"<ul> <li>Secure storage for configuration and secrets</li> <li>Optional Seamless Encryption using KMS</li> <li>Serverless, scalable, durable, easy SDK</li> <li>Version tracking of configuration / secrets</li> <li>Security through IMA</li> <li>Notification with Amazon EventBridge</li> <li>Integration with CloudFormation</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#aws-secret-manager","title":"AWS Secret Manager","text":"<ul> <li>Store securely application secrets, database credentials, API Keys, SSH Keys, passwords, etc</li> <li>Rotation can be used  </li> <li>Automate generation of secrets on rotation</li> <li>Integrated with Amazon RDS</li> <li>Replicate Secrets across multiple AWS Regions</li> <li>It keeps read replicas in sync with primary Secret</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#aws-audit-manager","title":"AWS Audit Manager","text":"<ul> <li>Continually audit your AWS usage</li> <li>Automated service that produces report to auditors for PCI, GDPR and more</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#guardduty","title":"GuardDuty","text":"<ul> <li>A threat detection service that uses machine learning to continuously monitor for malicious behavior</li> <li>Looks for</li> <li>Unusual API calls, calls from a known malicious IP</li> <li>Attempts to disable CloudTrail logging</li> <li>Unauthorized deployments</li> <li>Compromised instances</li> <li>Reconnaissance by would-be attackers</li> <li>Port scanning, failed logins</li> <li>Log analyzed</li> <li>CloudTrail logs</li> <li>VPC Flow logs  </li> <li>DNS logs</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#macie","title":"Macie","text":"<ul> <li>Uses machine learning and pattern matching to discover sensitive data stored in S3</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#amazon-inspector","title":"Amazon Inspector","text":"<ul> <li>Is an automated security assessment service that helps improve the security and compliance of application deployed on AWS</li> <li>Inspects the network, EC2 instances, Amazon ECR and Lambda Functions</li> <li>Two types of assessment</li> <li>Network assessment</li> <li>Host assessment</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#cloud-hsm","title":"Cloud HSM","text":"<ul> <li>KMS =&gt; AWS Manages software for encryption</li> <li>CloudHSM =&gt; AWS provisions encryption hardware</li> <li>Dedicated Hardware (HSM = Hardware Security Module)</li> <li>You manage your own encryption keys entirely (not AWS)</li> <li>The CloudHSM hardware device is tamper resistant</li> <li>FIPS 140-2 Level 3 compliance</li> <li>CloudHSM cluster are spread across multi AZ (HA)</li> <li>Supports both symmetric and asymmetric encryption (SSL/TLS keys)</li> <li>No free tier available</li> <li>Must use the CloudHSM Client Software</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#presigned-urls","title":"Presigned URLS","text":"<ul> <li>Generate an url which provides temporary access to an object to either upload or download object data</li> <li>They are commonly used to provide access to private objects</li> <li>You can use AWS CLI or AWS SDK to generate presigned urls</li> <li>Used to share private buckets in S3</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#aws-certificate-manager-acm","title":"AWS Certificate Manager (ACM)","text":"<ul> <li>Allows to create, manage, and deploy public and private SSL certificates for use with other AWS services</li> <li>No more paying for SSL certificates. You pay for the resources that utilize your certificates (such as ELB)</li> <li>Renew and deployment can be automated</li> <li>Easy to set up</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#aws-artifact","title":"AWS Artifact","text":"<ul> <li>Single source you can visit to get the compliance-related information that matters to you, such as AWS security and   compliance reports or select online agreements</li> <li>Free service</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#amazon-detective","title":"Amazon Detective","text":"<ul> <li>Analyze and investigate and quickly identify the root cause of potential security issue or suspicious activities</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#firewall-manager","title":"Firewall manager","text":"<ul> <li>Is security management service in single pane of glass</li> <li>This allows to centrally set up and manage firewall rules across multiple AWS accounts and applications in AWS Organizations</li> <li>It is integrated with AWS Organizations so you can enable AWS WAF rules, AWS Shield Advanced protection, security groups, AWS Network Firewall rules, and Amazon Route 53 Resolver DNS Firewall rules</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#network-firewall","title":"Network firewall","text":"<ul> <li>Managed service that makes it easy to deploy physical firewall protection across your VPCs</li> </ul>"},{"location":"cloud/aws/00-Security/common-security-tools/#security-hub","title":"Security Hub","text":"<ul> <li>A single place to view all your security alerts from services like Amazon GuardDuty, Inspector, Macie   and AWS Firewall</li> <li>Performs security best practice checks</li> <li>Aggregates alerts</li> <li>Enables automated remediation</li> </ul>"},{"location":"cloud/aws/00-Security/encryption/","title":"Encryption","text":"<ol> <li>Encryption on fly (SSL)</li> <li>Data encrypted before sending and decrypted when received by the server</li> <li>To protect against the MIMD (Man in the Middle Attack)</li> <li> <p>SSL certificates help to secure HTTPS</p> </li> <li> <p>Server side encryption at rest</p> </li> <li>Data is encrypted after being received by the server </li> <li>Data decrypted before being sent </li> <li> <p>It is stored in an encrypted form thanks to key (usually a data key) </p> </li> <li> <p>Client side encryption</p> </li> <li> <p>Data encrypted by the client and never decrypted by the server</p> </li> <li> <p>SSE-S3</p> </li> <li> <p>SSE-KMS</p> </li> <li> <p>SSE-C</p> </li> <li> <p>Client Side Encyption</p> </li> <li> <p>Encyption in tansit (SSL)</p> </li> </ol>"},{"location":"cloud/aws/00-Security/identityFederation/","title":"Identity Federation","text":"<ul> <li>Let outside users of AWS to assume temporary role for accessing AWS resource</li> <li>These users assume identity provided access role</li> <li>Federation assumes a form of 3rd party authentication, could be:</li> <li>LDAP</li> <li>Microsoft Active Directory (~= SAML)</li> <li>Single Sign On</li> <li>Open ID</li> <li> <p>Cognito</p> </li> <li> <p>Using federation, you don't need to create IAM users (user management is outside of AWS)</p> </li> <li> <p>QST May ask for given identity federation</p> </li> <li>SAML Federation</li> <li>Custom Identity Broker Application</li> <li> <p>AWS Cognito - Federation Identity Pools</p> <p></p> </li> </ul>"},{"location":"cloud/aws/00-Security/kms/","title":"Key Management Service (KMS)","text":"<ul> <li>Data protection service</li> <li>Create and control (manages) keys used to encrypt the data</li> <li>Fully managed</li> <li>Fully integrated with IAM for authorization</li> <li>Able to audit KMS Key usage using CloudTrail</li> <li>Seamlessly integrated into most AWS service (EBS, S3, ...)</li> <li>Keys</li> <li>AWS-owned keys</li> <li>AWS-managed keys</li> <li>Customer-managed keys</li> <li>Key Types</li> <li>Symmetric (AES-256 keys)</li> <li>Asymmetric (RSA &amp; ECC key pairs)</li> </ul>"},{"location":"cloud/aws/00-Security/sts/","title":"Security Token Service (STS)","text":"<ul> <li>Allows to grant limited and temporary access to AWS resources</li> <li>Token is valid for up to one hour (must be refreshed)</li> <li>Is the service that you can use to create and provide trusted users with temporary security credentials   that can control access to your AWS resources.</li> <li>Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use.</li> <li>You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security   credentials that can control access to your AWS resources.</li> <li>Temporary security credentials work almost identically to the long-term access key credentials that your IAM users   can use, with the following differences:</li> <li>Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours.</li> <li>After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.</li> <li>Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested.</li> <li>When (or even before) the temporary security credentials expire, the user can request new credentials,   as long as the user requesting them still has permissions to do so.</li> </ul>"},{"location":"cloud/aws/00-Security/waf/","title":"AWS Web Application Firewall","text":"<ul> <li>Monitor the HTTP and HTTPS request that are forwarded to Amazon CloudFront</li> <li>Can configure conditions such as what IP addresses are allowed to make this request or what query string parameters need to be passed for the request to be allowed</li> <li>Operates \u00e0 layer 7</li> </ul>"},{"location":"cloud/aws/01-Storage/FSx/","title":"FSx","text":"<ul> <li>Launch 3rd party high-performance file system on AWS</li> <li>Fully managed service</li> <li>Can be used:</li> <li>FSx for Luster</li> <li>FSx for Windows File Server</li> <li>FSx for NetApp ONTAP</li> <li> <p>FSx for OpenZFS</p> </li> <li> <p>Persistent File System: Provides long-term storage where data is replicated within the same AZ. Failed    files were replaced within minutes.</p> </li> <li>Scratch File System</li> </ul>"},{"location":"cloud/aws/01-Storage/ebs/","title":"EBS (Elastic Block Store)","text":"<ul> <li>Storage volumes to attach EC2 instances to</li> <li>Encryption is disable by default</li> <li>Encryption can be enabled for all EBS at EC2 settings</li> <li>Snapshot from encrypted volume is always encrypted</li> <li>Instance created from encrypted AMI are also encrypted</li> <li>Encryption doesn't impact instance performance</li> <li>EBS Encryption</li> <li> <p>Data is encrypted at-rest and in-transit</p> </li> <li> <p>EBS-Optimization</p> </li> <li> <p>Separate Storage Path from Network Path</p> </li> <li> <p>Types</p> </li> <li>Solid State Drives (SSD)<ul> <li>General Purpose SSD (gp2): suitable for boot disks and general applications</li> <li>General Purpose SSD (gp3): suitable for high performance applications</li> <li>Provisioned IOP (io1): for OLTP and latency-sensitive applications</li> <li>Provisioned IOP (io2): for OLTP and latency-sensitive applications (with grate nb of IOPS)</li> </ul> </li> <li> <p>Hard Disk Drives (HDD) </p> <ul> <li>Throughput Optimized HDD: for bigdata, warehouse and ETL</li> <li>Cold HDD (sc1): less frequently accessed data and low cost applications</li> </ul> </li> <li> <p>When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard)</p> </li> <li>Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ. Each EC2 instance has full read/write permissions</li> <li>Multi-attach feature can only be enabled on EBS Provisioned IOPS io2 or io1 volumes</li> </ul>"},{"location":"cloud/aws/01-Storage/ebs/#volumes-and-snapshot","title":"Volumes and Snapshot","text":"<ul> <li>Volumes</li> <li>Are where the data is stored</li> <li>They are always in the same AZ as EC2</li> <li>Can be resized in the  fly</li> <li>Can change the volume type in the fly</li> <li>Snapshot</li> <li>It's a current state of the volume at point in time</li> <li>For consistent snapshot it's recommended to stop the instance  then take the snap</li> <li>Snapshot can only be shared in the region where they are created, to share them with other regions we need to copy them to those regions</li> <li>EBS fast snapshot restore</li> </ul>"},{"location":"cloud/aws/01-Storage/efs/","title":"Elastic File System (EFS)","text":"<ul> <li>Managed NFS (Network File System) that can be mounted on many EC2 instances</li> <li>Works with EC2 instances in multiple AZ</li> <li>Highly available and scalable, however it's expensive</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/","title":"S3 (Simple Storage Service)","text":"<ul> <li>Store file or object</li> <li>Define at region level</li> <li>Naming convention</li> <li>No uppercase</li> <li>No underscore</li> <li>3-63 char long</li> <li>Not an IP</li> <li>Must start with lowercase letter or number</li> <li>Objects (files) have Key. Key is the FULL path</li> <li>No concept of directories</li> <li>Size:</li> <li>Max size of object 5TB</li> <li>If uploading more thant 5GB, must use mutli-part upload</li> <li>Object have metadata (list of test key-value pairs)</li> <li>Object have Tag (Unicode key-value pair up to 10), useful for security and lifecycle</li> <li>Version ID (if versioning is enabled)</li> <li>Support any formats (bin, parquet, csv, ...)</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#storage-classes","title":"Storage classes","text":"<ul> <li>Standard</li> <li>High availability and durability (&gt;= 3 AZ)</li> <li>Designed for frequent access</li> <li>Suitable for most workloads</li> <li>Standard-IA</li> <li>Less frequently and rapid access  </li> <li>One Zone Infrequent Access</li> <li>Like Standard-IA but data is stored in single AZ</li> <li>Cost less 20% than standard-IA</li> <li>Intelligent-Tiering </li> <li>Some time accessed frequently and sometime not</li> <li>Glacier</li> <li>Glacier Instant Retrieval: provides long-term archiving data with instant retrieval time for your data</li> <li>Glacier Flexible Retrieval: ideal storage class for archive data that does not require immediate access but needs<ul> <li>Expedited (1 to 5 minutes)</li> <li>Standard (3 to 5 hours)</li> <li>Bulk (5 to 12 hours)   flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use case.Can be up minutes   or up to 12 hours</li> </ul> </li> <li>Glacier Deep Retrieval: cheapest storage class and designed for customers that retain data sets for 7-10 years<ul> <li>Standard (12 hours)</li> <li>Bulk (48 hours)   or longer to meet customer needs and regulator compliance requirements. The standard retrieval time is 12 hours, and   the bulk retrieval time is 48 hours</li> </ul> </li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#versioning","title":"Versioning","text":"<ul> <li>Enable in bucket level</li> <li>Same Key overwrite will increment the version: 1, 2, 3</li> <li>It is best practice to version your buckets</li> <li>Protect against unintended deletes (ability to restore a version)</li> <li>Easy roll back to previous version</li> <li>Any file that is not versioned prior to enabling versioning will have version null</li> <li>You can suspend versioning</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#management-lifecycle-rules","title":"Management Lifecycle Rules","text":"<ul> <li>Set of rules to move data between different tiers, to save storage cost</li> <li>Can be used in conjunction with versionning</li> <li>Can be applied to the current versions and previous versions</li> <li>Ex: General Purpose =&gt; Infrequent Access =&gt; Glacier</li> <li>Transaction Actions: configure objects to transition to another storage class</li> <li>Expiration Actions: configure object to expire (delete) after some time</li> <li>Rules can be created for a certain prefix (s3://bucketname/test/*)</li> <li>Rules can be created for certain objects Tags</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#object-lock","title":"Object Lock","text":"<ul> <li>Compliance mode</li> <li>Governance mode</li> <li>Legal Hold ??</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#s3-replication-cross-region-replication","title":"S3 Replication (Cross Region Replication)","text":"<ul> <li>Move bucket from one region to another</li> <li>Must enable versioning (source and destination)</li> <li>Buckets must be in different AWS regions</li> <li>Can be in different accounts</li> <li>Copying is async</li> <li>Must give proper IAM permissions to S3</li> <li>Only new objects are replicated, to enable for existing object use S3 Batch Replication</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#mfa-delete","title":"MFA delete","text":"<ul> <li>Ensures users cannot delete objects from a bucket unless they provided their MFA code</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#tags","title":"Tags","text":"<ul> <li>How do you verify if a file has already been uploaded to S3</li> <li>Names work, but how are you sure the file is exactly the same ?</li> <li>For this, you can use AWS ETags:</li> <li>For simple uploads (less than 5GB), it's the MD5 hash</li> <li>For Multi-part uploads, it's more complicated, no need to know the algorithm</li> <li>Using a tag, we can ensure integrity of files</li> <li>Delete markers aren't replicated by default</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#requester-pays","title":"Requester Pays","text":"<ul> <li>The requester instead of the bucket owner pays the cost of the request and the data download from the bucket</li> <li>The requester must be authenticated in AWS (cannot be anonymous)</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#event-notifications","title":"Event Notifications","text":"<ul> <li>React to events that are happening in S3 (create, delete, ... object)</li> <li>We need to have IAM Permissions</li> <li>Targets</li> <li>SNS</li> <li>SQS</li> <li>Lambda Function</li> <li>EventBridge can also be used</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#performance","title":"Performance","text":"<ul> <li>Multi-Part upload</li> <li>S3 Transfer accelerator</li> <li>Byte Range Fetch</li> <li>S3 Select &amp; Glacier Select</li> <li>S3 Batch Operations: perform bulk operations on existing S3 objects with a single request, example:</li> <li>modify object metadata</li> <li>Copy objects between S3 buckets</li> <li>Encrypt un-encrypted objects</li> <li>Modify ACLs, tags</li> <li>Restore objects from S3 Glacier</li> <li>Prefix: spread our reads across multiple prefixes</li> <li>KMS come in with limits</li> <li>Multipart upload</li> <li>S3 Byte range fetches</li> <li>S3 Analytics ??</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#security","title":"Security","text":""},{"location":"cloud/aws/01-Storage/s3/#encryption","title":"Encryption","text":"<ul> <li>At rest: Server-side encryption</li> <li>SSE-S3:<ul> <li>Encryption using keys handled, managed and owned by AWS</li> <li>Object is encrypted server-side</li> <li>Encryption type is AES-256</li> <li>Must set header \"x-amz-server-side-encryption\": \"AES256\"</li> <li>Enabled by default for new buckets and new objects</li> </ul> </li> <li>SSE-KMS<ul> <li>Encryption using keys handled and managed by AWS KMS</li> <li>??</li> </ul> </li> <li>SSE-C: customer provide keys</li> <li>At rest: Client-side encryption</li> <li> <p>You encrypt your files before uploading them to S3  </p> </li> <li> <p>In transit</p> </li> <li>SSL/TLS</li> <li>HTTPS</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#acls","title":"ACLs","text":"<ul> <li>Works on individual object level</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#bucket-policies","title":"Bucket policies","text":"<ul> <li>Works on entire bucket level</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#s3-object-lambda","title":"S3 Object Lambda","text":"<ul> <li>Use to change object before it is retrieved by the caller application</li> </ul>"},{"location":"cloud/aws/01-Storage/s3/#princing","title":"Princing","text":"<p>You don't pay for data:</p> <ul> <li>In to S3 from the internet</li> <li>Out to EC2 in the same region</li> <li>Out to CloudFront</li> </ul>"},{"location":"cloud/aws/02-Compute/api-gateway/","title":"API Gateway","text":"<ul> <li>Fully managed service</li> <li>Allows to easily publish, create, maintain, monitor and secure your API. </li> </ul>"},{"location":"cloud/aws/02-Compute/api-gateway/#endpoint-types","title":"Endpoint Types","text":"<ul> <li>Edge-Optimized (default)</li> <li>Requests are routed through the CloudFront Edge locations (improves latency)</li> <li>The API Gateway still lives in only one region </li> <li>Regional</li> <li>For clients within the same region </li> <li>Cloud manually combine with CloudFront (more control over the caching strategies and distribution)</li> <li>Private </li> <li>Can only be accessed from your VPC using an interface VPC endpoint (ENI)</li> </ul>"},{"location":"cloud/aws/02-Compute/batch/","title":"AWS Batch","text":"<ul> <li>Jobs</li> <li>Job definitions</li> <li>Job Queues</li> <li>Compute environment</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/","title":"EC2","text":"<ul> <li>IaaS</li> <li>Regional service</li> <li>Has an AMI (Amazon Machine Image)</li> <li>Can use Instance Store Volume: they are not persistent and not secure</li> <li>Instance families</li> <li>General purpose: great for diversity of workloads such as web servers or code repositories<ul> <li>Balance between: memory, storage and networking</li> </ul> </li> <li>Compute optimized: for compute-intensive tasks that require high performance processors </li> <li>Memory optimized: fast performance for workloads that process large datasets in memory </li> <li>Storage optimized: for storage intensive tasks that require high, sequential read and write access to large data      sets on local storage </li> <li>Accelerated computing</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#ec2-princing","title":"EC2 Princing","text":"<ul> <li>On-demand</li> <li>Low cost and flexibility of Amazon EC2 without any upfront payment or long-term commitment </li> <li>Good fot the short-term (test use cases)</li> <li>Reserved Instances (RI)</li> <li>Spot Instances: can be terminated at any time</li> <li>Dedicated</li> <li>Most expensive</li> <li>Multi-Tenant (virtual isolation with other consumers) vs Single Tenant (physical isolation with other consumers)</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#instance-store-volume","title":"Instance store volume","text":"<ul> <li>Volume attached to EC2</li> <li>Offers some performance because is located in the same host as the instance</li> <li>It's ephemeral, if the instance remove the storage is removed too</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#elastic-block-store-ebs","title":"Elastic Block Store (EBS)","text":"<ul> <li>A virtual hard drive in the cloud</li> <li>Create new volumes attach to EC2 instances</li> <li>Backup via snapshots</li> <li>Can be encrypted</li> <li>IOPS ??</li> <li>Types ??</li> <li></li> <li>Size and the type can be changed without downtime</li> <li>Can be copied from zone to author one</li> <li>Snapshot</li> <li>Create backup of the volume</li> <li>Data is copied from EBS to S3, and they are replicated</li> <li>Best practice: shutdown the instance before making the snapshot</li> <li>Can be private or public</li> <li>Stores only the difference between two successes!! snapshot</li> <li>Data Lifecycle Manager</li> <li>Used to automate the creation of the snapshot</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#security-group-sg","title":"Security Group (SG)","text":"<ul> <li>A maximum Five Security Group per Elastic Network Interface (ENI)</li> <li>Two types of rules: Inbound and Outbound</li> <li>Important the two rules have Hidden Rule which is Default Deny Rule</li> <li>The traffic is stateful. We have juste to allow it in one direction</li> <li>Can reference another SG</li> <li>An SG can reference itself. Allow the services in the same groups to communicate between them</li> <li>You can change the security groups for an instance when the instance is in the running or stopped state</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#user-data","title":"User data","text":"<ul> <li>It's a boostrap script</li> <li>Uses th url: http://169.254.169.254/latest/user-data</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#instance-metadata","title":"Instance Metadata","text":"<ul> <li>Allows an application to get the information about the instance</li> <li>Uses th url: http://169.254.169.254/latest/meta-data</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part1/#auto-scaling","title":"Auto-Scaling","text":"<ul> <li>Define a template</li> <li>Network and purchasing</li> <li>ELB configuration</li> <li>Set scaling policy</li> <li>Dynamic scaling<ul> <li>Target Tracking Scaling: Increase and decrease the current capacity of the group based on a Amazon CloudWatch metric and a target value</li> <li>Step Scaling: increase and decrease based on a set of scaling adjustment</li> <li>Simple Scaling (Not recommended): increase and decrease based on a single scaling adjustment, with cooldown period between each scaling activity</li> </ul> </li> <li>Scheduled scaling<ul> <li>Anticipate scaling based on known usage pattern (at 10 am increase capacity to 5 instances)</li> </ul> </li> <li>Predictive scaling<ul> <li>Continuously forecast load and schedule scaling ahead</li> </ul> </li> <li>Notification</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/","title":"EC2","text":""},{"location":"cloud/aws/02-Compute/ec2-part2/#ami-amazon-machine-image","title":"AMI (Amazon Machine Image)","text":"<ul> <li>AMIs are built for a specific AWS Region, they're unique for each AWS Region. You can't launch an EC2 instance using   an AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create   your EC2 instances.</li> <li>Contains</li> <li>Snapshots, permissions and block device mapping</li> <li>Best practice: Stop the image before creating the image</li> <li>Can be:</li> <li>Private</li> <li>Public</li> <li>We can add account to use the image</li> <li>Advantage of using AMI</li> <li>IAM pre-backing ??</li> <li>Immutable IAM ??</li> <li>Used when we need just static provisioning</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#boostrap","title":"Boostrap","text":"<ul> <li>Give an EC2 instance a series of instructions to do (Customize an instance)</li> <li>We can use:</li> <li>Bash (linux) powershell (windows) file</li> <li>Cloudinit file</li> <li>Used to dynamically provision an instance</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#network","title":"Network","text":""},{"location":"cloud/aws/02-Compute/ec2-part2/#elastic-ip","title":"Elastic IP","text":"<ul> <li>Allows to allocate IP address to an instance</li> <li>You can only have 5 Elastic IP in the account (Ask AWS to increase the number)</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#placements-group","title":"Placements group","text":"<p>A way to logically grouping instances</p> <ul> <li>Cluster</li> <li>1 AZ</li> <li>Good for High Performance Application</li> <li>Spread</li> <li>Each instance is placed on a different rack</li> <li>When critical instances should be keep separate from each other</li> <li>You can spread a max 7 instances. Spreads can be multi-AZ</li> <li>Partition</li> <li>Spread instances across partitions</li> <li>Each partition do not share the underlying hardware with each other (rack per partition)</li> <li>Well suite for large distributed and replicated workloads (Hadoop, Cassandra, Kafka)</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#eni-elastic-network-interface-ip-and-dns","title":"ENI (Elastic Network Interface), IP, and DNS","text":"<ul> <li>For day-to-day networking (Which gives EC2 instance networking connectivity)</li> <li>It provides</li> <li>Private IPv4 addresses (one primary private IPv4 and one or more secondary private IPv4)</li> <li>Public IPv4 address</li> <li>Many IPv6 address</li> <li>MAC address</li> <li> <p>One or more Security Groups</p> </li> <li> <p>Public instance</p> </li> <li>Has public IP</li> <li>Public IP is changed when an instance is stopped</li> <li>Private instance</li> <li>Private IP doesn't change</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#en-enhanced-network","title":"EN (Enhanced Network)","text":"<ul> <li>Uses single I/O virtualization to provide high performance</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#efa-elastic-fabric-adapter","title":"EFA (Elastic Fabric Adapter)","text":"<ul> <li>Accelerates High Performance Computing (HPC) and machine learning applications</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#launch-template","title":"Launch Template","text":"<ul> <li>IAM type</li> <li>EC2 role</li> <li>SG</li> </ul>"},{"location":"cloud/aws/02-Compute/ec2-part2/#outposts","title":"Outposts","text":""},{"location":"cloud/aws/02-Compute/lambda/","title":"Lambda","text":""},{"location":"cloud/aws/02-Compute/lambda/#lambda","title":"Lambda","text":"<ul> <li>Serverless compute service that lets run code without provisioning or managing the underlying servers </li> <li>Billed for resources assigned and the code run time</li> <li>Limitations:</li> <li>Execution<ul> <li>Memory allocation 128 MB to 10GB (1 MB increment)</li> <li>Maximum execution time 900 seconds (15 minutes)</li> <li>Environment variables (4 KB) </li> <li>Disk capacity in the function container (in /tmp): 512 MB to 10 GB</li> <li>Concurrency executions: 1000 (can be increased)</li> </ul> </li> <li> <p>Deployment</p> <ul> <li>Lambda function deployment size (compressed .zip): 50MB</li> <li>Size of uncompressed deployment (code + dependencies): 250 MB </li> <li>Can use the /tmp directory to load other files at startup</li> <li>Size of environment variables: 4KB</li> </ul> </li> <li> <p>Lambda@Edge is a feature of CloudFront that lets you run code closer to your users, which improves performance  and reduces latency.</p> </li> </ul>"},{"location":"cloud/aws/02-Compute/lambda/#serverless-application-repository","title":"Serverless Application Repository","text":"<ul> <li>Allows users to easily find, deploy or even publish their own serverless applications</li> <li>Uses AWS SAM template </li> <li>Two actions </li> <li>publish </li> <li>deploy </li> </ul>"},{"location":"cloud/aws/02-Compute/lambda/#best-practices","title":"Best practices","text":"<ul> <li>Separate business logic </li> <li>Modular function</li> <li>Treact function as stateless</li> <li>Only include what you need</li> </ul>"},{"location":"cloud/aws/03-Network/cloud-front/","title":"CloudFront","text":"<ul> <li>Is used Content Distributed Network (CDN)</li> <li>Take content and copy it to different edges location around the world</li> <li>For example if we have some videos (static content, and we want to share them around the word), share buckets, ...</li> <li>Components</li> <li>Origin: the location where all of original files are located. It can be S3, EC3, ELB, Route 53</li> <li>Edge location: the location where content will be cached</li> <li>Distribution : collection of Edge Locations which defines how cached content should behave</li> <li>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency</li> </ul>"},{"location":"cloud/aws/03-Network/cloud-front/#content-delivery-network-cdn","title":"Content Delivery Network (CDN)","text":"<ul> <li>Distributed network of servers which delivers web pages and content to users base on their geographical location, the   origin of the webpage, and a content delivery server</li> </ul>"},{"location":"cloud/aws/03-Network/elb/","title":"Elastic Load Balancer","text":"<ul> <li>Route the traffic to your instances that are healthy</li> <li>Must have at least two Availability Zone</li> <li>Cannot go cross-region. You must create one per region</li> <li>Can attach Amazon Certification Manager SSL to any ELB for SSL</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#application-load-balancer","title":"Application Load Balancer","text":"<ul> <li>Operates on the layer 7 of OSI model</li> <li>Listener: checks for connection requests from clients, using the protocol and port you configure</li> <li>Rule</li> <li>Target Group</li> <li>Target</li> <li>Supports only HTTP,  HTTPS and WebSocket</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#network-load-balancer","title":"Network Load Balancer","text":"<ul> <li>Operate at layer 4</li> <li>Use where we need extreme performance</li> <li>Other use cases are where you need protocols not supported by Application Load Balancers </li> <li>It can decrypt traffic, but you will need to install the certificate on the load balancer</li> <li>Support TCP and HTTP health check</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#classic-load-balancer","title":"Classic Load Balancer","text":"<ul> <li>A 504 error means the gateway has timed out </li> <li>Need the IPv4 address of your end use: look for the X-Forwarded-For header</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#sticky-sessions","title":"Sticky Sessions","text":"<ul> <li>Allow to bind a user's session to a specific EC2 instance</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#x-forwarded-for","title":"X-Forwarded-For","text":"<ul> <li>Allow to determine users IP </li> <li>X-Forwarded-For: get IP address</li> <li>X-Forwarded-Port: get the port</li> <li>X-Forwarded-Proto: get the protocol</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#registration-delay","title":"Registration Delay","text":"<ul> <li>Allows load balancer to keep existing connection open if the EC2 instance are de-registered or become unhealthy</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#health-checks","title":"Health Checks","text":"<ul> <li>Communicates with instances to determine their state</li> <li>ELB does not terminate (kill) unhealthy instance. It will just redirect traffic to healthy instance</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#cross-zone-load-balancer","title":"Cross-Zone Load Balancer","text":"<ul> <li>Each Load Balancer instance distributes evenly across all registered instances in all AZ</li> <li>ALB </li> <li>Enabled by default (can be disabled at The Target Group level)</li> <li>No charges for inter AZ data</li> <li>NLB and GLB</li> <li>Disabled by default</li> <li>You pay charges for inter AZ data if enabled </li> <li>CLB</li> <li>Disabled by default</li> <li>No charges for inter AZ data</li> </ul>"},{"location":"cloud/aws/03-Network/elb/#auto-scaling-group","title":"Auto Scaling Group","text":"<ul> <li>Scale out (add EC2 instances) to match an increased load  </li> <li>Scale in (remove EC2 instances) to match a decreased load</li> </ul>"},{"location":"cloud/aws/03-Network/route53/","title":"Route 53","text":"<ul> <li>A highly available, scalable, fully managed and authoritative DNS</li> <li>It is also Domain Registrar</li> <li>It's global service</li> <li>It allows to register domain names, create hosted zones, and manage and create DNS records</li> <li>Hosted Zone: is a container for records that define how to route traffic to a domain and its subdomains</li> <li>CNAME: map domain name to another (example.net --&gt; example.com)</li> <li>Alias record: is specifically used to point a domain apex (the root domain itself, such as example.com) or a subdomain to another DNS name, including third-party DNS names or AWS resources</li> <li>NS Record: is where DNS information are stored</li> <li>An A Record</li> <li>Allows to convert the name of a domain directly into an IP address</li> <li>They can also be used on the root (naked domain) itself</li> <li>AAAA</li> <li>maps a hostname to IPv6</li> <li>Supports IPv4 and IPv6</li> <li>7 Routing policies are available</li> <li>TTL (time-to-live) is the length of time that a DNS record get cached on the resolving server or the users own local machine</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#policies","title":"Policies","text":""},{"location":"cloud/aws/03-Network/route53/#simple-routing-policy","title":"Simple Routing Policy","text":"<ul> <li>You can have one record with multiple IP addresses</li> <li>If you specify multiple values in a record, Route 53 return all values to the user in random order</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#weighted-routing-policy","title":"Weighted Routing Policy","text":"<ul> <li>Allows to split the traffic based on different weights assigned</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#failover-routing-policy","title":"Failover Routing Policy","text":"<ul> <li>Used to create an active/passive setup</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#geolocation-routing-policy","title":"Geolocation Routing Policy","text":"<ul> <li>Choose where the traffic will be sent based on the geographic location of your users</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#geo-proximity-routing-policy","title":"Geo-proximity Routing Policy","text":"<ul> <li>Route traffic to the resources based on the geographic location of your uses and your resources</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#latency-routing-policy","title":"Latency Routing Policy","text":"<ul> <li>Allows to route traffic based on the lowest latency for your end user</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#multivalue-routing-policy","title":"MultiValue Routing Policy","text":"<ul> <li>Return multiple values, such as IP addresses for your web servers, in response to DNS queries</li> <li>It's like a Simple routing using Health check</li> </ul>"},{"location":"cloud/aws/03-Network/route53/#traffic-flow","title":"Traffic Flow","text":"<ul> <li>A visual editor lets you create sophisticated routing configuration for your resources using existing routing types</li> <li>Supports versionning so can roll out or roll back updates</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/","title":"Virtual Private Cloud (VPC)","text":"<ul> <li>Logically isolated part of cloud where you can define your owen network</li> <li>You can create up to 5 VPC per region</li> <li>Can have 200 subnets per VPC</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#subnet","title":"Subnet","text":"<ul> <li>Is a virtual firewall</li> <li>ex: eu-west has 3 AZ =&gt; 3 subnets</li> <li>By default, is private</li> <li>A subnet is always assigned to one AZ</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#internet-gateway","title":"Internet Gateway","text":"<ul> <li>A virtual router to connect VPC to internet</li> <li>VPC can only have one Internet Gateway</li> <li>It's responsible for a Static Network Address Translation (translate private ip to public ip)</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#route-table","title":"Route Table","text":"<ul> <li>Used to determine where network traffic is directed</li> <li>One route table per VPC</li> <li>Controls what's VPC router does with traffic leaving subnet</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#security-group","title":"Security group","text":"<ul> <li>We can add any allowing rules (open ports)</li> <li>It's linked to instances</li> <li>They are stateful</li> <li>Default SG can't be deleted</li> <li>You can assign up to five security groups to the instance</li> <li>Can allow traffic from:</li> <li>Range or individual IP address</li> <li>Another security group</li> <li>Rules are permissive</li> <li>Can have</li> <li>Up to 10000 SG per region (default 2500)</li> <li>60 inbound and outbound rules per SG</li> <li>16 SG per Elastic Network Interface</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#network-access-control-lists-nacls","title":"Network Access Control Lists (NACLs)","text":"<ul> <li>Is an optional layer of security for VPC that acts as a firewall controlling traffic in and out of one or more subnets</li> <li>It is used to block specific IP address or range of IP address</li> <li>Can allow and deny rules</li> <li>It can have multiple subnets but a subnet is only associated to only one NACLs</li> <li>They are stateless</li> <li>Default rule cannot be updated</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#nat-gateway","title":"NAT Gateway","text":"<ul> <li>Access internet from private subnet</li> <li>One sense private subnet ==&gt; internet</li> <li>Redundant inside AZ</li> <li>Start at 5 Gbps and scales currently to 45 Gbps (Bandwidth)</li> <li>No need to patch</li> <li>No associated to Security Group</li> <li>Automatically assigned a public IP address</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#vpc-endpoints","title":"VPC Endpoints","text":"<ul> <li>Access AWS services from private subnet</li> <li>Two types</li> <li>Interface endpoints</li> <li>Gateways endpoints (Support connection to S3 and DynamoDB)</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#vpc-flow-logs","title":"VPC Flow logs","text":"<ul> <li>Allow to capture IP traffic information in-and out of Network Interfaces within a VPC</li> <li>Can be created for:</li> <li>VPC</li> <li>Subnets</li> <li>Network Interface</li> <li>Track the logs</li> <li>Cannot be tagged as other resources</li> <li>Contains source and destination IP addresses</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#difference-security-group-and-nacls","title":"Difference Security Group and NACLs","text":"<ul> <li>Security groups are assigned to a specific resource, while NACLs are assigned to a subnet</li> <li>Security groups do not allow explicit denies, while NACLs do</li> <li>Security groups are stateful, while NACLs are stateless</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#vpc-peering","title":"VPC Peering","text":"<ul> <li>Link VPCs together</li> <li>Allows you to connect 1 VPC with another via a direct network route using private IP address</li> <li>Instances behave as they were in the same VPC</li> <li>We can peer between region</li> <li>Transitive peering is not supported</li> <li>No overlapping CIDR address ranges</li> <li>Data is encrypted</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#privatelinks","title":"PrivateLinks","text":"<ul> <li>Connect services privately from your service VPC to customers VPC</li> <li>Doesn't need VPC Peering, public internet, NAT Gateway, Route Tables</li> <li>Must be used with Network Load Balancer &amp; ENI</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#direct-connect-dx","title":"Direct Connect (DX)","text":"<ul> <li>Establish a dedicated network connection from on-premise to AWS</li> <li>Two types</li> <li>Dedicated connection ??</li> <li>Hosted connection ??</li> <li>Useful for high throughput workloads</li> <li>Helpful when you need stable and reliable secure connection  </li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#site-to-site-vpn-connection","title":"Site-To-Site (VPN Connection)","text":"<ul> <li>Utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet</li> <li>VPN Connections can be configured in minutes and are a good solution if you have an immediate need have low-to-modest   bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity</li> <li>Customer Gateways: Created on-premise side</li> <li>Virtual Private Gateway: Created on AWS side</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#aws-wavelength","title":"AWS Wavelength","text":"<ul> <li>AWS service embeds AWS compute and storage services within 5G networks, providing mobile edge computing infrastructure   for developing, deploying, and scaling ultra-low-latency applications</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#bastion-jumpbox","title":"Bastion (Jumpbox)","text":"<ul> <li>Get access via SSH to private subnet</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#endpoints","title":"Endpoints","text":""},{"location":"cloud/aws/03-Network/vpc/#interface-endpoints","title":"Interface Endpoints","text":"<ul> <li>They are Elastic Network Interfaces (ENI) with private IP address</li> <li>They serve as an entry point for traffic going to a supported service</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#gateway-endpoints","title":"Gateway Endpoints","text":"<ul> <li>Is a target for a specific route in the route table</li> <li>Supports only DynamoDB and S3</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#cloudhub","title":"CloudHub","text":"<ul> <li>Connect multiple sites</li> <li>Low cost and easy to manage</li> <li>It operates over public network, but all traffic between Customer Gateway and AWS VPN CloudHub is encrypted</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#transit-gateway","title":"Transit Gateway","text":"<ul> <li>Connects VPCs and on-premise networks through a central hub (single gateway)</li> <li>Scales elastically based on the volume of network traffic</li> <li>Routing through a transit gateway operates at layer 3, where the packets are sent to a specific next-hop attachment, based on their destination IP addresses</li> </ul>"},{"location":"cloud/aws/03-Network/vpc/#egress-only-internet-gateway","title":"Egress-only Internet Gateway","text":"<ul> <li>Used for IPv8 only</li> <li>Similar to a NAT Gateway but for IPv6</li> </ul>"},{"location":"cloud/aws/03-Network/vpcEndpoints/","title":"VPC Endpoints","text":"<ul> <li>Allows to connect to AWS Services using private network instead of the public www network</li> <li>They scale horizontally and are redundant</li> <li>They remove the need of IGW, NAT, etc ... to access AWS Services</li> <li>Two types:</li> <li>Gateway:<ul> <li>Provisions a target and must be used in a route table</li> <li>Only S3 and DynamoDB</li> </ul> </li> <li>Interface<ul> <li>Provisions an ENI (Private IP address) as an entry point (must attach security group)</li> <li>Most AWS Services  </li> <li>Also called VPC PrivateLink</li> </ul> </li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/","title":"Relational Database Service (RDS)","text":"<ul> <li>It allows to create and manage relational databases</li> <li>It Supports:</li> <li>SQL Server</li> <li>Mysql</li> <li>Postgres</li> <li>Oracle</li> <li>MariaDB</li> <li>Aurora</li> <li>Why to use RDS:</li> <li>Automated provisioning and OS patching</li> <li>Continuous backups and restore to specific timestamp (Point in Time Restore)</li> <li>Monitoring dashboards</li> <li>Read replicas for improved read performance</li> <li>Multi AZ setup for Disaster Recovery</li> <li>Maintenance windows for upgrades</li> <li>Scaling capability (Vertical and Horizontal)</li> <li>Storage backed by EBS (gp2 or io1)</li> <li>Instance for prod, dev/test and free tier</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#storage-auto-scaling","title":"Storage Auto Scaling","text":"<ul> <li>Helps to increase the storage on the RDS instance</li> <li>When RDS detects you are running out of free database storage, it scales automatically</li> <li>Need to set Maximum Storage Threshold (maximum limit for DB storage)</li> <li>Automatically modify storage if:</li> <li>Free storage is less than 10 %</li> <li>Low-storage lasts at least 5 minutes</li> <li>6 hours have passed since the last modification</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#multi-az","title":"Multi-AZ","text":"<ul> <li>An exact copy of your  database in another AZ</li> <li>Used for disaster recovery</li> <li>In the event of a failure, RDS will automatically fail over to the standby instance</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#read-replicas","title":"Read replicas","text":"<ul> <li>Read-only copy of primary database</li> <li>Great for read-heavy workloads and takes the load off your primary database</li> <li>Can be within AZ, cross-AZ or Cross-region</li> <li>Can be promoted to be their own databases</li> <li>Require automatic backup</li> <li>Used to boost performance</li> <li>Can have up to 5 replicas of a database</li> <li>Each Read Replicas will have its own DNS Endpoint</li> <li>Provides asynchronous replication</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#rds-custom","title":"RDS Custom","text":"<ul> <li>Supported only by Oracle and SQL Server</li> <li>Gives full access to underlining OS</li> <li>Deactivate automation mode when performing task on the OS and before some action take a snapshot</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#encryption","title":"Encryption","text":"<ul> <li>You can turn on encryption at-rest for all RDS engines</li> <li>It'll also encrypt the automated backups, snapshots, and read replicas</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#backups","title":"Backups","text":"<ul> <li>Automated backup</li> <li>Retention period between 1 and 35 days</li> <li>Stores transaction logs throughout the day</li> <li>They are enabled by default</li> <li>All data is stored inside S3</li> <li>There is no additional charge for buckup storage</li> <li>Storage I/O may be suspended during backup</li> <li>Manual Snapshots</li> <li>Taken manually by the user</li> <li>Backups persist even if you delete the original RDS instance</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#rds-aurora-security","title":"RDS &amp; Aurora Security","text":"<ul> <li>At-rest encryption</li> <li>Database master and replicas encryption using AWS KMS must be defined as launch time</li> <li>If the master is not encrypted, the read replicas cannot be encrypted</li> <li>To encrypt an un-encrypted database, go through a DB Snapshot and restore as encrypted</li> <li>In-flight encryption</li> <li>TLS-ready by default, use the AWS TLS root certificate client-side</li> <li>IAM Authentication</li> <li>IAM roles to connect to your database (instead of username/pwd)</li> <li>Security Groups</li> <li>Control Network access to your RDS / Aurora DB</li> <li>No SSH available except on RDS Custom</li> <li>Audit Logs can be enabled and sent to CloudWatch Logs for longer retention</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#scaling","title":"Scaling","text":"<ul> <li>Vertical scaling</li> <li>Scaling storage</li> <li>Read replicas</li> <li>Aurora serverless</li> </ul>"},{"location":"cloud/aws/04-Databases/RDS/#rds-proxy","title":"RDS Proxy","text":"<ul> <li>Middleman between you and your RDS</li> <li>Handles all network traffic including:</li> <li>Establishing, maintaining and terminating connections</li> <li>Reduces DB workload by requiring fewer connections</li> <li>Simplifies application logic</li> <li>Enforce IAM Authentication for DB, and securely store credentials in AWS Secrets Manager</li> <li>Can be used with Postgres, MySQL or Aurora</li> <li>Serverless, autoscaling, highly available (multi-AZ)</li> </ul>"},{"location":"cloud/aws/04-Databases/aurora/","title":"Aurora","text":"<ul> <li>Amazon propriety database</li> <li>MySql and Postgresql compatible database engine</li> <li>5x than Mysql and 3x than Postgresql in performance</li> <li>Can have up to 15 replicas and the replication process is faster than MySQL</li> </ul>"},{"location":"cloud/aws/04-Databases/aurora/#basics","title":"Basics","text":"<ul> <li>Start with 10 GB, scale with 10GB-increments to 128 TB (storage auto-scaling)</li> <li>Compute resources can scale uo to 96 vCPUs and 768 GB of memory</li> <li>2 copies of data are contained in each AZ, with a minimum of 3 AZ. 6 copies of your data</li> <li>Backups only for 35 days</li> </ul>"},{"location":"cloud/aws/04-Databases/aurora/#advanced","title":"Advanced","text":"<ul> <li>Replicas Auto-scaling: add reader replicas</li> <li>Custom Endpoints: define a subset of Aurora instances as a Custom Endpoint and the Reader Endpoint is generally not   used after defining Custom Endpoint</li> <li>Serverless</li> <li>Automated database and auto-scaling on actual usage</li> <li>Good for infrequent intermittent or unpredictable workloads</li> <li>No capacity planning needed</li> <li>Pay per second, can be more cost-effective</li> <li>Aurora Multi-Master</li> <li>In case you can't continuously write availability for the writer nodes</li> <li>Every node does R/W vs promoting as Read Replica as the new master</li> <li>Global Aurora</li> <li>Aurora Cross Region Read Replicas<ul> <li>Useful for disaster recovery</li> <li>Simple to put in place  </li> </ul> </li> <li> <p>Aurora Global Database (Recommended)</p> <ul> <li>1 Primary Region (Read / Write)</li> <li>Up to 5 secondary (read-only) regions, replication lag is less than 1 second</li> <li>Up to 16 Read Replicas per secondary region</li> <li>Promoting another region  (for disaster recovery) has an RTO of &lt; 1 minute</li> <li>Typical cross-region replication takes less than 1 secon</li> </ul> </li> <li> <p>Reader endpoint</p> </li> <li>Amazon Aurora Parallel Query: enables Amazon Aurora to push down and distribute the computational load of a single query across thousands of CPUs in Aurora's storage layer.</li> </ul>"},{"location":"cloud/aws/04-Databases/db/","title":"NoSQL Databases","text":"<ul> <li>AWS Document DB (Mongodb)</li> <li>Amazon keyspaces (for Apache Cassandra)</li> <li>Neptune (Graph db)</li> <li>Amazon QLDB (Quantum Ledger Database)</li> <li>Amazon Timestream (time series database)</li> </ul>"},{"location":"cloud/aws/04-Databases/dynamoDb/","title":"DynamoDB","text":"<ul> <li>Fully managed service</li> <li>Is made of tables (no need to create databases)</li> <li>Each table must have a primary key (decided at creation time)</li> <li>Each table can have an infinite items (rows)</li> <li>Each item have attributes (can be added over time, can be null)</li> <li>Maximum size of item is 400KB</li> <li>Supported data types:</li> <li>Scalar types: String, Number, Binary, Boolean, Null</li> <li>Document Types: List, Map</li> <li>Set Types: String set, Number Set, Binary Set</li> </ul>"},{"location":"cloud/aws/04-Databases/dynamoDb/#primary-keys","title":"Primary Keys","text":"<ul> <li>Option 1: Partition key only (Hash)</li> <li>Partition Key must be unique for each item</li> <li> <p>Partition Key must be diverse so that the data is distributed</p> </li> <li> <p>Option 2: Partition Key + Sort Key</p> </li> <li>The combination must be unique</li> <li>Data is grouped by partition key</li> <li>Sort == range key</li> </ul>"},{"location":"cloud/aws/04-Databases/dynamoDb/#anti-pattern","title":"Anti-Pattern","text":"<ul> <li>Pre-written application to traditional relational database =&gt; use RDS instead</li> <li>Joins or complex transactions</li> <li> <p>Binary Large Object (BLOB) =&gt; Store in S3 &amp; metadata in DynamoDB</p> </li> <li> <p>When you create table in DynamoDB you must provide:</p> </li> <li> <p>Read Capacity Units: throughput for reads</p> <ul> <li>How we compute it ?</li> <li>Once write capacity unit: represents one write per second for an item up to 1KB in size</li> </ul> </li> <li> <p>Write Capacity Units: throughput for writes</p> </li> </ul>"},{"location":"cloud/aws/04-Databases/dynamoDb/#partitions","title":"Partitions","text":"<ul> <li>You start with one partition</li> <li> <p>Each partition:</p> <ul> <li>Max of 3000 RCU / 1000 WCU</li> <li>Max of *10GB</li> </ul> </li> <li> <p>To compute the number of partitions:</p> <ul> <li>By capacity =&gt; <code>(TOTAL RCU / 3000) + (TOTAL WCU / 1000)</code><ul> <li>By size =&gt; <code>(TOTAL table size / 10 BG)</code></li> </ul> </li> <li>Total partitions = <code>CEILING(MAX(Capacity, Size))</code></li> </ul> </li> <li> <p>WCU and RCU are spread evenly between partitions</p> </li> </ul>"},{"location":"cloud/aws/04-Databases/dynamoDb/#auto-scaling","title":"Auto Scaling","text":""},{"location":"cloud/aws/Analysis/athena/","title":"Athena","text":"<ul> <li>Is serverless to query S3 using SQL</li> <li>Uses SQL language to query files (Build on top of Apache Presto)</li> <li>It's schema on read</li> <li>Accepts format:</li> <li>CSV</li> <li>Json</li> <li>Parquet (columnar, splittable)</li> <li>ORC (columnar, splittable)</li> <li>Avro (columnar, splittable)</li> <li>XML</li> <li>Princing: fixe amount per TB of data scanned </li> <li>Structured, semi-structured an unstructured data</li> <li>Uses cases: BI, analytics, reporting, query logs, Ad-hoc queries </li> <li>Athena detect automatically the tables from Glue, and it creates queryable table</li> </ul>"},{"location":"cloud/aws/Analysis/athena/#performance","title":"Performance","text":"<ul> <li>Use columnar data for cost-savings (less scan)</li> <li>Apache Parquet or ORC is recommended</li> <li>Use Glue to convert data to Parquet or ORC </li> <li>Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd, ...)</li> <li>Partition datasets in S3 for easy querying on virtual columns</li> <li>Use larger files ( &gt; 128 MB) to minimize overhead </li> </ul>"},{"location":"cloud/aws/Analysis/athena/#costs","title":"Costs","text":"<ul> <li>Pay-as-you-go </li> <li>$5 per TB scanned<ul> <li>Successful or cancelled queries count, failed queries do not</li> <li>No charge for DDL (Create/ Alter, Drop, ...)</li> </ul> </li> <li>Save money by using columnar formats<ul> <li>ORC, Parquet</li> <li>Save 30-90% and get better performance</li> <li>Good partitioning (Less query CPU)</li> </ul> </li> <li>Glue and S3 have their own charges</li> </ul>"},{"location":"cloud/aws/Analysis/athena/#federated-query","title":"Federated Query","text":"<ul> <li>Query data anywhere (S3 ou on-premise)</li> <li>Use Data Source Connectors thar run on AWS Lambda to run Federated Queries</li> </ul>"},{"location":"cloud/aws/Analysis/athena/#security","title":"Security","text":"<ul> <li>Access control</li> <li>IAM, ACLs, S3 bucket policies</li> <li>AmazonAthenFullAcces/</li> <li>Encrypt .... ???</li> <li>Cross-acount in S3 bucket policy possible</li> <li>Transport Layer Security (TLS) encrypts in-transit (between Athena and S3)</li> </ul>"},{"location":"cloud/aws/Analysis/athena/#anti-pattern","title":"Anti-Pattern","text":"<ul> <li>Highly formatted reports / visualization =&gt; That's what QuickSight is for</li> <li>ETL (not suitable for ETL actions) =&gt; use Gue instead</li> </ul>"},{"location":"cloud/aws/Analysis/data-pipeline/","title":"AWS Data Pipeline","text":"<ul> <li>Managed ETL</li> <li>Define data-driven workflows and automate them</li> <li>Highly available</li> </ul>"},{"location":"cloud/aws/Analysis/data-pipeline/#components","title":"Components","text":"<ul> <li>Pipeline definition</li> <li>Managed component</li> <li>Task runners</li> <li>Data nodes</li> </ul>"},{"location":"cloud/aws/Analysis/emr/","title":"Elastic Map Reduce (EMR)","text":"<ul> <li>Create a Hadoop clusters to process and analyze a vast amount of data</li> <li>The cluster can be made of hundreds of EC2 instances</li> <li>We can autoscale the cluster and it's integrated with EC2 spot instances</li> <li>Nodes:</li> <li>Master node: manage the cluster</li> <li>Core node: run tasks and store data</li> <li>Task node (optional): juste to run task (usually spot node)</li> </ul>"},{"location":"cloud/aws/Analysis/glue/","title":"Glue","text":"<ul> <li>Fully managed ETL</li> <li>Table definition and ETL</li> <li>Glue crawler will extract partitions based on how your S3 data is organized</li> <li>We can use Glue metadata catalog as metastore for hive, inversely we can import hive metastore to Glue</li> <li>Runs on serverless Spark platform</li> <li>Encryption:</li> <li>Server-side (at rest)</li> <li>SSL (in transit)</li> <li>Can be event-driven ??</li> <li>Can provision additional DPU's (data processing units) to increase performance of underlying Spark jobs ??</li> <li>Errors reported to CloudWatch</li> <li>Data Catalog: metadata repository that can serve as drop-in replacement for hive metastore</li> <li>Crawlers: programs that run through data ton infer schemas and partitions</li> <li>Bookmarking ??</li> </ul>"},{"location":"cloud/aws/Analysis/glue/#glue-cost-and-anti-patterns","title":"Glue cost and anti-patterns","text":"<ul> <li>Billed by the minute for crawler and ETL jobs</li> <li>First million objects stored and accesses are free for the Glue Data Catalog</li> <li>Development endpoints for developing ETL code charged by the minute ???</li> <li>Anti-patterns:</li> <li>Streaming data (Glus is batch oriented, minimum 5 minutes interval) =&gt; use for example Kinesis instead</li> <li>Multiple ETL engines (if we'll use multiple engines, hive ...) =&gt; use EMR service instead</li> <li>NoSQL databases (Glue objective is to work with Structured data)</li> </ul>"},{"location":"cloud/aws/Analysis/kinesis/","title":"Kinesis","text":"<ul> <li>Kinesis Streams </li> <li>Real-time for ingesting data </li> <li>You're responsible for creating the consumer and scaling the stream </li> <li>Data Firehose</li> <li>Data transfer tool to get information to S3, Redshift, ELK or splunk </li> <li>Near real-time (60 seconds)</li> <li> <p>Plug and play with AWS architecture</p> </li> <li> <p>Performs real-time analysis on streams using SQL</p> </li> </ul> <p></p> <ul> <li>Common use-cases</li> <li>Streaming ETL </li> <li>Continues metric generation </li> <li>Responsive analytics</li> </ul>"},{"location":"cloud/aws/Analysis/kinesis/#cost","title":"Cost","text":"<ul> <li>Pay only for resources consumed (but it's not cheap)</li> <li>Serverless; scales automatically </li> <li>Use IAM permissions to access streaming source and destination(s)</li> <li>Schema discovery</li> <li>RANDOM_CUT_FOREST</li> <li>SQL function used for anomaly detection on numeric columns in stream </li> <li>They're especially proud of this because they published a paper on it </li> <li>it's a novel way to identify outliers in data set, so you can handle them however you need to </li> <li>Example: detect anomalous subway ridership during the NYC marathon</li> </ul>"},{"location":"cloud/aws/Analysis/mks/","title":"Amazon Managed Service for Apache Kafka (MSK)","text":"<ul> <li>Fully managed service for running data streaming that leverage Kafka </li> <li>Provide control plane applications (create, update, delete clusters as required)</li> <li>Encryption </li> <li>data encrypted at rest by default</li> <li>Uses TLS 1.2 to encrypt data in transit </li> <li>Brokers logs can be delivered to S3, CloudWatch and Kinesis Data Firehose</li> <li>Metrics are collected to CloudWatch </li> <li>All API calls are logged to CloudTrail</li> </ul>"},{"location":"cloud/aws/Analysis/open-search/","title":"Amazon OpenSearch (ELK)","text":"<ul> <li>Search engine</li> <li>Analysis tool</li> <li>Visualization tool (Kibana)</li> <li>A data pipeline (Beat / Logstash)</li> <li>Horizontally scalable</li> </ul>"},{"location":"cloud/aws/Analysis/open-search/#elk-in-aws","title":"ELK in AWS","text":"<ul> <li>Fully manged service (but not serverless)</li> <li>Scale up or down without downtime (But this isn't automatic)</li> <li>Pay for use (instance-hours, storage, data transfer)</li> <li>Network isolation</li> <li>AWS integration</li> <li>S3 buckets (via lambda to Kinesis)</li> <li>Kinesis Data Streams</li> <li>DynamoDB Streams</li> <li>CloudWatch / CloudTrail</li> <li> <p>Zone awareness</p> </li> <li> <p>Cognito ??</p> </li> </ul>"},{"location":"cloud/aws/Analysis/redshift/","title":"Redshift","text":"<ul> <li>Fully managed</li> <li>It's OLAP engine (analytics and data warehousing), build on Postgres</li> <li>Petabyte-scale data warehouse service</li> </ul>"},{"location":"cloud/aws/Analysis/redshift/#architecture","title":"Architecture","text":"<ul> <li>Performance</li> <li>Massively Parallel Processing (MPP)</li> <li>Columnar Data Storage</li> <li>Column Compression (is column level operation, reduce space, reduce disc I/O)</li> <li>Is a single AZ</li> <li>Block size of 1MB</li> <li>Indexes are materialized but not required</li> <li>Data is stored in three different places:</li> <li>Original data within cluster</li> <li>Backup within cluster</li> <li> <p>Backup in S3</p> </li> <li> <p>Durability</p> </li> <li> <p>Distribution Styles</p> </li> <li>AUTO</li> <li>EVEN</li> <li>KEY</li> <li> <p>ALL</p> </li> <li> <p>Sort Keys</p> </li> <li>Single column</li> <li>Compound (default)</li> <li>Interleaved</li> </ul>"},{"location":"cloud/aws/Analysis/redshift/#snapshots-and-dr","title":"Snapshots and DR","text":"<ul> <li>Redshift has Multi-AZ mode for some clusters</li> <li>Snapshots are point-in-time backups of a cluster stored internally in S3</li> <li>Snapshots are incremental (only what has changed is saved)</li> <li>It can be restored into a new cluster</li> <li>Can be</li> <li>Manual: snapshot is retained until you delete it</li> <li>Automatic: e.g every 8 hours, every 5 GB, or on schedule. You can set a retention for a automated snapshot</li> <li>You can configure Redshift to copy snapshot either they are manual or automatic into another region</li> </ul>"},{"location":"cloud/aws/Analysis/redshift/#copy-command","title":"COPY command","text":"<ul> <li>From S3, EMR, DynamoDB, remote hosts to Redshift</li> <li> <p>S3 requires a manifest file and IAM role</p> </li> <li> <p>UNLOAD command: unload from a table into files in S3</p> </li> </ul>"},{"location":"cloud/aws/Analysis/redshift/#redshift-spectrum","title":"Redshift Spectrum","text":"<ul> <li>Query the data is already in S3 without loading it into Redshift</li> </ul>"},{"location":"cloud/aws/AppIntegration/appFlow/","title":"AppFlow","text":"<ul> <li>SaaS data ingestion</li> <li>Fully managed service for easily automating the bidirectional exchange of data to SaaS vendors from AWS services like Amazon S3</li> <li>This helps avoid resource constraints</li> </ul>"},{"location":"cloud/aws/AppIntegration/appSync/","title":"AWS AppSync","text":"<ul> <li>Is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications</li> <li>It provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs</li> </ul>"},{"location":"cloud/aws/AppIntegration/mq/","title":"Amazon MQ","text":"<ul> <li>Message broker service allowing migration of existing application to cloud</li> <li>Supports</li> <li>ActiveMQ</li> <li>RabbitMQ</li> <li>Good for specific protocols: JMS or messaging protocols like AMQP 0-9-1, OpenWire, ...</li> </ul>"},{"location":"cloud/aws/AppIntegration/stepFunction/","title":"AWS Step Functions","text":"<ul> <li>Orchestration service </li> <li>Combine AWS Lambda Function with other AWS service</li> <li>Allows to implement long-running workflows with wait periods and conditional catches</li> </ul>"},{"location":"cloud/aws/Cache/cache/","title":"Caching","text":""},{"location":"cloud/aws/Cache/cache/#elasticcache","title":"ElasticCache","text":"<ul> <li>Managed service of Memcached and Redis</li> <li>Helps reduce load of databases for read intensive workloads</li> <li>Helps make your application stateless</li> <li>Memcached</li> <li>Simple database caching solution</li> <li>Not a database by itself</li> <li>No failover or Multi-AZ support</li> <li>No backups</li> <li>Multi-node for partitioning of data (sharding)</li> <li>Redis</li> <li>Supported as a caching solution</li> <li>Functions as a standalone database</li> <li>Read Replicas to scale reads and have high availability</li> <li>Failover and Multi-AZ support</li> <li>Supports backups</li> <li>Data Durability using AOF persistence</li> <li>Supports Sets and Sorted Sets  </li> </ul>"},{"location":"cloud/aws/Cache/cache/#cache-security","title":"Cache Security","text":"<ul> <li>Supports IAM Authentication for Redis</li> <li>IMA policies on ElasticCache are only used for AWS API-level security</li> <li>Redis AUTH</li> <li>You can set a \"password/token\" when you create a Redis cluster</li> <li>This is an extra level of security for your cache (on top of security group)</li> <li>Support SSL in flight encryption</li> <li>Memcached</li> <li>Support SASL-based authentication (advanced)</li> </ul>"},{"location":"cloud/aws/Cache/cache/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator (DAX)","text":"<ul> <li>Is a fully managed, highly available, in-memory cache for DynamoDB</li> <li>Can reduce DynamoDB response times from milliseconds to microseconds</li> <li>Highly available and lives inside the VPC you specify</li> </ul>"},{"location":"cloud/aws/Cache/cloud-front/","title":"CloudFront","text":"<ul> <li>Fast Content Delivery Network (CDN) service that securely delivers data, videos, applications, and APIs to customers   globally</li> <li>It helps reduce latency and provide higher transfer speeds using AWS edge locations</li> <li>The only option to add HTTPS to a static website being hosted in an S3 bucket </li> <li>Improves users experience </li> <li>216 Point of Presence globally (edge locations)</li> <li>DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall</li> </ul>"},{"location":"cloud/aws/Cache/cloud-front/#origins","title":"Origins","text":"<ul> <li>S3 Buckets </li> <li>For distributing files and caching them at the edge</li> <li>Enhanced security with CloudFront Origin Access Control (OAC)</li> <li>Custom Origin (HTTP)</li> <li>Application Load Balancer </li> <li>EC2 instance </li> <li>S3 website (must first enable the bucket as a static S3 website)</li> <li>Any Http backend</li> </ul>"},{"location":"cloud/aws/Cache/cloud-front/#cloudfront-vs-s3-cross-region-replication","title":"CloudFront vs S3 Cross Region Replication","text":"<ul> <li>CloudFront </li> <li>Global Edge network </li> <li>Files are cached for TTL (maybe a day)</li> <li>Great for static content that must be available everywhere </li> <li>S3 Cross Region Replication </li> <li>Must be setup for each region you want replication to happen </li> <li>Files are updated in near real-time</li> <li>Read only </li> <li>Great for dynamic content that needs to be available at low-latency in few regions</li> </ul>"},{"location":"cloud/aws/Cache/cloud-front/#pricing","title":"Pricing","text":"<ul> <li>The cost of data out per edge location varies</li> <li>Three price classes </li> <li>Price Class All: all regions - best performance </li> <li>Price Class 200: most regions, but excludes the most expensive regions </li> <li>Price Class 100: only the least expensive regions </li> </ul>"},{"location":"cloud/aws/Cache/cloud-front/#cache-invalidation","title":"Cache Invalidation","text":"<ul> <li>In case you update the back-end origin, CloudFront doesn't know about it and will only get the refreshed content after   the TTL has expired </li> <li>However, you can force an entire or partial cache refresh (tus bypassing the TTL) by performing a CloudFront invalidation </li> <li>You can in validate all files () or a special path (/pathname/)</li> </ul>"},{"location":"cloud/aws/Cache/global-accelerator/","title":"Global Accelerator","text":"<ul> <li>Is networking service that sends your users' traffic through AWS's global network infrastructure. It can increase   performance and help deal with IP caching</li> <li>3 top features</li> <li>Masks complex architecture</li> <li>Speeds thing up</li> <li>Weighted pools</li> </ul>"},{"location":"cloud/aws/Collection/sns/","title":"Sns","text":""},{"location":"cloud/aws/Collection/sns/#simple-notification-service-sns","title":"Simple Notification Service (SNS)","text":"<ul> <li>Message publishing and processing service </li> <li>Push based</li> </ul>"},{"location":"cloud/aws/Collection/sqs/","title":"Simple Queue Service (SQS)","text":"<ul> <li>Oldest offering (over 10 years old)</li> <li>Fully managed (Serverless)</li> <li>Scales from 1 message per second to 10,000s per second</li> <li>No limit to how many messages can be in the queue</li> <li>Low latency (&lt; 10 ms on publish and receive)</li> <li>Horizontal scaling in terms of number of consumers</li> <li>Can have duplicate messages (at least once delivery, occasionally)</li> <li>Can have out of order messages (the best effort ordering)</li> <li>Delivery delay: 0 to 15 minutes</li> <li>Limitation of 256KB per message sent</li> <li>Message are encrypted by default in transit, but not at rest</li> <li>Default retention of messages: 1 minutes, to 14 days default 4 days</li> <li>Long Polling</li> <li>When a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue</li> <li>The ReceiveMessageWaitTimeSeconds is the queue attribute that determines whether you are using Short or Long polling.   By default, its value is zero which means it is using Short polling. If it is set to a value greater than zero, then it is Long polling</li> </ul>"},{"location":"cloud/aws/Collection/sqs/#dead-letter-queues","title":"Dead Letter Queues","text":""},{"location":"cloud/aws/Collection/sqs/#standard-and-fifo","title":"Standard and FIFO","text":"<ul> <li>Standard: </li> <li>at-least-one delivery</li> <li>Best effort ordering: occasionally messages may be delivered in order different from which they were sent</li> <li>FIFO </li> <li>Messages are processed exactly one in the exact order that they were sent</li> <li>Support only 300 messages per seconds</li> </ul>"},{"location":"cloud/aws/Collection/kinesis/kinesis/","title":"Kinesis","text":"<ol> <li>Kinesis Streams</li> <li>Kinesis Data Firehose (KDF)</li> <li>Kinesis Data Streams vs Firehose</li> </ol>"},{"location":"cloud/aws/Collection/kinesis/kinesis/#1-kinesis-streams","title":"1. Kinesis Streams","text":"<p>Goal: low latency streaming ingest at scale</p> <ul> <li>Streams are divided in ordered Shards / Partitons</li> <li>Data retention is 24 hours by default, can go up to 7 days</li> <li>Ability to repocess / replay data</li> <li>Multiple applications can consume the same stream</li> <li>Real-time processing with scale of throughput</li> <li>Once data is insereted in Kinesis, is can't be deleted (immutablity)</li> </ul>"},{"location":"cloud/aws/Collection/kinesis/kinesis/#kinesis-streams-shards","title":"Kinesis Streams Shards","text":"<ul> <li>One stream is made of one or multiple shards</li> <li>Billing is per shard provisioned, can have as shards as we want</li> <li>Batching available or per message calls</li> <li>Number of shards can evolve over time (reshard, merge)</li> <li>Record are not to be ordered globally they'll be ordered per shard</li> </ul>"},{"location":"cloud/aws/Collection/kinesis/kinesis/#limits","title":"Limits","text":"<ul> <li>Producer</li> <li>1 MB/s or 1000 messages/s at write PER SHARD</li> <li>ProvisionedThroughputException otherwise</li> <li>Consumer</li> <li>2 MB/s at read PER SHARD accross all consumers</li> <li>5 API calls per second PER SHARD across all consumers</li> <li>Consumer Enhanced Fan-Out</li> <li>2 MB/s at read PER SHARD, PER ENHANCED CONSUMER</li> <li>No API calls need (push model)</li> <li>Data Retention</li> <li>24 hours data retention by default</li> <li>Can be extended to 7 days</li> </ul>"},{"location":"cloud/aws/Collection/kinesis/kinesis/#2-kinesis-data-firehose-kdf","title":"2. Kinesis Data Firehose (KDF)","text":"<ul> <li> <p>Goal: load into S3, Redshift, ELK and Splunk</p> </li> <li> <p>Fully Managed Service, no administration</p> </li> <li>Near Real Time 60 seconds latency mim for non full batches</li> <li>Load data into Readshift / Amazon S3 / ElasticSearch / Splunk</li> <li>Automtic scaling</li> <li>Supports many data formats</li> <li>Data Conversions from JSON to Parquet / ORC (Only for S3)</li> <li>Data Transformationi through AWS Lambda (ex: CSV =&gt; JSON)</li> <li>Support compression when target is S3 (GZIP, ZIP and SNAPPY)</li> <li>Only GZIP is the data is further loaded into Redshift</li> <li>Pay for the amount of data going through Firehose</li> <li>Spark / KCL do not read from KDF</li> <li>Write to KDF<ul> <li>SDK</li> <li>Kinesis Producer Library (KPL)</li> <li>Kinesis Agent</li> <li>Kinesis Data Streams (KDS)</li> <li>CloudWatch Logs &amp; Events</li> <li>IoT rules action</li> </ul> </li> </ul> <p></p> <ul> <li>Firehose Buffer Size<ul> <li>Firehose accumulates records in a buffer</li> <li>The buffer is flushed based on time and size rules</li> <li>Buffer Size (ex: 32MB): if size is reached, it's flushed</li> <li>Buffer Time (ex: 2 minutes): if that time is reached, it's flushed</li> <li>Firehose can automatically increase the buffer size to increase throughput</li> <li>High throughput =&gt; Buffer Size will be hit : usually we flush based on size</li> <li>Low throughput =&gt; Buffer Time be hit : usually we flush on time  </li> </ul> </li> </ul>"},{"location":"cloud/aws/Collection/kinesis/kinesis/#3-kinesis-data-streams-vs-firehose","title":"3. Kinesis Data Streams vs Firehose","text":"<ul> <li>Streams<ul> <li>Going to write custom code (producer / consumer)</li> <li>Real time (~ 200 ms latency for class, ~ 70 ms latency for enhanced fan-out)</li> <li>Must manage scaling (shard spliting / merging)</li> <li>Data storage for 1 to 7 days , replay capability, mutli consumers</li> <li>Use with Lambda to insert data in real-time to ElasticSearch (for example)</li> </ul> </li> <li> <p>Firehose</p> <ul> <li>Fully managed, send to S3, Splunk, Redshift, ElasticSearch</li> <li>Serveless data transformations with Lambda</li> <li>Near real time (lowest buffer time is 1 minute)</li> <li>Autmated Scaling</li> <li>No data storage</li> </ul> </li> <li> <p>Install Kinesis Agent <code>sudo yum install -y aws-kinesis-agent</code></p> </li> </ul>"},{"location":"cloud/aws/Containers/ecs-eks/","title":"Elastic Container Service (ECS)","text":"<ul> <li>A container is a standard unit of software that packages up code and all its dependencies, so the application runs quickly and reliably from one computing environment to another</li> <li>Two mode</li> <li>EC2 Mode: ec2 instances to managed by the customer</li> <li>Fargate Mode: managed by aws</li> <li>Integrate with ELB </li> <li>Role integration </li> <li>Ease to use </li> </ul>"},{"location":"cloud/aws/Containers/ecs-eks/#amazon-ecs-fargate","title":"Amazon ECS Fargate","text":"<ul> <li>Fargate's containers offer the least disruptive changes, while also minimizing the operational overhead of managing   the compute service</li> </ul>"},{"location":"cloud/aws/Containers/ecs-eks/#ecs-service-auto-scaling","title":"ECS Service Auto Scaling","text":"<ul> <li>Automatically increase/decrease the desired number of ECS task </li> <li>Uses AWS Application Auto Scaling</li> <li>ECS Service Average CPU Utilization </li> <li>ECS Service Average Memory Utilization -Scale on RAM </li> <li> <p>ALB Request Count Per Target - metric coming from the ALB </p> </li> <li> <p>Target Tracking: scaled based on target value for a specific CloudWatch metric</p> </li> <li>Step Scaling: scale based on a specified CloudWatch Alarm  </li> <li>Scheduled Scaling: scale based on a specified date/time (predictable changes) </li> </ul>"},{"location":"cloud/aws/Containers/ecs-eks/#eks-distro","title":"EKS Distro","text":"<ul> <li>Amazon EKS is based on the EKS Distro, which allows you to leverage the best practices and established processes    on-premises that Amazon EKS uses in AWS</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/automation/","title":"Automation","text":""},{"location":"cloud/aws/ManagementGoverance/automation/#cloudformation","title":"CloudFormation","text":"<ul> <li>Is declarative programming language</li> <li>It supports either JSON or YAML formatting</li> <li>Hard-coded IDs, especially AMIs can be the reason templates fail to create</li> <li>Parameters</li> <li>Input that are allowed to be passed at runtime</li> <li>Mappings</li> <li>A lookup table (e.g. IAM by region)</li> <li>Conditions</li> <li>Whether resources are created or properties are assigned</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/automation/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<ul> <li>You can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that    runs those applications</li> <li>Reduces management complexity without restricting choice or control</li> <li>You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning,   load balancing, scaling, and application health monitoring</li> <li>Supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby</li> <li>When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one    or more AWS resources, such as Amazon EC2 instances, to run your application</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/automation/#aws-system-manager","title":"AWS System Manager","text":"<ul> <li>Automation documents</li> <li>Parameter Store</li> <li>Session Manager</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/","title":"Governance","text":""},{"location":"cloud/aws/ManagementGoverance/governance/#organizations","title":"Organizations","text":"<ul> <li>Is a free governance tool that allows you to create and manage multiple AWS accounts</li> <li>Service Control Policies (SCPs)</li> <li>Can control your accounts from a single location rather than jumping from account to account</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#resource-access-manager-ram","title":"Resource Access Manager (RAM)","text":"<ul> <li>Share resources with other AWS accounts</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#cross-account-role-access","title":"Cross Account Role Access","text":"<ul> <li>Define an IAM Role for another account to access</li> <li>Define which accounts can access this IAM Role</li> <li>Use AWS STS to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)</li> <li>Temporary credentials can be valid between 15 minutes to 1 hour</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-config","title":"AWS Config","text":"<ul> <li>An inventory management and control tool</li> <li>It allows to show the history of your infrastructure along with creating rules to make sure it conforms to the best   practices you've laid out</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-appconfig","title":"AWS AppConfig","text":"<ul> <li>??</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#directory-service","title":"Directory Service","text":"<ul> <li>Fully managed service of active Directory</li> <li>Two components</li> <li>Microsoft AD</li> <li>Connector AD</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#compute-optimizer","title":"Compute Optimizer","text":"<ul> <li>??</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#trusted-advisor","title":"Trusted Advisor","text":"<ul> <li>Fully managed best-practice auditing</li> <li>Provides realtime guidance to help you provision your resources following AWS best practices</li> <li>Trusted advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs,     and monitor service limits</li> <li>It'll scan 5 different parts of your account and look for places where you could improve your adoption of the   recommended best practices provided by AWS</li> <li>Cost Optimization</li> <li>Performance</li> <li>Security</li> <li>Fault Tolerance</li> <li>Service limits</li> <li>Operational Excellence</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-control-tower","title":"AWS Control Tower","text":"<ul> <li>Way to set up and govern an AWS multi-account environment</li> <li>Automates account creation and security controls via other AWS services</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-licence-manager","title":"AWS Licence Manager","text":"<ul> <li>Simplifies managing software licences with different vendors</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-service-catalog","title":"AWS Service Catalog","text":"<ul> <li>Allow organization to create and manage catalogs of approved IT services</li> <li>Standardize: restrict launching products to a specific list of preapproval solutions</li> <li>Self-service: end users can browse products and deploy approved service on their own</li> <li>Access control: add constraints and grand access to products using AWS IAM</li> <li>Versionning: update products to newer versions and propagate changes automatically</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-proton","title":"AWS Proton","text":"<ul> <li>Is service that create and manage infrastructure and deployment tooling for users as well as serverless and container-based application</li> <li>Automate IaC provisioning and deployments</li> <li>Define standardized infrastructures for user serverless and container-based apps</li> <li>Use templates to define and manage apps stacks that contain all components</li> <li>Automatically provision resources, configure CI/CD, and deploys the code</li> <li>Supports AWS CloudFormation and Terraform IaC providers</li> </ul>"},{"location":"cloud/aws/ManagementGoverance/governance/#aws-well-architected-tool","title":"AWS Well-Architected Tool","text":"<ul> <li>Operational Excellence</li> <li>Reliability</li> <li>Security</li> <li>Performance Efficiency</li> <li>Const Optimization</li> <li>Sustainability</li> </ul>"},{"location":"cloud/aws/Monitoring/cloudWatch/","title":"CloudWatch","text":"<ul> <li>Monitoring and observability platform</li> <li>Metrics </li> <li>Default</li> <li>Custom </li> </ul>"},{"location":"cloud/aws/Monitoring/cloudWatch/#system-metrics","title":"System metrics","text":"<ul> <li>They are metrics that you get  out iof the box. The more managed the service is, the more you get.</li> </ul>"},{"location":"cloud/aws/Monitoring/cloudWatch/#application-metrics","title":"Application metrics","text":"<ul> <li>By installing the CloudWatch agent, you can get information from inside your EC2 instances</li> </ul>"},{"location":"cloud/aws/Monitoring/cloudWatch/#alarms","title":"Alarms","text":""},{"location":"cloud/aws/Monitoring/cloudWatch/#cloudwatch-logs","title":"CloudWatch Logs","text":"<ul> <li>Log Group: a collection of log steam</li> <li>Log Stream: a collection of log events from the same source create a log stream</li> <li>Log Event: the record of what happened. It contains a timestamp and the data</li> <li>Logs can be streamed to </li> <li>S3</li> <li>Kinesis Data Stream</li> <li>Kinesis Firehose</li> <li>AWS Lambda</li> <li>Logs are encrypted by default and custom encryption keys can be applied</li> </ul>"},{"location":"cloud/aws/Monitoring/eventBridge/","title":"EventBridge","text":"<ul> <li>Formerly CloudWatch Events</li> <li>Schedule: cron jobs</li> <li>Event Pattern: Event rules to react to a service doing somthing</li> <li>Trigger Lambda functions, send SQS/SNS messages...</li> <li>Type:</li> <li>Default Event Bus</li> <li>Partner Event Bus</li> <li>Custom Event Bus</li> <li>Schema Registry</li> <li>Can analyze the events in your bus and infer the schema</li> <li>Allows you to generate code for your application, that will know in advance how data is structured in the event bus</li> <li>Schemas can be versioned</li> <li>Resource-based Policy</li> <li>Manage permissions for a specific Event Bus</li> </ul>"},{"location":"cloud/aws/Monitoring/healthDashboard/","title":"Monittoring","text":""},{"location":"cloud/aws/Monitoring/healthDashboard/#aws-health","title":"AWS Health","text":"<ul> <li>Gain visibility of resources and availability of AWS services or accounts</li> <li>Receive notifications and alerts for affected resources and upcoming events</li> <li>Automate actions based on events using EventBridge</li> </ul>"},{"location":"cloud/aws/Monitoring/healthDashboard/#aws-health-dashboard","title":"AWS Health Dashboard","text":"<ul> <li>Provides alerts and remediation guidance when AWS is experiencing events that may impact you</li> <li>While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives    you a personalized view of the performance and availability of the AWS services underlying your AWS resources. </li> <li>The dashboard displays relevant and timely information to help you manage events in progress,    and provides proactive notification to help you plan for scheduled activities.</li> <li>With Personal Health Dashboard, alerts are triggered by changes in the health of AWS resources, giving you event   visibility and guidance to help quickly diagnose and resolve issues.</li> </ul>"},{"location":"cloud/aws/Monitoring/monitoring/","title":"Monitoring","text":""},{"location":"cloud/aws/Monitoring/monitoring/#amazon-managed-service-for-prometheus","title":"Amazon Managed Service for Prometheus","text":"<ul> <li>Prometheus offers open-source monitoring</li> <li>Is a serverless, Prometheus-compatible monitoring service for container metrics</li> <li>It is perfect for monitoring Kubernetes clusters at scale</li> </ul>"},{"location":"cloud/aws/Monitoring/monitoring/#amazon-managed-grafana","title":"Amazon Managed Grafana","text":"<ul> <li>Grafana is a well-known open-source analytics and monitoring application</li> <li>Amazon Managed Grafana offers a fully managed service for infrastructure for data visualizations</li> <li>You can leverage this service to query, correlate, and visualize operational metrics from multiple sources</li> </ul>"},{"location":"cloud/aws/Processing/Lambda/lambda/","title":"AWS Lambda","text":"<ul> <li>Don't need to worry about which service to run or how to manage them</li> <li>Only used to execute background tasks</li> <li>Can give a function memory to use</li> <li>Timeout can be adjusted</li> <li>By default, it has no VPC (no VPC), it's public</li> <li>We can assign it a VPC</li> </ul>"},{"location":"cloud/aws/Processing/Lambda/lambda/#architecture","title":"Architecture","text":"<ul> <li>Event-Driven<ul> <li>System react to event or actions</li> </ul> </li> <li>Serverless<ul> <li>FaaS (Function as a Service)</li> <li>No need to manage infrastructure</li> </ul> </li> </ul>"},{"location":"cloud/aws/Visualization/quickSight/","title":"quickSight","text":""},{"location":"cloud/aws/Visualization/quickSight/#quicksight","title":"QuickSight","text":"<ul> <li>Data visualization service</li> <li>Data sources</li> <li>Redshift</li> <li>Aurora / RDS</li> <li>Athena</li> <li>EC2-hosted databases</li> <li>Files (S3 or on-premises)<ul> <li>Excel</li> <li>CSV, TSV</li> <li>Common or extended log format</li> </ul> </li> <li> <p>Data preparation allows limited ETL (ex: eliminate some column in excel file)</p> </li> <li> <p>SPICE</p> </li> <li>Data sets are imported into SPICE<ul> <li>Super-fast, Parallel, In-memory Calculation Engine</li> <li>Uses columnar storage, in-memory, machine code generation</li> <li>Accelerates interactive queries on large datasets</li> </ul> </li> <li>Each user gets 10GB of SPICE</li> <li>Highly available / durable</li> <li>Scales to hundreds of thousands of users</li> </ul>"},{"location":"cloud/azure/cloud-vs-onprime/","title":"Cloud vs onprime","text":""},{"location":"cloud/azure/cloud-vs-onprime/#cloud-adavantages","title":"Cloud adavantages:","text":"<ul> <li> <p>Cost-effective: you pay for what you consume. Pay-as-you-go (PAYG)</p> </li> <li> <p>Global: we can choose regions in every part in world</p> </li> <li> <p>Secure: cloud provider put a lot of effort to secure services</p> </li> <li> <p>Reliable</p> </li> <li>data backups</li> <li>disaster recovery</li> <li>data replication</li> <li> <p>fault tolerance</p> </li> <li> <p>Scalable: increase and decrease resources and services based on demand</p> </li> <li>Elastic: automate scaling during the spikes and drop in demand</li> </ul>"},{"location":"cloud/azure/cloud-vs-onprime/#cloud-drawbacks","title":"Cloud drawbacks","text":"<ul> <li>Less customisable: could provider sometimes doesn't provide customised support and some issue may take a long time to be fixed</li> <li>Long terme costs: using some cloud services for a long terme may become expensive and also sometimes to get most of some services  it requires to pay for additional features</li> <li>Data ownership and transparency: less clarity about owenership and how the data is stored </li> </ul>"},{"location":"cloud/azure/cloud-vs-onprime/#on-premise-brawbacks","title":"On-premise brawbacks","text":"<ul> <li>Maintenace</li> <li>Less scalability: often require to buy the new server to be able to add computing resources</li> <li>Less availability: </li> <li>Support: should support</li> </ul>"},{"location":"cloud/azure/concept/","title":"Azure basics","text":""},{"location":"cloud/azure/concept/#benefits","title":"Benefits","text":"<ul> <li>Cost-effective: you pay for what you consume. Pay-as-you-go (PAYG)</li> <li>Global: we can choose regions in every part in world</li> <li>Secure: cloud provider put a lot of effort to secure services</li> <li>Reliable</li> <li>data backups</li> <li>disaster recovery</li> <li>data replication</li> <li>fault tolerance</li> <li>Scalable: increase and decrease resources and services based on demand</li> <li>Elastic: automate scaling during the spikes and drop in demand</li> </ul>"},{"location":"cloud/azure/concept/#cloud-services-type","title":"Cloud services type","text":"<ul> <li>SaaS (Software as a Service) --&gt; For customers</li> <li>PaaS (Platform as a Service) --&gt; For developers</li> <li>IaaS (Infrastructure as a Service) --&gt; For Admins</li> </ul>"},{"location":"cloud/azure/concept/#cloud-types","title":"Cloud types","text":"<ul> <li>Public Cloud: everything is built on the cloud provider (Known also as Cloud-Native)</li> <li>Private Cloud: everything is built on the company's datacenter (Known also as On-premise)</li> <li>Hybrid Cloud:  using both On-premise, and a Cloud Service Provider</li> </ul>"},{"location":"cloud/azure/concept/#advantages","title":"Advantages","text":"<ul> <li>High Availability</li> <li>High Scalability</li> <li>Scale up (vertical scaling)</li> <li>Scale out (horizontal scaling)</li> <li>High Elasticity (scale automatically)</li> <li>High Durability</li> <li>Be fast to restore in case of disaster</li> </ul>"},{"location":"cloud/azure/concept/#global-infrastructure","title":"Global infrastructure","text":"<ul> <li>A region a grouping of multiple datacenters (Availability Zones)</li> <li>58 regions available across 140 countries</li> <li>A geography is discreet market of two or more regions that preserves data residency and compliance boundries</li> <li>United States</li> <li>Azure Government(US)</li> <li>Canada</li> <li>Brazil</li> <li>Mexico</li> <li>Paired region each region is paired with another region 300 miles (ca. 483 km)</li> <li>Two types of regions</li> <li>Recommended region<ul> <li>Supports Availability Zones</li> </ul> </li> <li>Alternate (other) region<ul> <li>Do not support Availability Zones</li> </ul> </li> <li>General availability (GA) is when a service is considered ready to be used publicly by everyone </li> <li>Service available</li> <li>Foundational: when GA,<ul> <li>Immediately or in 12 months in Recommended an Alternate Regions</li> </ul> </li> <li>Mainstream: When GA,<ul> <li>Immediately or in 12 months in Recommended Regions</li> <li>May become available in Alternate Regions based on customer demand</li> </ul> </li> <li>Specialized: available in Recommended or Alternate Region based on customer demand</li> <li>Availability Zone (AZ) is physical location made up of one or more datacenter</li> <li>A region will generally contain 3 availability Zones</li> <li>Datacenters within a region will be isolated from each other (so different buildings). But they will be close enough     provide low-latency</li> <li>AZ is made of<ul> <li>Fault domain</li> <li>Update domain</li> </ul> </li> <li>Sovereign region: region dedicated to certain sovereign</li> </ul>"},{"location":"cloud/azure/concept/#services","title":"Services","text":"<ul> <li>Compute services</li> <li>Azure VM</li> <li>Container instances..</li> <li>Service Fabric</li> <li>Functions</li> <li> <p>Batch</p> </li> <li> <p>Storage services</p> </li> <li> <p>Azure Data Lake storage: to store structured and unstructured data (used when we are working with big data)</p> </li> <li> <p>Database services</p> </li> <li> <p>Azure Cosmos DB</p> <ul> <li>A fully managed NoSQL databases</li> <li>Designed for scale with a guarantee of 99,999% availability</li> </ul> </li> <li> <p>Azure SQL Database</p> <ul> <li>Fully managed MS SQL database with auto-scale</li> <li>Integral intelligence and robust security</li> </ul> </li> <li>Azure Database for PostgreSQL/MYSQL/MariaDB</li> <li>Azure Synapse Analytics (Azure SQL Data Warehouse)</li> <li>Cache for Redis</li> <li>Application integration</li> <li>Services that help applications to talk to each other</li> <li>Mobile</li> <li> <p>...</p> </li> <li> <p>Azure Resource Manage (ARM) for Infrastructure as Code</p> </li> <li> <p>Use Azure Quick Start Template to start up rapidly</p> </li> <li> <p>Azure Virtual Network (vNet) and Subnets</p> </li> <li> <p>Big Data and Analytics Services</p> </li> <li>Synapse Analytics</li> <li>HDInsight</li> <li> <p>Azure Databricks</p> </li> <li> <p>Key Vault: helps you safeguard cryptographic keys and other secrets used by cloud apps and services</p> </li> <li> <p>Azure Marketplace: apps and services are made available by a third-party publishers to quickly get started</p> </li> <li> <p>Support plans</p> </li> <li>Basic</li> <li>Developer</li> <li>Standard</li> <li>Professional Direct</li> </ul>"},{"location":"cloud/azure/concept/#hierarchy","title":"Hierarchy","text":"<pre><code>---&gt; Management group\n  | \n  |---&gt; Subscription\n      |\n      |--&gt; Resource Group\n         |\n         |--&gt; Resource\n</code></pre>"},{"location":"cloud/azure/concept/#resources","title":"Resources","text":"<ul> <li>Objects used to manage services in Azure</li> <li>Represent service lifecycle</li> <li>All resources are represented as Json template</li> </ul>"},{"location":"cloud/azure/concept/#resource-group","title":"Resource group","text":"<ul> <li>Logic container for resources</li> <li>Grouping of resources</li> <li>Some strategies to group resources:</li> <li>Type (sql, web, ...)</li> <li>Lifecycle (app, environment)</li> <li>Departament</li> <li>Billing, location or combination of those</li> <li>Resource can only be assigned to one resource group</li> <li>They cannot be nested</li> <li>IAM is used to manage access  </li> <li>Azure Resource Explorer to view resource details</li> </ul>"},{"location":"cloud/azure/concept/#subscription","title":"Subscription","text":"<ul> <li>Unit of management</li> <li>A way to logically manage resources group and facilate billing</li> <li>Two types of subscriptions can be used</li> <li>Billing boundary</li> <li>Access control boundar</li> </ul>"},{"location":"cloud/azure/concept/#management-group","title":"Management group","text":"<ul> <li>Mange a group of subscriptions</li> <li>Management group can be nested</li> </ul>"},{"location":"cloud/azure/concept/#resource-manager","title":"Resource Manager","text":"<ul> <li>Resources can be managed using one of these mean</li> <li>Portal</li> <li>Rest</li> <li>Powershell</li> <li>CLI</li> <li>SDKs</li> <li>All the tools uses Azure Resource Manager</li> <li> <p>RM controls access and resources</p> </li> <li> <p>tenantID: A tenant is a specific organization that owns and manages a instance of Azure</p> </li> </ul>"},{"location":"cloud/azure/concept/#cost-managment","title":"Cost managment","text":"<ul> <li>Cost analysis: explore and analyze organization cost</li> <li>Cost alerts: raise an alerts</li> <li>Budget alerts</li> <li>Credit alerts</li> <li>Departement spending quota alerts</li> <li> <p>Budget: a way to set budget by subscription, resource group, resouce, ...</p> </li> <li> <p>Pricing calculator Vs Total Cost of Ownership (TCO) calculator</p> </li> <li>Tags ?</li> </ul>"},{"location":"cloud/azure/compute/concepts/","title":"Concepts","text":"<ul> <li> <p>Visualization</p> <ul> <li>Emulation of physical machines</li> <li>Different virtual hardware configuration per machine/app</li> <li>Different operating systems per machine/app</li> <li>Total separation of environments<ul> <li>file systems</li> <li>services</li> <li>ports</li> <li>middleware</li> <li>configuration</li> </ul> </li> </ul> </li> <li> <p>Azure VM</p> <ul> <li>IaaS</li> </ul> </li> <li> <p>Azure Container Instances</p> <ul> <li>Simplest and fastest wat to run a container in Azure</li> <li>PaaS</li> <li>Serverless containers</li> </ul> </li> <li> <p>Azure Kubernetes Services (AKS)</p> <ul> <li>Open-source container orchestration platform</li> <li>PaaS</li> <li>Highly scalable and customizable</li> <li>Designed for high scale container deployments</li> </ul> </li> <li> <p>App Service</p> <ul> <li>Designed as enterprise grade web application service</li> <li>PaaS</li> <li>Supports multiple programming languages and containers</li> <li>Guaranteed 99,95% availability</li> <li>Can be<ul> <li>Scaled up</li> <li>Scaled out</li> </ul> </li> </ul> </li> <li> <p>Azure functions</p> <ul> <li>PaaS</li> <li>Serverless</li> <li>Two hosting/pricing models<ul> <li>Consumption-based plan</li> <li>Dedicated plan</li> </ul> </li> <li>Designed for micro/nano-services</li> </ul> </li> </ul>"},{"location":"cloud/azure/deployment/arm/","title":"Azure Resource Manager","text":""},{"location":"cloud/azure/deployment/arm/#concepts","title":"Concepts","text":"<ul> <li>Offers multiple deployment interfaces</li> <li>Centralized layer</li> <li>Secured with Azure AD</li> </ul>"},{"location":"cloud/azure/deployment/arm/#format","title":"Format","text":"<ul> <li>Parameters</li> <li>Variables</li> <li>Resources</li> <li>Outputs: returns output from template execution</li> <li>Functions</li> </ul>"},{"location":"cloud/azure/deployment/arm/#tool-to-use","title":"Tool to use","text":"<ul> <li>Template deployment</li> <li>VSCode<ul> <li>Azure Resource Manager Tools</li> <li>ARM Template Viewer</li> </ul> </li> </ul>"},{"location":"cloud/azure/deployment/arm/#deployement","title":"Deployement","text":"<ul> <li> <p>Powershell</p> <ul> <li>Connect to account <code>Connect-AzAccount</code></li> <li>Run the scrpit <code>ps-deploy.ps1</code></li> </ul> </li> <li> <p>Cloud Shell</p> </li> <li>Template deployment</li> </ul>"},{"location":"cloud/azure/deployment/arm/#parametrization","title":"Parametrization","text":"<ul> <li>Expression: dymamically executed code</li> <li>Parameters:<ul> <li>Provide input values</li> <li>Can be provided within the template file or in separate parameter file</li> </ul> </li> <li>Variables: calculated values, centralization of all calculated values</li> </ul>"},{"location":"cloud/azure/deployment/arm/#copy","title":"Copy","text":"<ul> <li>Duplicate resource multiple time using one template</li> </ul>"},{"location":"cloud/azure/deployment/arm/#links","title":"Links","text":"<ul> <li>Azure git</li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/","title":"Fundamental","text":""},{"location":"cloud/azure/exams/az-900/fundamental/#shared-responsibility","title":"Shared responsibility","text":"<ul> <li>Responsibility is shared between the cloud provider and consumer</li> <li>IaaS</li> <li>PaaS</li> <li> <p>SaaS</p> </li> <li> <p>Consumer is always responsible for </p> </li> <li>Devise used to connect to the cloud </li> <li>Data and information sotred </li> <li>The accounts and identities of people, services and devices used by the organization</li> <li>Provider is always responsible for</li> <li>Physical hosts </li> <li>Physical network </li> <li>Physical datacenters</li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#cloud-models","title":"Cloud models","text":"<ul> <li>private</li> <li>public </li> <li>hybrid </li> <li> <p>multi-cloud</p> </li> <li> <p>Whats is Azure Arc and Azure VMWare ?</p> </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#cost","title":"Cost","text":"<ul> <li>Capital expenditure (CapEx)</li> <li>Operational expenditure (OpEx)</li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#tools-to-interact-with-azure","title":"Tools to interact with azure","text":"<ul> <li>Azure portal </li> <li>CLoud shell</li> <li>Supports PowerShell and Bash </li> <li>Azure CLI</li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#manage-service-in-azure-and-on-premise","title":"Manage service in Azure and on-premise","text":"<ul> <li>Azure Arc </li> <li>simplifies governance and management by delivering a consistent multi-cloud and on-premises management platform</li> <li>It can manage<ul> <li>servers </li> <li>k8s cluster </li> <li>SQL servers </li> <li>VM</li> </ul> </li> <li>Azure data services </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#provision-resource","title":"Provision resource","text":"<ul> <li>Azure Resource Manager</li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#monitor","title":"Monitor","text":"<ul> <li>Azure Advisor</li> <li>Evaluate resource and makes recommendations to help to improve the usage of the resource</li> <li>Azure Service Health</li> <li>helps you keep track of Azure resource, both your specifically deployed resources and the overall status of Azure.</li> <li>Combines three services:<ul> <li>Azure status</li> <li>Service Health</li> <li>Resource Health </li> </ul> </li> <li>Azure Monitor</li> <li>Collect, analyze, visualize and acts on data </li> <li>Can monitor resource on Azure, on-premise and multi-cloud</li> <li>Azure Log Analytics</li> <li> <p>Azure Monitor Alerts</p> </li> <li> <p>Application Insights: </p> </li> <li>monitors your web applications</li> <li>Can monitor applications running on Azure, on-premise and multi-cloud </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#azure-blueprints","title":"Azure Blueprints","text":"<ul> <li>Standardize cloud subscription or environment deployment </li> <li>Each component know as artifact</li> <li>No additional parameters are required </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#azure-policy","title":"Azure Policy","text":"<ul> <li>How to ensure that resources stay compliant</li> <li>Types</li> <li>individual policies </li> <li>groups policies (initiatives)  </li> <li>Can be set on </li> <li>resource </li> <li>resource group </li> <li>subscription</li> <li>Are inherited </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#azure-locks","title":"Azure locks","text":"<ul> <li>Prevent a resource for been deleted or changed</li> <li>can be applied to resource, group event to subscription </li> <li>locks are inherited </li> </ul>"},{"location":"cloud/azure/exams/az-900/fundamental/#service-trust-portal","title":"Service Trust portal","text":"<ul> <li>Portal that gives access to various tools, content and resources about Microsoft privacy, security and compliance </li> </ul>"},{"location":"cloud/azure/exams/az-900/services/","title":"Services","text":""},{"location":"cloud/azure/exams/az-900/services/#vms","title":"VMs","text":"<ul> <li>IaaS</li> <li>It's created from an image</li> <li>Scale sets: manage a group of identical, load-balanced VMs<ul> <li>Can scale on demand or on defined schedule</li> <li>Set automatically a load balancer to ensure that resources are being used efficiently</li> </ul> </li> <li>Availability sets: ensure VMs stagger updates, power and network connectivity preventing from losing VMs<ul> <li>Update domain</li> <li>Fault domain</li> <li>Groups VMs by common source power and network switch</li> <li>Split Vms across three fault domains</li> </ul> </li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-desktop","title":"Azure Desktop ??","text":""},{"location":"cloud/azure/exams/az-900/services/#azure-container","title":"Azure container","text":"<ul> <li>PaaS </li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-function","title":"Azure function","text":"<ul> <li>event-driven</li> <li>serverless</li> <li>Benefit</li> <li>No infra management</li> <li>Scalability</li> <li>Pay for what you use</li> <li>Stateless (default) or stateful </li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-app-service","title":"Azure App Service","text":"<ul> <li>Support Windows and Linux </li> <li>Automated deployment from Github, Azure DevOps and any Git repository</li> <li>Is HTTP-based service for hosting web apps, Rest full APIs and mobile back ends</li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-virtual-network","title":"Azure Virtual Network","text":"<ul> <li>Enable resource to communicate</li> <li>Supports private and public endpoints </li> <li>Filter traffic</li> <li>Network security groups (inbound and outbound rules) </li> <li>Network virtual appliance: specialized VMs (running firewall, performing wide area network (WAN) optimization)</li> <li>Peering: enables to connect VNs </li> <li>Network between peered network is private</li> <li>Travel in Microsoft backbone network</li> <li>User Defined Routes (UDR)</li> <li>Allows to control routing table between subnets or between VNs </li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-virtual-private-network","title":"Azure Virtual Private Network","text":"<ul> <li>Uses encrypted tunnel </li> <li>Only one VPN gateway can be deployed in each virtual network </li> <li>VPN gateway </li> <li>policy-based </li> <li>route-based </li> <li>both use pre-shaded key as the only method of authentication</li> </ul>"},{"location":"cloud/azure/exams/az-900/services/#azure-expressroute","title":"Azure ExpressRoute","text":"<ul> <li>Extends on-premise network to cloud over private connection </li> <li>ExpressRoute connections don't go over the public internet</li> </ul>"},{"location":"cloud/azure/exams/az-900/storage/","title":"Storage","text":"<ul> <li> <p>The interval between the most recent writes to the primary region and the last write to the secondary region is known  as the recovery point objective (RPO).</p> </li> <li> <p>Storage types</p> </li> <li>LRS</li> <li>ZRS</li> <li>GRS</li> <li>GZRS</li> <li>RA-GRS and RA-GZRS</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/azure-stream-analytics/","title":"Azure Stream Analytics","text":"<ul> <li>It's PaaS</li> <li>To process streaming data and respond to data anomalies in real time</li> <li>For Internet of Things (IoT) monitoring, web logs, remote patient monitoring, and point of sale (POS) systems</li> <li>Stream Analytics handles security at the transport layer between the device and Azure IoT Hub</li> <li>Stream Analytics guarantees exactly once event processing and at-least-once event delivery, so events are never lost</li> <li>In-memory compute, so it offers high performance</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/azure-stream-analytics/#windows","title":"Windows","text":"<p>Five kinds of temporal windowing functions are supported: - Tumbling: window functions segment a data stream into a contiguous series of fixed-size, non-overlapping time segments and operate against them. Events can't belong to more than one tumbling window - Hopping - Sliding - Session - Snapshot</p>"},{"location":"cloud/azure/exams/dp-203-exam/azure-stream-analytics/#supported-inputs","title":"Supported inputs","text":"<ul> <li>Azure Event Hubs</li> <li>Azure IoT Hubs</li> <li>Azure Blob or Data Lake Gen 2 Storage</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/databricks/","title":"Databricks","text":"<ul> <li>Data Science and Engineering</li> <li>Machine Learning</li> <li>SQL (workloads are only available in premium tier workspaces)</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/databricks/#delta-lake","title":"Delta lake","text":"<p>It brings to datalake the features bellow - CRUD  - ACID  - Data versioning and time travel - Support for batch and streaming data - Standard formats and interoperability: uses parquet as format to store data</p>"},{"location":"cloud/azure/exams/dp-203-exam/databricks/#retrieve-version","title":"Retrieve version","text":"<ul> <li>versionAsOf</li> <li>timestampAsOf</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/databricks/#sql-warehouses","title":"SQL Warehouses","text":"<p>The data is stored in files that are abstracted by Delta tables in a hive metastore,  but from the perspective of the user or client application, the SQL Warehouse behaves like a relational database. - Dashboards: can be scheduled the dashboard to refresh is data periodically, and notify subscribers by email that new data is available.</p>"},{"location":"cloud/azure/exams/dp-203-exam/prep/","title":"Prep","text":""},{"location":"cloud/azure/exams/dp-203-exam/prep/#data-forms","title":"Data forms","text":"<ul> <li>test</li> <li>stream</li> <li>audio</li> <li>video</li> <li>metadata</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#data-can-be","title":"Data can be","text":"<ul> <li>Structured (Microsoft SQL Server, Azure SQL Databases, Azure Data Warehouse)</li> <li>Data have a schema</li> <li>Schema defined at design time </li> <li>React slowly to changes</li> <li>Unstructured (Azure Blob Storage, Cosmos DB, HDI) </li> <li>each element can have its own schema at query time (binary, audio and image file)</li> <li>Key value, document, graph and column database</li> <li>Aggregated</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#data-engineer-must-maintain-system-that-are","title":"Data engineer must maintain system that are","text":"<ul> <li>Accurate </li> <li>Highly secured</li> <li>Constantly available</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#on-premise","title":"On-premise","text":"<ul> <li>Infra </li> <li>Licence (per sever or per CAL \"client Access Licence\")</li> <li>Maintenance</li> <li>Scalability</li> <li>Availability</li> <li>Support </li> <li>TCO total cost of ownership (TCO) describes the final cost of owning a given technology.</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#on-cloud","title":"On-Cloud","text":"<ul> <li>LCID (langauge code identifier) </li> <li>lift anf shift </li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#etl-vs-elt","title":"ETL vs ELT","text":""},{"location":"cloud/azure/exams/dp-203-exam/prep/#azure-iot-hub","title":"Azure IoT Hub","text":"<ul> <li>Is the cloud gateway that connects IoT devices. IoT hubs gather data to drive business insights and automation</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#azure-event-hubs","title":"Azure Event Hubs","text":"<ul> <li>Provides big-data streaming services. </li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#azure-storage","title":"Azure Storage","text":"<ul> <li>Types </li> <li>Blob </li> <li>Files</li> <li>Queue </li> <li>Table</li> <li>Provides </li> <li>API </li> <li>SDKs in multiple languages </li> <li>PS and azure cli </li> <li>Support ACL and POSIX </li> <li>All data writen to azure storage is encrypted by using either Microsoft or customer-managed keys</li> <li>Can be accessed via keys or shared access signature</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#cosmos-db","title":"Cosmos DB","text":"<ul> <li>Globally dis</li> <li>Multimodal </li> <li>When to use:</li> <li>Need NoSQL for supported API model </li> <li>Planet disb</li> <li>99,999 resp</li> <li>response in 10 ms if correctly provisioned </li> <li>Stored Procedure, triggers and UDFs can be added</li> <li>Js API can be used</li> <li>Security</li> <li>Support encryption, IP firewall configurations, and access from virtual networks</li> <li>Data is encrypted automatically</li> <li>User authentication is based tokens </li> <li>Azure Active Directory provides role-based security</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#azure-sql-database","title":"Azure SQL Database","text":"<ul> <li>Structured data</li> <li>Support also unstructured like spatial and xml</li> <li>Is PaaS</li> <li>Provides OLTP than can scale on demand</li> <li>When to use<ul> <li>Scale up and scale down needed</li> <li>Take advantage of security and available features</li> <li>Easy to maintain than on-premise server (avoid the risks of capital expenditures)</li> </ul> </li> <li>How to ingest and process<ul> <li>Using SDKs of multiple languages like Java, python,..</li> <li>With T-SQL </li> <li>From Azure Data Factory</li> </ul> </li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#azcopy","title":"AzCopy","text":"<ul> <li>Can copy 1TB</li> <li>Splits files than executed 200 GB</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/prep/#azure-purview","title":"Azure Purview","text":"<ul> <li>source, ingest, prepare, analyze, and consume</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/","title":"Sotrage account","text":""},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#encryption-at-rest","title":"Encryption at rest","text":"<ul> <li>Storage Service Encryption (SSE) with a 256-bit Advanced Encryption Standard (AES) cipher, and is FIPS 140-2 compliant</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#encryption-at-transit","title":"Encryption at transit","text":"<ul> <li>It's done via HTTPS</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#rbac","title":"RBAC","text":"<ul> <li>Can be assigned to security principal and managed identities and data operations </li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#auditing","title":"Auditing","text":"<ul> <li>Can be done via Storage Analytics service.</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#keys","title":"Keys","text":"<ul> <li>Access Keys: gives access to all resources in storage account</li> <li> <p>Shared access signature: used for untrusted clients</p> </li> <li> <p>ACL are applied to the principals in the same tenant and they don't apply to users who use Shared Key or shared  access signature (SAS) token authentication.</p> </li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#microsoft-defender-for-storage","title":"Microsoft Defender for Storage","text":"<p>It provides an extra layer of security intelligence that detects unusual and potentially harmful attempts  to access or exploit storage accounts.</p>"},{"location":"cloud/azure/exams/dp-203-exam/sotrage-account/#query-acceleration","title":"Query acceleration","text":""},{"location":"cloud/azure/exams/dp-203-exam/synapse-analytics/","title":"Azure Synapse Analytics","text":"<ul> <li>Brings together data warehousing and bigdata analytics</li> <li>SQL Pools</li> <li>Uses MMP to query and run petabytes of data</li> <li>Could have dedicated resources or serverless</li> <li>Data Movement Service (DMS) coordinates and transports data between compute nodes as necessary</li> <li>Supports three types of distributed tables: </li> <li>Hash</li> <li>Round-robin</li> <li>Replicated</li> <li>Linked services</li> <li>Create links to external resources</li> <li>PolyBase </li> <li> <p>Security</p> <ul> <li>Supports SQL Server authentication and Azure Active Directory</li> <li>Can set up multifactor authentication</li> <li>Supports security at the level of both columns and rows</li> </ul> </li> <li> <p>Spark pools</p> </li> <li>Can be run using notebook</li> <li> <p>Job definition: using jars </p> </li> <li> <p>Dataflow</p> </li> <li>Pipelines</li> </ul>"},{"location":"cloud/azure/exams/dp-203-exam/synapse-analytics/#synapse-analytics-sql","title":"Synapse Analytics SQL","text":"<p>A distributed SQL query system and it offers two runtime environments - Dedicated SQL pool  - Serverless SQL pool </p>"},{"location":"cloud/azure/exams/dp-203-exam/synapse-analytics/#objects","title":"Objects","text":"<ul> <li>External Data source </li> <li>External files </li> <li>External tables </li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/","title":"Core concepts","text":""},{"location":"cloud/azure/exams/dp-900/core-concpets/#data-format","title":"Data format","text":"<ul> <li>Structured (relational db)</li> <li>Semi-structured (json, csv, ...)</li> <li>Unstructured (images, videos, audios, binary files, ...)</li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/#databases","title":"Databases","text":"<ul> <li>Relational</li> <li>NoSql</li> <li>Key-value</li> <li>Document: a specific form of key-value </li> <li>Column oriented: uses column family to group a set of columns</li> <li>Graph: store entities as nodes and links to define relationship between them  </li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/#transactional-processing","title":"Transactional processing","text":"<ul> <li>Online Transactional Processing (OLTP)</li> <li>Uses databases optimized for read and write</li> <li>Require system with ACID </li> <li>Used to support live application called (LOB: line of business)</li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/#analytic-processing","title":"Analytic processing","text":"<ul> <li>Read only </li> <li>Rely on vast volumes of historical data or system metrics  </li> <li>Data warehouses are an established way to store data in a relational schema that is optimized for read operations \u2013 primarily queries to support reporting and data visualization. The data warehouse schema may require some denormalization of data in an OLTP data source (introducing some duplication to make queries perform faster) (Source Azure)</li> <li>online analytical processing (OLAP)</li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/#data-roles-and-services","title":"Data roles and services","text":""},{"location":"cloud/azure/exams/dp-900/core-concpets/#roles","title":"Roles","text":"<ul> <li>Database administrator</li> <li>Data engineer</li> <li>Data Analyst</li> </ul>"},{"location":"cloud/azure/exams/dp-900/core-concpets/#services","title":"Services","text":"<ul> <li>Azul SQL </li> <li>Azure SQL Database: fully managed database (PaaS)</li> <li>Azure SQL Managed instance: a hosted instance of Sql Server </li> <li>Azure Database VM: a virtual machine with installed Sql server </li> <li>Open source databases </li> <li>Mysql </li> <li>MariaDB: optimize version of Mysql and compatible with Oracle </li> <li>Postgrsql: hybrid relational-object database </li> <li>Azure cosmos DB </li> <li>Alos to store data in:<ul> <li>Document format (json)</li> <li>Graph</li> <li>Key-value pair </li> <li>Column families</li> </ul> </li> <li>Compatible Apis </li> <li>Azure Storage </li> <li>Blob container </li> <li>Files share</li> <li> <p>Table: key-value</p> </li> <li> <p>Data factory</p> </li> <li>Create and schedule data pipeline</li> <li>Used to create ETL</li> <li>Can interact with other storage type</li> <li>Azure Synapse Analytics </li> <li>HInsight </li> <li>Databricks </li> <li>Azure Stream Analytics</li> <li>Azure PureView</li> <li>PowerBI</li> </ul>"},{"location":"cloud/azure/exams/dp-900/relationa-data/","title":"Relationa data in Azure","text":"<ul> <li>Azure SQL VMs </li> <li>IaaS </li> <li>Azure SQL Managed instance: PaaS</li> <li>Azure SQL Database: PaaS designed for the cloud  </li> <li>Azure SQL Edge: SQL engine optimized for IoT scenarios that need to work with streaming time-series data</li> </ul>"},{"location":"cloud/azure/hdinsight/concept/","title":"Concept","text":"<ul> <li>HDInsight cluster's ability to access files in Data Lake Storage Gen2 is controlled through managed identities</li> <li> <p>ACLs aren't inherited, so reapplying ACLs requires updating the ACL on every file and subdirectory</p> </li> <li> <p>Spark Apps run as sets of process coordinated by Spark Context.</p> </li> <li> <p>Apache livy</p> </li> <li>Azure event hubs</li> <li>ODBC driver for connectivity from tools such Power BI.</li> </ul>"},{"location":"cloud/azure/hdinsight/concept/#security","title":"Security","text":"<ul> <li>Security perimeter</li> <li>Authentication<ul> <li>AD Auth</li> <li>Multi-user</li> <li>RBAC ACL</li> </ul> </li> <li>Authorization</li> <li>Ranger</li> <li>Auditing</li> <li>Azure monitor</li> <li>Encryption<ul> <li>At rest: costumer key</li> <li>In transit: TLS, IPSec</li> </ul> </li> </ul>"},{"location":"cloud/azure/network/concepts/","title":"Concepts","text":"<ul> <li> <p>VNP</p> <ul> <li>Designed for isolation, segmentation, communication, filtering, routing between resources</li> <li>Composed of one or multiple subnets</li> <li>Can reside only in one region</li> <li>To connect multiple VNP we use VNET peering or VNP Gateway</li> </ul> </li> <li> <p>VPN Gateway</p> <ul> <li>Connect to on-premise</li> <li>Connect to other VPN (rarely used)</li> </ul> </li> <li> <p>Load Balancer</p> <ul> <li>Even traffic distribution</li> <li>Supports both inbound and outbound scenarios</li> <li>Can be private (internal) or public</li> <li>Used for high availability and scalability scenarios</li> </ul> </li> <li> <p>Application Gateway</p> <ul> <li>Web traffic load balancer</li> <li>Web app firewall</li> <li>Redirection</li> <li>Session affinity</li> <li>URL routing</li> <li>SSL termination</li> </ul> </li> <li> <p>Content Delivery Network</p> <ul> <li>Deliver web content to users</li> <li>Minimize latency</li> <li>POP (points of presence) locations    </li> </ul> </li> </ul>"},{"location":"cloud/azure/security/AD/","title":"Azure Active Directory","text":"<ul> <li>Identity: think that get authenticated</li> <li>User with username and pwd</li> <li> <p>Application or service using secret key or certificates</p> </li> <li> <p>Azure AD: is an identity provider</p> </li> <li>Centralize user management</li> <li>Enable high security</li> <li> <p>Additional feature like MFA, Access List Control (ACL),...</p> </li> <li> <p>Managed identities</p> </li> <li>System-assigned<ul> <li>Its lifecycle depends on Azure resource lifecycle (automatic management)</li> <li>Cannot be shared with multiple resources</li> </ul> </li> <li>User-assigned<ul> <li>Its lifecycle is independent of any Azure resource lifecycle</li> <li>Can be shared with other resources</li> </ul> </li> </ul> <p></p>"},{"location":"cloud/azure/security/concepts/","title":"Concepts","text":"<ul> <li> <p>Azure Identity Service</p> <ul> <li>Identity</li> <li>User identity</li> <li>Service principal (Hosted services, Application, Automated tool)</li> <li>Managed identity</li> </ul> </li> <li> <p>Security principal </p> </li> <li> <p>An object (identity) that can be assigned to a role</p> </li> <li> <p>Azure AD</p> <ul> <li>Manages identity and access to resources</li> <li>To manage AD the Global administrator role is required</li> <li>Used by multiple Microsoft cloud platforms<ul> <li>Azure</li> <li>Microsoft 365</li> <li>Office 365</li> <li>...        </li> </ul> </li> </ul> </li> <li> <p>Syncs with on-premises AD via sync service</p> </li> <li> <p>Network Security Group</p> <ul> <li>Filter traffic to inbound and from outbound Azure resources located in VNET</li> <li>Filtering is controlled by roles </li> <li>Ability to have multiple inbound and outbound rules</li> </ul> </li> <li> <p>Application Security Group</p> <ul> <li>Allows grouping of virtual machines located in VNET</li> <li>Designed to reduce the maintenance effort</li> </ul> </li> <li> <p>Multi-Factor Authentication (MFA)</p> <ul> <li>Use more than one factor (evidence) to prove identity</li> </ul> </li> <li> <p>Security Center</p> <ul> <li>Scan all azure resource security</li> <li>Provides solutions and recommendations about security vulnerabilities</li> <li>Natively embedded in Azure services</li> <li>Integrated with Azure Advisor</li> <li>Tiers<ul> <li>Free (Azure Defender OFF)</li> <li>Paid (Azure Defender ON)</li> </ul> </li> </ul> </li> <li> <p>Azure Key Vault</p> <ul> <li>Managed service for storing sensitive information</li> <li>Store keys, secrets and certificates</li> <li>Access monitoring and logging</li> </ul> </li> <li> <p>RBAC</p> </li> <li>Role</li> <li>Security principal<ul> <li>User</li> <li>Group</li> <li>Service principal</li> <li>Managed identity</li> </ul> </li> <li>Scope: one or more azure resources that the access applies to</li> <li>Role Assignement: combinaison of Role definition + Security principal + Scope </li> <li> <p>Resource Locks</p> <ul> <li>Designed to prevent accidental deletion and/or modification</li> <li>Two types<ul> <li>Read-only: read actions are allowed</li> <li>Delete: all actions are allowed except delete</li> </ul> </li> <li>Scopes are hierarchical (inherited)</li> <li>Management Groups can't be locked</li> <li>Only Owner and User Access Administrator roles can manage locks</li> </ul> </li> <li> <p>User Defined Routes</p> <ul> <li>Create custom (user-defined, static) routes</li> <li>Designed to override Azure default routing or add new routes</li> <li>Managed via Azure Route Table resource</li> <li>Associated with or more virtual network subnets</li> </ul> </li> <li> <p>DDoS protection</p> <ul> <li>DDoS: multiple service are attacking</li> <li>Designed to<ul> <li>Detect malicious traffic and block it while allowing legitimate users to connect</li> <li>Prevent additional costs for auto-scaling environments</li> </ul> </li> <li>Two tiers<ul> <li>Basic: automatically enabled for Azure platform</li> <li>Standard: additional mitigation &amp; monitoring capabilities for Azure virtual network resources</li> </ul> </li> </ul> </li> <li> <p>Azure firewall</p> <ul> <li>Allow monitor and control incoming and outgoing traffic using rules</li> <li>Managed service (PaaS)</li> <li>Build-in high availability</li> <li>Highly scalable</li> <li>Support FQDN</li> </ul> </li> <li> <p>Sharded Access Signature</p> </li> </ul>"},{"location":"cloud/azure/storage/choose-storage/","title":"Choose storage","text":"<ul> <li>How to classify data </li> <li>How data will be used</li> <li>How to get the best performance for data</li> </ul>"},{"location":"cloud/azure/storage/databases/","title":"Databases","text":"<ul> <li>Azure cosmos</li> <li>Fully managed and serverless service  </li> <li>Globally distributed</li> <li>Used to store semi-structured data (schema less) </li> <li>Ability to replicate geographically</li> <li>Designed for: <ul> <li>Highly responsive application (real time)</li> <li>Multi-region</li> </ul> </li> <li>Consistency ?? </li> </ul>"},{"location":"cloud/azure/storage/databases/#rdms","title":"RDMS","text":"<ul> <li>Azure SQL</li> <li>Store structured data</li> <li>PaaS (or sometimes called Database as Service)</li> <li>SQL query capabilities</li> <li>High performance, reliable, fully managed and secure DB</li> <li>DB for Postgresql</li> <li>DB for Mysql</li> <li>DB for MariaDB</li> </ul>"},{"location":"cloud/azure/storage/datalake/","title":"Data Lake Storage (Gen 2)","text":""},{"location":"cloud/azure/storage/datalake/#data-lake","title":"Data lake","text":"<ul> <li>Hadoop compatible access</li> <li>A superset of POSIX permissions</li> <li>Optimized driver </li> </ul>"},{"location":"cloud/azure/storage/datalake/#blob-storage","title":"Blob Storage","text":"<ul> <li>Low cost</li> <li>Storage tiers (Hot, cold, archive)</li> <li>High availability and disaster recovery</li> </ul>"},{"location":"cloud/azure/storage/datalake/#access-control","title":"Access Control","text":"<ul> <li>Supports </li> <li>Azure RBAC</li> <li>POSIX-like Access Control List (ACL) </li> </ul>"},{"location":"cloud/azure/storage/datalake/#architecture","title":"Architecture","text":""},{"location":"cloud/azure/storage/storage-account/","title":"Azure Storage Account","text":"<ul> <li>It's a big service designed for storage in Azure</li> <li>Highly scalable (up to petabytes of data)</li> <li>Highly durable</li> <li> <p>It contains the following services</p> </li> <li> <p>Azure Blob Storage</p> </li> <li>Used to store any kind type of data (unstructured data)</li> <li> <p>Three storage tiers</p> <ul> <li>Hot: frequently accessed data</li> <li>Cool </li> <li>Infrequently accessed data (lower availability and high durability)</li> <li>Useful to store older versions and backups of applications </li> <li>Archive: rarely accessed data</li> </ul> </li> <li> <p>Azure Queue Storage</p> </li> <li>Store small pieces of data (messages)  </li> <li> <p>Designed for scalable asynchronous processing</p> </li> <li> <p>Azure Table Storage</p> </li> <li> <p>Store semi-structured data</p> </li> <li> <p>Azure File Storage</p> </li> <li>Similar to Blob Storage</li> <li>In case of file we access Shares (in Blob we access containers)</li> <li>Accessed via shared drive protocols</li> <li> <p>Designed to extend on-premise file shares or implement lift-and-shift scenarios</p> </li> <li> <p>Azure Disk Storage</p> </li> <li>Disk emulation in the cloud</li> <li>Persistent storage for virtual machines</li> <li>Different<ul> <li>Sizes</li> <li>Types (SSD, HDD)</li> <li>Performance tiers</li> </ul> </li> <li>Disk can be undamaged or managed</li> </ul>"},{"location":"cloud/azure/storage/storage-account/#replication","title":"Replication","text":"<ul> <li>Locally-redundant storage (LRS)</li> <li>Zone-redundant storage (ZRS)</li> <li>Geo-redundant storage (GRS)</li> <li>Geo-zone-redundant storage (GZRS)</li> </ul>"},{"location":"cloud/azure/streaming/event-hub/concepts/","title":"Concepts","text":"<ul> <li>Event Producers<ul> <li>Separated in 1 to 32 partitions</li> <li>Inside a partition the events are ordered</li> <li>Namespace: collection of event hubs</li> </ul> </li> </ul>"},{"location":"cloud/gcp/","title":"Google Cloud Data Engineer","text":"<p>In this present documentations you'll find a usefull courses to help you to prepare Google Data Engineer Professional Certificate</p>"},{"location":"cloud/gcp/#tools","title":"Tools","text":"<ul> <li>gcloud</li> <li>gsutil (Storage)</li> <li>bq (Bigquery)</li> <li>datalab</li> <li>cbt (Bigtable)</li> </ul>"},{"location":"cloud/gcp/basics/","title":"Basics","text":"<ul> <li> <p>Compute</p> <ul> <li>App Engine</li> <li>Compute Engine</li> <li>Kubernetes Engine</li> <li>Cloud Functions</li> </ul> </li> <li> <p>Network</p> </li> <li>Data Transfert</li> </ul>"},{"location":"cloud/gcp/basics/#compute","title":"Compute","text":""},{"location":"cloud/gcp/basics/#app-engine","title":"App Engine","text":"<ul> <li>Is Paas (Platform as Service)</li> <li>Auto scaling and and Load-Balancing</li> <li>Two environments:</li> <li>Standard</li> <li>Flexible</li> </ul>"},{"location":"cloud/gcp/basics/#compute-engine","title":"Compute Engine","text":"<ul> <li>Is IaaS (Infrastructure as Service)</li> <li>Scalable and high perforamnce VMs</li> <li>Two types instances group:</li> <li>Managed: collection of instances that are identical</li> <li>Unmanaged: collection of instances with different config</li> <li>For each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every additional minute</li> </ul>"},{"location":"cloud/gcp/basics/#cloud-functions","title":"Cloud Functions","text":"<ul> <li>Runs in response to the event</li> <li>Serverless</li> <li>Pay for CPU and RAM</li> <li>Each function get HTTP endpoint</li> </ul>"},{"location":"cloud/gcp/basics/#network","title":"Network","text":"<ul> <li>Composed of locations</li> <li>Locations are devided into Regions</li> <li>Regions are composed of Zones</li> </ul> <p> Source: GCP</p>"},{"location":"cloud/gcp/basics/#vpc","title":"VPC","text":"<p>Is a secure, individual, private cloud-computing model hosted within a public Cloud.</p>"},{"location":"cloud/gcp/basics/#load-balancer","title":"Load Balancer","text":""},{"location":"cloud/gcp/basics/#cloud-cdn","title":"Cloud CDN","text":"<ul> <li>Accelerate delivery from Compute Engine and Cloud Storage</li> <li>Lower latency</li> </ul>"},{"location":"cloud/gcp/basics/#data-transfert","title":"Data Transfert","text":"<ul> <li>Data Transfert Appliance</li> <li>Transfert data from external storage to cloud</li> <li>Data Transfert Appliance</li> <li>Transfert data from cloud to cloud</li> </ul>"},{"location":"cloud/gcp/basics/#serverless","title":"Serverless","text":"<ul> <li>Cloud Functions</li> <li>Cloud Run</li> </ul>"},{"location":"cloud/gcp/basics/#hierarchy","title":"Hierarchy","text":"<ul> <li>Resource</li> <li>Belongs to only one project</li> <li>Project</li> <li>Project ID</li> <li>Project Name</li> <li>Project number</li> <li>Folder</li> <li> <p>Folder's policies are inhireted by the project</p> <p> Source: GCP</p> </li> </ul>"},{"location":"cloud/gcp/dataviz/","title":"Dataviz","text":""},{"location":"cloud/gcp/dataviz/#looker","title":"Looker","text":"<ul> <li>Steps to create pipeline </li> <li>Choose a template </li> <li>Link the dashboard to datasource </li> <li>Explore your dashboard</li> </ul>"},{"location":"cloud/gcp/streaming/","title":"Streaming","text":""},{"location":"cloud/gcp/streaming/#4v","title":"4V","text":"<ul> <li>Variety (data format)</li> <li>Volume (data size)</li> <li>Velocity (data speed)</li> <li>Veracity (data accuracy)</li> </ul>"},{"location":"cloud/gcp/SRE/terraform-for-gcp/","title":"Terraform","text":"<ul> <li>Is an IaC. Focus on what to do instead of how to do.</li> </ul> <p>Allows to:</p> <ul> <li>Like resources by specifing dependencies</li> <li>Standardize the resource by using modules</li> <li>Limit values by the mean of variables</li> </ul>"},{"location":"cloud/gcp/basics/compute/","title":"Compute","text":""},{"location":"cloud/gcp/basics/compute/#compute-engine","title":"Compute engine","text":""},{"location":"cloud/gcp/basics/compute/#kubernetes-engine","title":"Kubernetes Engine","text":"<ul> <li>Managed orchestrated environnement for containerrized app</li> <li>Serverless</li> <li>Benefits:</li> <li>Load Balancing intergrated</li> <li>Automtic upgrade</li> <li>Logging and monitoring is handled by stackdriver</li> <li>...</li> </ul>"},{"location":"cloud/gcp/basics/compute/#app-engine","title":"App Engine","text":"<ul> <li>PaaS</li> <li>Bind code to libraries</li> </ul>"},{"location":"cloud/gcp/basics/compute/#cloud-functions","title":"Cloud Functions","text":"<ul> <li>Is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events without the need to manage a server or a runtime environment</li> <li>Serverless service</li> </ul>"},{"location":"cloud/gcp/basics/compute/#cloud-run","title":"Cloud Run","text":"<ul> <li>Is a managed compute platform that runs stateless containers via web requests or Hub sub events</li> <li>Serverless service</li> </ul>"},{"location":"cloud/gcp/basics/storage/","title":"Storage","text":""},{"location":"cloud/gcp/basics/storage/#cloud-storage","title":"Cloud Storage","text":"<ul> <li>Classes</li> <li>Standard storage: hot data</li> <li>Nearline storage : once per month</li> <li>Coldline storage: once every 90 days</li> <li>Archive storage: once per year</li> </ul>"},{"location":"cloud/gcp/basics/storage/#sql","title":"SQL","text":"<ul> <li>Fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service</li> </ul>"},{"location":"cloud/gcp/basics/storage/#cloud-spanner","title":"Cloud Spanner","text":"<ul> <li>Fully managed relational database service that scales horizontally, is strongly consistent, and speaks SQL</li> </ul>"},{"location":"cloud/gcp/basics/storage/#firestore","title":"Firestore","text":"<ul> <li>Is a flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development</li> <li>Data is stored in documents and then organized into collections</li> </ul>"},{"location":"cloud/gcp/basics/storage/#cloud-bigtable","title":"Cloud Bigtable","text":"<ul> <li>Is NoSQL Big data database service.</li> </ul>"},{"location":"cloud/gcp/basics/storage/#when-to-use","title":"When to use ?","text":"<p> Source: cloudskillsboost</p>"},{"location":"cloud/gcp/bigdata/bigquery/","title":"BigQuery","text":"<ul> <li>Fully managed serverless data warehouse</li> <li>Provide storage and analytics at the same time</li> <li>Deals with</li> <li>Storage:<ul> <li>Data is stored in structured table</li> <li>Replicated, distributed storage</li> </ul> </li> <li>Ingestion</li> <li>Querying</li> <li>Pay-as-you go pricing model</li> <li>Encryption at the rest by default</li> <li>Supports SQL</li> <li>Near real-time query analysis</li> <li>Accept batch and streaming</li> <li>Can be used for storage and analysis</li> <li>Tables are immutable are optimized for reading and appending data and not optimized for updating</li> <li>Replicated and durable</li> <li>Is columnar</li> <li>Built-in machine learning models</li> <li>Supports repeated columns and nested schema</li> <li>Supports view and materialized views</li> <li>Policy Tag used to hide or obfuscate a column</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#queries-types","title":"Queries types","text":"<ul> <li>Native tables</li> <li>Federated queries</li> <li>Bq create tmp table to query the data. It's retained for 24h. so if you run the exact same query again, and if the results would not be different, then BigQuery will simply return a pointer to the cached results  </li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#iam","title":"IAM","text":"<ul> <li>Control can be done by project, dataset, tables, views or columns</li> <li>Has predefined roles ...</li> <li>Sharding dataset</li> <li>View can be used to restrict access to some data</li> <li>Supports primitives roles for Project Level access</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#cache","title":"Cache","text":"<ul> <li>Enabled by default</li> <li>Are per user</li> <li>No charge for queries executed from the cache</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#partition","title":"Partition","text":"<ul> <li>Types</li> <li>Ingestion-time partitioned</li> <li>Partition by specific timestamp/date column</li> <li>Range partition (integer-type colum)</li> <li>Best practice is to ensure that partition column is always invoked in querying</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#clustering","title":"Clustering","text":"<ul> <li>??</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#udf","title":"UDF","text":"<ul> <li>Combine JavaScript with SQL</li> <li>Allow more complex operations like loops, combination, ...</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#princing","title":"Princing","text":"<ul> <li>Charged for: Storage, Querying and Streaming insert</li> <li>No charge for Batch Loading</li> <li>Flat rate ??</li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#import-and-export-data-to-bigquery","title":"Import and export data to BigQuery","text":"<ul> <li>Load</li> <li>Command</li> </ul> <p><code>bq load names.baby_names gs://(YOUR_BUCKET)/names/yob*.txt Name:STRING,Gender:STRING,Number:INTEGER</code></p> <ul> <li>Export</li> <li>BigQuery &lt;=&gt; BigQuery</li> <li>Table's data can only be exported to GCS with one of this format csv, json, avro</li> <li>To get data from Dataproc, a connector is installed by default in Dataproc and the data will be exported to GSP then to bucket will be read from BigQuey</li> <li>Can only export up to 1 GB</li> <li>Command<ul> <li><code>bq extract 'projectid:dataset:table' gc://bucket_name/folder/object_name</code></li> </ul> </li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#monitor-logging","title":"Monitor &amp; Logging","text":"<ul> <li>Monitor allows to visualize:</li> <li>Metrics (alter on metric)</li> <li>Performance</li> <li>Resource capacity/usage (Slots)</li> <li> <p>Logging:   Who is doing what (show actions history)</p> </li> <li> <p>What does colors mean in query details ?</p> </li> </ul>"},{"location":"cloud/gcp/bigdata/bigquery/#bq-tool","title":"bq tool","text":"<ul> <li>Create dataset</li> </ul> <p><code>bq  mk --dataset ds_name</code></p> <ul> <li>Select from table</li> </ul> <p><code>bq query --use_legacy_sql=false 'select * from ds_result.top_qsts'</code></p>"},{"location":"cloud/gcp/bigdata/bigquery/#bigquery-ml","title":"Bigquery ML","text":"<ul> <li>ML model in two steps:</li> <li>Build the model</li> <li>Train the model</li> <li>Manually or automatically control hyperparameters</li> <li>Supports</li> <li>Supervised</li> <li>Unsupervised</li> <li>Steps</li> <li>Extract, transform and load data into Bigquery</li> <li>Select and preprocess features</li> <li>Create the model inside Bigquery</li> <li>Evaluate the performance of evaluated trained model</li> <li>Use the model to make predictions</li> </ul>"},{"location":"cloud/gcp/bigdata/cloud-storage/","title":"Cloud Stroage","text":"<ul> <li>Is the essential storage service for working with data, especially unstructured data in the cloud</li> <li>Data is durable and strongly consistent</li> <li>Globally available</li> <li>Buckets are containers for Objects</li> <li>The metadata are used for purposes such as access control, compression, encryption, and lifecycle management</li> <li>We can set:</li> <li>Rentention period</li> <li>Versionning</li> <li>Life cycle management</li> </ul>"},{"location":"cloud/gcp/bigdata/cloud-storage/#secure-gcs","title":"Secure GCS","text":""},{"location":"cloud/gcp/bigdata/cloud-storage/#access","title":"Access","text":"<ul> <li>IAM</li> <li>It is set at the bucket level and applies uniform access rules to all objects within a bucket</li> <li>Roles<ul> <li>Bucket Role Level</li> <li>Reader</li> <li>Writer</li> <li>Owner</li> <li>Set ACL policy</li> <li>Project Role Level</li> <li>Viewer</li> <li>Editor</li> <li>Owner</li> <li>Custom Roles</li> </ul> </li> <li>ACL</li> <li>Can be applied at the bucket level or on individual objects, so it provides more fine-grained access control</li> <li>Enabled by default</li> </ul>"},{"location":"cloud/gcp/bigdata/cloud-storage/#encyption","title":"Encyption","text":"<ul> <li>Data encrypted at-rest and transit</li> <li>No way de disabled encryption</li> <li>GMEK (Google Managed Encryption Keys)</li> <li>Is done by Google using encryption keys that we manage, Google-managed encryption keys, or GMEK</li> <li> <p>Hapens in two levels</p> <ul> <li>First, the data is encrypted using a data encryption key</li> <li>Then the data encryption key itself is encrypted using a key encryption key, or KEK</li> </ul> </li> <li> <p>CMEK (Customer Managed Encryption Keys)</p> </li> <li>You control the creation and the existence of the KEK in Cloud KMS</li> <li>CSEK (Customer Supplied Encryption Keys)</li> <li>You provid the KEK key</li> </ul>"},{"location":"cloud/gcp/bigdata/cloud-storage/#data-locking","title":"Data locking","text":"<ul> <li>For audit purposes, you can place a hold on an object, and all operations that could change or delete the object are suspended until the hold is released</li> </ul>"},{"location":"cloud/gcp/bigdata/composer/","title":"Composer","text":"<ul> <li>Fully managed service of Apache Airflow</li> <li>Allows to: create, schedule and monitor a data workflow</li> <li>Composer = Airflow + Google Kubernetes Engine (GKE) + Cloud Storage</li> </ul> <pre><code>from airflow import DAG\nfrom airflow.operators import BashOperator\nfrom datetime import datetime, timedelta\n\n# 1 - declare defautl arguments\ndefault_args = {\n    'owner': 'massipssa',\n    'depends_on_past': False,\n    'start_date': datetime(2020, 6, 12),\n    'email': ['kerrache.massipssa@gmail.com'],\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1),\n}\n\n# 2 - define a DAG\ndag = DAG('helloworld_dag', default_args=default_args)\n\n# 3 - define DAG's tasks\ntask_1 = BashOperator(\n    task_id='task_1',\n    bash_command='echo \"Hello World from Task 1\"',\n    dag=dag)\n\ntask_1 = BashOperator(\n    task_id='task_2',\n    bash_command='echo \"Hello World from Task 2\"',\n    dag=dag)\n\n# 4 - set task dependencies\ntask_1 &gt;&gt; task_2\n</code></pre>"},{"location":"cloud/gcp/bigdata/dataflow/","title":"Dataflow","text":"<ul> <li>Fully managed service for executing Apache Beam pipelines within GCP  </li> <li>Allows code optimization</li> <li>Dynamic workload re-balancing (straggler problem)</li> <li>Autoscaling (more or less workers)</li> <li>Is serverless and NoOps</li> <li>Designed to be low maintenance</li> <li>Pipelines are regional based</li> <li>Integrates with other tools like Bigquery, pub/sub using connectors</li> <li>Best practices:</li> <li>Handles errors<ul> <li>Gracefully catch errors using Try-Catch blocks</li> <li>Output errors to PCollection and set the collector for the later analysis</li> <li>Think about it as recycling the bad data</li> </ul> </li> <li>How to update exiting code ?</li> <li>Window</li> <li>Global</li> <li>Fixed</li> <li>Sliding</li> <li>Session</li> </ul>"},{"location":"cloud/gcp/bigdata/dataflow/#iam","title":"IAM","text":"<ul> <li>Three check are performed when a job is submitted to Dataflow</li> <li>User role with IAM<ul> <li>Dataflow Viewer</li> <li>Dataflow Developer</li> <li>Dataflow Admin</li> </ul> </li> <li>Dataflow Service Account<ul> <li>Interact between your project and Dataflow</li> <li>Used for user creation and monitoring</li> <li>Assigned the Dataflow Service Agent role</li> </ul> </li> <li>Controller Service Account<ul> <li>Used by the workers to access resources needed by the pipeline</li> </ul> </li> </ul>"},{"location":"cloud/gcp/bigdata/dataflow/#quota","title":"Quota","text":"<ul> <li>CPU quota</li> <li>Is the total number of virtual CPUs across all of your VM instances in a region or a zone</li> <li>IP address:</li> <li>Quota limits the number of VMs that can be launched with an external IP address for each region in your project</li> <li>Persistent disk</li> </ul>"},{"location":"cloud/gcp/bigdata/dataflow/#security","title":"Security","text":"<ul> <li>Data locality</li> <li>Sharded VPC</li> <li>Private IPs</li> <li>CMEK</li> </ul>"},{"location":"cloud/gcp/bigdata/dataflow/#beam-best-practices","title":"Beam best practices","text":"<ul> <li>Use schema</li> <li>Handling un-processed data</li> <li>Error handling</li> <li>AutoValue code generation</li> <li>Json data handling</li> <li>Utilize DoFn function</li> <li>Pipeline optimizes</li> </ul>"},{"location":"cloud/gcp/bigdata/datafusion/","title":"Cloud data Fusion","text":"<ul> <li>Fully managed</li> <li>Quick building and managing of pipelines</li> </ul>"},{"location":"cloud/gcp/bigdata/datalab/","title":"Datalab (Deprecated)","text":"<ul> <li>Interactive tool for exploring and visualize data</li> <li>Build on Jupyter</li> <li>Support languages:</li> <li>Python</li> <li>SQL</li> <li>Javascript</li> <li>Allows to interact with:</li> <li>BigQuery</li> <li>ML Engine</li> <li>Stackdriver</li> <li>Compute Engine</li> <li>Cloud Storage</li> <li>Create Datalab: <code>datalab create (instance-name)</code></li> <li>Connect to Datalab: <code>datalab connect (instance-name)</code></li> <li>When Datalab is created, the followings are created:</li> <li>VPC</li> <li>Compute Engine</li> <li>Source Repository stores the notebooks  </li> </ul>"},{"location":"cloud/gcp/bigdata/dataprep/","title":"Dataprep","text":"<ul> <li>Intelligent data preparation</li> <li>Serverless</li> <li>Is not Google product (Partner Trifacta product)</li> <li>Supports:</li> <li>CSV, JSON, Avro,  ...</li> <li> <p>Output: CSV, Json, Bigquery and the file can be compressed or uncompressed</p> </li> <li> <p>IAM</p> </li> <li>dataprep.projects.user: allows user to run the Cloud Dataprep</li> <li>dataprep.serviceAgent: gives access to:<ul> <li>Storage (GCS)</li> <li>Datasets (BQ)</li> <li>Workflow (Dataflow)</li> </ul> </li> </ul>"},{"location":"cloud/gcp/bigdata/dataproc/","title":"Dataproc","text":"<ul> <li>Deploy On demand, managed Hadoop, Spark cluster</li> <li>Connectors to Google services are already added (reduce administrator work)</li> <li>Other ecosystem tools can be instated via initialization</li> <li>Initialize action: install tools like Kafka by given the location of the bucket and when cluster starts, it'll install the tool</li> <li>Bucket location: <code>gsutil ls gs://dataproc-initialization-actions</code></li> <li>Create a cluster:</li> </ul> <p><code>gcloud dataproc create cluster [cluster_name] --zone [zone_name]</code></p> <ul> <li>Available connectors:</li> <li>Spark, Hadoop &lt;=&gt; Bigquery, CloudStorage, Bigtable</li> <li>Preemptive nodes (VMs)</li> <li>Excellent low cost worker nodes</li> <li>Dataproc manages entire leave/join process</li> <li>Access cluster</li> <li>Master node is in the same zone from where gcloud is running <code>ssh [master-node-name]</code></li> <li><code>gcloud compute ssh [master-node-name] --zone [zone_name]</code></li> <li>Access via Web GUI</li> <li>Open firewall port to network</li> <li>Use SOCKS proxy - does not expose firewall ports</li> </ul>"},{"location":"cloud/gcp/bigdata/dataproc/#iam","title":"IAM","text":""},{"location":"cloud/gcp/bigdata/dataproc/#connect-to-dataproc","title":"Connect to Dataproc","text":"<ul> <li>Open ssh tunel to service like yarn GUI <code>gclould compute ssh ....</code></li> <li>Connect to vm:</li> <li>Generate ssh key using <code>ssh key-gen ....</code></li> <li>See: https://www.youtube.com/watch?v=6DD-vBdJJxk</li> </ul>"},{"location":"cloud/gcp/bigdata/dataproc/#monitoring-and-logs","title":"Monitoring and logs","text":"<ul> <li>Use Cloud logging and Cloud Monitoring</li> </ul>"},{"location":"cloud/gcp/bigdata/datastudio/","title":"Data Studio","text":"<ul> <li>The service is not present in GCP</li> <li>Is part from G Suite</li> <li>No IAM are applicable. Permissions in GCP will determine access to resources</li> <li>Caching<ul> <li>Query caching<ul> <li>Remembers queries issues by reports components</li> <li>When performing same query, pulls from cache</li> <li>If query cache cannot help, goes to prefetch cache</li> <li>Cannot be turnned of</li> </ul> </li> <li>Prefetch cache<ul> <li>Smart cache, predicts what might be requested</li> <li>If prefetch cache cannot serve data, pulls from live data set</li> <li>Only active for data sources that use owner's credentials for data access</li> <li>Can be turned of</li> </ul> </li> </ul> </li> <li>Turn the caching off<ul> <li>Nedd to view fresh data from rapidly caching dataset</li> </ul> </li> </ul>"},{"location":"cloud/gcp/bigdata/intro/","title":"Intro","text":""},{"location":"cloud/gcp/bigdata/intro/#data-engineer-challenges","title":"Data engineer challenges","text":"<ul> <li>Access Data</li> <li>Data accuracy and quality</li> <li>Availability of computational resources</li> <li>Query performance</li> </ul>"},{"location":"cloud/gcp/bigdata/intro/#load-data-into-the-cloud","title":"Load data into the cloud","text":"<ul> <li>Extract and Load (EL)</li> <li>Extract, Load and Transform (ELT)</li> <li>Extract, Transform and Load</li> </ul>"},{"location":"cloud/gcp/bigdata/pubsub/","title":"Pub/Sub","text":"<ul> <li>Distributed messagin service</li> <li>Serverless</li> <li>Global scale message buffer/coupler</li> <li>No-ops, global availability and auto-scaling</li> <li>Decouple sender and receiver</li> <li>Guarantee At-last-once delivery</li> <li>exaclty-one delivery can only be available for pull method</li> <li>Asynchronous messaging (ex: many-to-many) or other combination</li> <li>Message are stored in Message Storage</li> <li>Ensure end-to-end encryption (in-transit and rest)</li> <li>Kafka Connect can be useb to link PubSub to Kafka</li> <li> <p>Supports many inputs and outputs</p> </li> <li> <p>Patterns</p> </li> <li>One publisher to one subscriber</li> <li>Fan-in or loadbalancer: multiple pub and multiple sub</li> <li>Fan-out: one pub to multiple sub</li> </ul>"},{"location":"cloud/gcp/bigdata/pubsub/#pull","title":"Pull","text":"<ul> <li>Subscriber must send and ack to the topic.</li> <li>Messages are stored up to 7 days.</li> </ul>"},{"location":"cloud/gcp/bigdata/pubsub/#push","title":"Push","text":"<p>Subscriber respond only with 200 OK for HTTP call that tell to Pub-Sub the message delivery was successful.</p> <p></p>"},{"location":"cloud/gcp/bigdata/pubsub/#commands","title":"Commands","text":"<ul> <li>Pull: <code>gcloud pubsub subscriptions pull sub-name --auto-ack</code></li> </ul>"},{"location":"cloud/gcp/bigdata/pubsub/#iam","title":"IAM","text":""},{"location":"cloud/gcp/bigdata/pubsub/#example","title":"Example","text":"<pre><code>gcloud pubsub topics create sandiego\ngcloud pubsub topics publish sandiego --message \"hello\"\ngcloud pubsub subscriptions create --topic sandiego mySub1\ngcloud pubsub subscriptions pull --auto-ack mySub1\ngcloud pubsub subscriptions delete mySub1\n</code></pre>"},{"location":"cloud/gcp/databases/","title":"Databases","text":""},{"location":"cloud/gcp/databases/#decision-tree","title":"Decision tree","text":""},{"location":"cloud/gcp/databases/#comapraison","title":"Comapraison","text":""},{"location":"cloud/gcp/databases/best-practices/","title":"Best Practices","text":""},{"location":"cloud/gcp/databases/best-practices/#sql","title":"SQL","text":"<ul> <li>Small table instead or larger ones (data denormalization)</li> <li>Select with fields names instead of SELECT *</li> <li>Use INNER JOIN instead of WHERE (Where creates more combinaison =&gt; more work)</li> <li>Do biggest join and filter per join</li> <li>Filter early and big with WHERE</li> <li>Partiton data (For BigQuery)</li> <li>Partition by ingest time</li> <li>Partition by specied data columns</li> <li>LIMIT does not affect performance</li> </ul>"},{"location":"cloud/gcp/databases/best-practices/#dataflow","title":"Dataflow","text":"<ul> <li>Handles errors</li> <li>Gracefully catch errors using Try-Catch blocks<ul> <li>Output errors to PCollection and set the collector for the later analysis</li> <li>Think about it as recyling the bad data</li> </ul> </li> <li>How to update exiting code ?</li> </ul>"},{"location":"cloud/gcp/databases/bigtable/","title":"Bigtable","text":"<ul> <li>High performance and massively scalable NoSQL database</li> <li>What is used for ?</li> <li>High throughput analytic</li> <li>Huge datasets</li> <li>Access control</li> <li>Project level or instance level</li> <li>Read/Write/Manage access</li> </ul>"},{"location":"cloud/gcp/databases/bigtable/#features","title":"Features","text":"<ul> <li>No-ops (Servers)</li> <li>An instance: is group of nodes (cluster)</li> <li>Auto-scaling</li> <li>02 instances types:</li> <li>Dev: low cost, single node and no replication</li> <li>Prod: replication is available and throughput guarantee</li> </ul>"},{"location":"cloud/gcp/databases/bigtable/#replicates-and-changes","title":"Replicates and changes","text":"<ul> <li>Synchronize date between clusters</li> <li>Resizing: add or remove a node without downtime</li> <li>Changing disk type (ex: from HDD to SSD) requires new instance</li> </ul>"},{"location":"cloud/gcp/databases/bigtable/#interacting-with-bigtable","title":"Interacting with Bigtable","text":"<ul> <li>Using cbt tool (recommended)</li> <li>With HBase shell</li> </ul>"},{"location":"cloud/gcp/databases/bigtable/#example","title":"Example","text":"<ul> <li><code>echo -e \"project = [PROJECT_ID]\\ninstance = [INSTANCE_ID]\" &gt; ~/.cbtrc</code></li> <li>Create table <code>cbt createtable my-table</code></li> <li>Add column family <code>cbt createfamily my-table cf1</code></li> <li>Add to row 1 the value to column c1 <code>cbt set my-table r1 cf1:c1=test-value</code></li> <li>Read table <code>cbt read my-table</code></li> <li>Delete table <code>cbt deletetable my-table</code></li> </ul>"},{"location":"cloud/gcp/databases/bigtable/#schema-design","title":"Schema Design ??","text":""},{"location":"cloud/gcp/databases/bigtable/#tall-and-narrow-tables","title":"Tall and Narrow Tables ??","text":""},{"location":"cloud/gcp/databases/bigtable/#key","title":"Key ??","text":""},{"location":"cloud/gcp/databases/bigtable/#iam","title":"IAM","text":"<ul> <li>Project, instance and table level</li> </ul>"},{"location":"cloud/gcp/databases/datastore/","title":"Cloud Datastore","text":"<ul> <li>No relational database</li> <li>NoSQL</li> <li>Transactional</li> <li>ACID support</li> <li>Is schema-less</li> <li>Uses SQL like language GQL</li> <li>One Datastore per project</li> <li>Data structure</li> <li>Kind &lt;=&gt; Table</li> <li>Entity &lt;=&gt; Row</li> <li>Property &lt;=&gt; Column</li> <li>Key &lt;=&gt; Primary Key</li> <li>Consistency:</li> <li>How up to date are the results ?</li> <li>Does the order matter ?</li> <li>Two types of consistencies:<ul> <li>Strongly: order matter but queries are long</li> <li>Eventually: order not matter but queries are very fast</li> </ul> </li> </ul>"},{"location":"cloud/gcp/databases/spanner/","title":"Cloud Spanner","text":"<ul> <li>Feature</li> <li>Fully managed</li> <li>Highly scalable and available</li> <li> <p>Relational database</p> </li> <li> <p>What is used for</p> </li> <li>Critical mission databases which needs strong ACID</li> <li>Wide scale availability</li> <li>Support SQL</li> <li>It's horizontally scalable (more node) vs Vertical (RAM and CPU)</li> <li>IAM</li> <li>Project, instance or database level</li> <li>Role/Spanner<ul> <li>Admin</li> <li>Database Admin: create/edit/delete and grant access to databases</li> <li>Reader: read/execute database/schema</li> <li>Viewer: view instances and databases</li> </ul> </li> <li>Interleave table</li> <li>Avoid hotspoting</li> <li>No sequential numbers with ID</li> <li>No timestamps (also sequential)</li> <li> <p>Use descending order timestamp if is required</p> </li> <li> <p>Distinguished from Cloud SQL by its global consistent and size</p> </li> <li>Secondary indexes</li> <li>Allows to search quickly for values</li> <li>Adding index to existing table doesn't lock the table</li> </ul>"},{"location":"cloud/gcp/databases/sql/","title":"Cloud SQL","text":"<ul> <li>Fully managed relational databases service</li> <li>Available RDMS</li> <li>Postgresql</li> <li>MySql</li> <li>SQL Server</li> <li>Accessible from other GCP services and external services</li> <li>Benefit from Google security</li> <li>Vertical scaling (read and write)</li> <li>Scale up to 64 processor cores and more than 100 gigabytes of RAM</li> <li>Horizontally (read)</li> <li>Automatic replication</li> <li>Read replicas: used in case a read throughput</li> <li>Retains up to seven backups for each instance, which is included in the cost of your instance</li> </ul>"},{"location":"cloud/gcp/labs/bigquery/","title":"Bigquery","text":"<pre><code>create or replace TABLE ecommerce.products AS\nSELECT\n*\nFROM\n`data-to-insights.ecommerce.products`\n</code></pre> <ul> <li>Partitionned dataset</li> </ul> <pre><code>CREATE TABLE covid.oxford_policy_tracker\nPARTITION BY date\nOPTIONS (\n  partition_expiration_days=1080\n) AS\nSELECT\n  *\nFROM\n  `bigquery-public-data.covid19_govt_response.oxford_policy_tracker`\nWHERE\n  alpha_3_code NOT IN ('GBR', 'BRA', 'CAN', 'USA');\n</code></pre>"},{"location":"cloud/gcp/labs/gke/","title":"Gke","text":"<pre><code>gcloud config set compute/region us-central1\ngcloud config set compute/zone us-central1-f\n\n# create the cluster\ngcloud container clusters create --machine-type=e2-medium --zone=us-central1-f lab-cluster \n\n# get credentials \ngcloud container clusters get-credentials lab-cluster \n\n# deploy hello app \nkubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0\nkubectl expose deployment hello-server --type=LoadBalancer --port 8080\nkubectl get service\n\n# delete the cluster \ngcloud container clusters delete lab-cluster \n</code></pre>"},{"location":"cloud/gcp/labs/hadoop_tutorial/","title":"Hadoop tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Example Airflow DAG that creates a Cloud Dataproc cluster, runs the Hadoop\nwordcount example, and deletes the cluster.\n\nThis DAG relies on three Airflow variables\nhttps://airflow.apache.org/concepts.html#variables\n* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.\n* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be\n  created.\n* gcs_bucket - Google Cloud Storage bucket to used as output for the Hadoop jobs from Dataproc.\n  See https://cloud.google.com/storage/docs/creating-buckets for creating a\n  bucket.\n\"\"\"\n</pre> \"\"\"Example Airflow DAG that creates a Cloud Dataproc cluster, runs the Hadoop wordcount example, and deletes the cluster.  This DAG relies on three Airflow variables https://airflow.apache.org/concepts.html#variables * gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster. * gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be   created. * gcs_bucket - Google Cloud Storage bucket to used as output for the Hadoop jobs from Dataproc.   See https://cloud.google.com/storage/docs/creating-buckets for creating a   bucket. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import datetime\nimport os\n</pre> import datetime import os In\u00a0[\u00a0]: Copied! <pre>from airflow import models\nfrom airflow.contrib.operators import dataproc_operator\nfrom airflow.utils import trigger_rule\n</pre> from airflow import models from airflow.contrib.operators import dataproc_operator from airflow.utils import trigger_rule In\u00a0[\u00a0]: Copied! <pre># Output file for Cloud Dataproc job.\noutput_file = os.path.join(\n    models.Variable.get('gcs_bucket'), 'wordcount',\n    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n# Path to Hadoop wordcount example available on every Dataproc cluster.\nWORDCOUNT_JAR = (\n    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n)\n# Arguments to pass to Cloud Dataproc job.\nwordcount_args = ['wordcount', 'gs://pub/shakespeare/rose.txt', output_file]\n</pre> # Output file for Cloud Dataproc job. output_file = os.path.join(     models.Variable.get('gcs_bucket'), 'wordcount',     datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep # Path to Hadoop wordcount example available on every Dataproc cluster. WORDCOUNT_JAR = (     'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar' ) # Arguments to pass to Cloud Dataproc job. wordcount_args = ['wordcount', 'gs://pub/shakespeare/rose.txt', output_file] In\u00a0[\u00a0]: Copied! <pre>yesterday = datetime.datetime.combine(\n    datetime.datetime.today() - datetime.timedelta(1),\n    datetime.datetime.min.time())\n</pre> yesterday = datetime.datetime.combine(     datetime.datetime.today() - datetime.timedelta(1),     datetime.datetime.min.time()) In\u00a0[\u00a0]: Copied! <pre>default_dag_args = {\n    # Setting start date as yesterday starts the DAG immediately when it is\n    # detected in the Cloud Storage bucket.\n    'start_date': yesterday,\n    # To email on failure or retry set 'email' arg to your email and enable\n    # emailing here.\n    'email_on_failure': False,\n    'email_on_retry': False,\n    # If a task fails, retry it once after waiting at least 5 minutes\n    'retries': 1,\n    'retry_delay': datetime.timedelta(minutes=5),\n    'project_id': models.Variable.get('gcp_project')\n}\n</pre> default_dag_args = {     # Setting start date as yesterday starts the DAG immediately when it is     # detected in the Cloud Storage bucket.     'start_date': yesterday,     # To email on failure or retry set 'email' arg to your email and enable     # emailing here.     'email_on_failure': False,     'email_on_retry': False,     # If a task fails, retry it once after waiting at least 5 minutes     'retries': 1,     'retry_delay': datetime.timedelta(minutes=5),     'project_id': models.Variable.get('gcp_project') } In\u00a0[\u00a0]: Copied! <pre>with models.DAG(\n        'composer_sample_quickstart',\n        # Continue to run DAG once per day\n        schedule_interval=datetime.timedelta(days=1),\n        default_args=default_dag_args) as dag:\n\n    # Create a Cloud Dataproc cluster.\n    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n        task_id='create_dataproc_cluster',\n        # Give the cluster a unique name by appending the date scheduled.\n        # See https://airflow.apache.org/code.html#default-variables\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        num_workers=2,\n        region='us-central1',\n        zone=models.Variable.get('gce_zone'),\n        image_version='2.0',\n        master_machine_type='n1-standard-2',\n        worker_machine_type='n1-standard-2')\n\n    # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster\n    # master node.\n    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n        task_id='run_dataproc_hadoop',\n        region='us-central1',\n        main_jar=WORDCOUNT_JAR,\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        arguments=wordcount_args)\n\n    # Delete Cloud Dataproc cluster.\n    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n        task_id='delete_dataproc_cluster',\n        region='us-central1',\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted\n        # even if the Dataproc job fails.\n        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)\n\n    # Define DAG dependencies.\n    create_dataproc_cluster &gt;&gt; run_dataproc_hadoop &gt;&gt; delete_dataproc_cluster\n</pre> with models.DAG(         'composer_sample_quickstart',         # Continue to run DAG once per day         schedule_interval=datetime.timedelta(days=1),         default_args=default_dag_args) as dag:      # Create a Cloud Dataproc cluster.     create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(         task_id='create_dataproc_cluster',         # Give the cluster a unique name by appending the date scheduled.         # See https://airflow.apache.org/code.html#default-variables         cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',         num_workers=2,         region='us-central1',         zone=models.Variable.get('gce_zone'),         image_version='2.0',         master_machine_type='n1-standard-2',         worker_machine_type='n1-standard-2')      # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster     # master node.     run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(         task_id='run_dataproc_hadoop',         region='us-central1',         main_jar=WORDCOUNT_JAR,         cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',         arguments=wordcount_args)      # Delete Cloud Dataproc cluster.     delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(         task_id='delete_dataproc_cluster',         region='us-central1',         cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',         # Setting trigger_rule to ALL_DONE causes the cluster to be deleted         # even if the Dataproc job fails.         trigger_rule=trigger_rule.TriggerRule.ALL_DONE)      # Define DAG dependencies.     create_dataproc_cluster &gt;&gt; run_dataproc_hadoop &gt;&gt; delete_dataproc_cluster"},{"location":"cloud/gcp/labs/vm/","title":"Vm","text":""},{"location":"cloud/gcp/labs/vm/#steps","title":"steps","text":"<pre><code>gloud auth list \ngloud config list project \ngcloud config set compute/region us-central1\nexport REGION=us-central1\nexport ZONE=us-central1-f\ngcloud compute instances create gcelab2 --machine-type e2-medium --zone=$ZONE\n</code></pre>"},{"location":"cloud/gcp/ml/ai-platform/","title":"AI platform","text":"<ul> <li> <p>Component of distributed model</p> <ul> <li>Master: manages other nodes</li> <li>Workers: works on the portion of the training</li> <li>Parameters server: coordinates shaded model state between worker</li> </ul> </li> <li> <p>Types of predictions</p> <ul> <li>Online<ul> <li>Real-time processing with fully managed ML Engine</li> <li>No Docker container required &amp; supports multiple framework</li> </ul> </li> <li>Batch<ul> <li>For asynchronous operations</li> <li>Scales to terabytes of data</li> </ul> </li> </ul> </li> <li>Terminolgy<ul> <li>Model</li> <li>Version: instance of model</li> <li>Job: interaction with AI Platform</li> </ul> </li> </ul>"},{"location":"cloud/gcp/ml/ai-platform/#iam-roles","title":"IAM Roles","text":"<ul> <li>Project and Model roles<ul> <li>Admin</li> <li>Developer</li> <li>Viewer</li> </ul> </li> <li>Model Only roles<ul> <li>Model Owner: full access to model and versions</li> <li>Model User:<ul> <li>Read models and use for prediction</li> <li>Easy to share specific models</li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud/gcp/ml/ai-platform/#machine-scale","title":"Machine scale","text":"<ul> <li>BASIC</li> <li>STANDARD_1</li> <li>PREMIUM_1</li> <li>BASIC_GPU</li> <li> <p>CUSTOM</p> </li> <li> <p>GPU(Graphic Processing Unit) /TPU (Tensor Processing Unit)</p> </li> </ul>"},{"location":"cloud/gcp/ml/basics/","title":"Basics","text":"<ol> <li>Basics</li> <li>Overfitting</li> </ol>"},{"location":"cloud/gcp/ml/basics/#basics_1","title":"Basics","text":"<ul> <li>Supervised Learning</li> </ul> <p>Based on historical data that has labels, train a model to predict the values of labels (dependent variable) based on features. Example: Predict person income (labels) based on the age (feature)</p> <ul> <li>Two groups of supervised learning</li> <li>Classification: train an algorithm to predict a dependent variable that is categorical (beloging to discrete, finite set of values)<ul> <li>Binary classification</li> <li>Multiple classification</li> </ul> </li> <li> <p>Regression: predict on continuous variable</p> </li> <li> <p>Recommendation engines:</p> </li> <li>Suggest based on the behavior</li> <li>Ex: based on the rating, suggest to customer the product</li> <li>Unsupervised Learning: try to find a patterns or discover the underlying structure in a given data set</li> <li>Clustering</li> <li>Anomaly detection</li> <li>Topic modeling</li> </ul>"},{"location":"cloud/gcp/ml/basics/#overfitting","title":"Overfitting","text":"<ul> <li>Model is unable to generalize with new data</li> <li>Causes</li> <li>Not enough training data</li> <li>Too many feature (Too complex model)</li> <li> <p>Model fited too necessary features Noise</p> </li> <li> <p>Solutions</p> </li> <li>Mode data</li> <li>Make model less complex</li> <li> <p>Remove noise from the model by increasing the regularization parameters</p> </li> <li> <p>What's regularization ?</p> </li> <li> <p>Wide neural and Deep neural ?</p> </li> <li>Wide models are used for memorization. Deep models are for generalization</li> <li>Deep and wide models are ideal for a recommendation application.</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/","title":"ML in GCP","text":"<p>It offers four options to build ML models</p> <ul> <li>BigQuery ML: uses sql query to create ml models</li> <li>Pre-Builds APIs: programming options</li> <li>Auto ML: no code options to build models on Vertex AI</li> <li>Custom training: custom environment</li> </ul> <p></p> <ul> <li>[source]: google training session</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/#how-to-choose","title":"How to choose ?","text":""},{"location":"cloud/gcp/ml/ml_in_gcp/#pre-builds-apis","title":"Pre-Builds APIs","text":"<ul> <li>Neutral Language API</li> <li>Extract entities</li> <li>Detect sentiments</li> <li>Analyze syntax</li> <li>Classify content</li> <li> <p>Vision API</p> <ul> <li>Lab &amp; web detection</li> <li>Logo detection</li> <li>Landmark detection</li> <li>Crop hints</li> <li>Explicit content detection</li> </ul> </li> <li> <p>Cloud Speech-to-Text API</p> </li> <li>Three types of reorganization<ul> <li>Synch (REST and gRPC): 1 minute or less</li> <li>Asynch (REST and gRPC): up to 480 minutes</li> <li>Streaming (gRPC): for real-time speech</li> </ul> </li> <li>Sample rate between 8000 Hz and 48000 Hz</li> <li>Recommended 16000 Hz</li> <li>No need to resample existing audio</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/#automl","title":"AutoML","text":"<ul> <li>Transfer learning</li> <li>Neural Architecture Search: find the optimal model for the relevant project.</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/#custom-training","title":"Custom training","text":"<ul> <li>Vertex AI Workbench</li> <li>Pre-built container: Tensorflow, Pytorch, Scikit-learn, ...</li> <li>Custom container</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/#vertex-ai","title":"Vertex AI","text":"<ol> <li> <p>Data preparation</p> </li> <li> <p>Upload data: text, tabular, image, video,</p> </li> <li> <p>Feature engineering</p> </li> <li> <p>Train model</p> </li> <li> <p>Model training</p> </li> <li>Model evaluation</li> </ol> <p></p> <ul> <li> <p>[source]: google training session</p> </li> <li> <p>Model serving</p> </li> <li> <p>Model deployment</p> </li> <li>Model monitoring</li> </ul>"},{"location":"cloud/gcp/ml/ml_in_gcp/#bigquery-ml","title":"BigQuery ML","text":"<ul> <li>Extract data from dataset</li> <li>Create the model (specify the type, hyperparameters)</li> <li>Evaluate model</li> <li>Predict</li> </ul>"},{"location":"cloud/gcp/security/security/","title":"Basics","text":""},{"location":"cloud/gcp/security/security/#iam","title":"IAM","text":"<p>Apply policies that define who can do what on which resources.</p> <p>The who part of an IAM policy can be:</p> <ul> <li>Google account</li> <li>Google group</li> <li>service account</li> <li>Cloud Identity domain</li> </ul> <p>The can do what part of an IAM policy is defined by a role.</p>"},{"location":"cloud/gcp/security/security/#roles","title":"Roles","text":"<p>A role is a collection of permissions.</p> <ul> <li>Primitive roles</li> <li>Owner</li> <li>Editor</li> <li>Viewer</li> <li>Billing Admin</li> <li>Predefinded roles (Managed by Google)</li> <li>Custom roles</li> <li>Can only be applied to either the project level or organization level. They can\u2019t be applied to the folder level.</li> </ul>"},{"location":"cloud/gcp/security/security/#cloud-identity","title":"Cloud Identity","text":"<ul> <li>IDaaS (Identity as a Service)</li> <li>Origanilly from G-Suite</li> <li>Allows Single Sing On (SSO)</li> <li>Available identities:</li> <li>Google account (managed account) or unmanaged account</li> <li>Service acount<ul> <li>Used when you want authenticate machines to like VM to use other GCP services</li> </ul> </li> <li>Google Group, G-Suite Domain, Cloud Identity Domain</li> </ul>"},{"location":"cloud/gcp/security/security/#resources","title":"Resources","text":"<ul> <li>Project or Folders</li> <li>Cloud services (Pub/Sub, BigQuery, ...)</li> <li>Aspect of service (Topics, Datasets, Tables, ...)</li> </ul>"},{"location":"cloud/gcp/security/security/#cloud-kms","title":"Cloud KMS","text":""},{"location":"cloud/gcp/security/security/#cloud-iap","title":"Cloud IAP","text":""},{"location":"elk/Elasticsearch/concepts/","title":"Concepts","text":""},{"location":"elk/Elasticsearch/concepts/#componenets","title":"Componenets","text":"<ul> <li>Index: virtual namespace which points to shard. It is similar to BD<ul> <li>Is created automtically if not exists (Diasable the option in prod mode)</li> </ul> </li> <li>Mapping: schema</li> <li>Type =&gt; table</li> <li> <p>Document: une ent\u00e9e dans le type</p> <ul> <li>A document is indexed into an index</li> <li>Doc is update if we put new one with same ID (overwrites new one) (201 http status)</li> <li>_create: endpoint to avoid overwriting existing document (409 http status)</li> <li>_update: to update filed in document</li> <li>_mget: get multiple documents in a single request</li> </ul> </li> <li> <p>Node: is an instance of ES</p> <ul> <li>A java process that runs in JVM</li> <li>Has a unique ID (randomly generated UUID)</li> <li>Has a name (default is hostname)<ul> <li>Can be set in .yml file</li> <li>or <code>./bin/elasticsearch -E node.name=name_node</code></li> </ul> </li> </ul> </li> <li>Cluster:<ul> <li>Is one or multiple nodes working together in distributed mode</li> <li>Has a name (default elasticsearch)<ul> <li>Can be set in .yml file</li> <li>or <code>./bin/elasticsearch -E cluster.name=name_cluster</code></li> </ul> </li> </ul> </li> <li> <p>Shard: is worker unit that holds data and can be assigned to nodes</p> <ul> <li>Primary: original shard of the index</li> <li>Replica: copy of the primary shard</li> <li>The primary and replica are always in different nodes</li> </ul> </li> <li> <p>Replicas</p> </li> </ul> <p>Archi  - bin   - config  - data : indexed data in ES  - jdk  - lib  - logs  - modules  - plugins : each plugin is contained in a subdirectory </p> <p>Run as deamon :  <pre><code>./bin/ealasticsearch/ -d -p elastic.pid \n</code></pre></p>"},{"location":"elk/Elasticsearch/concepts/#aggregation-type","title":"Aggregation type:","text":"<ul> <li>Metrics </li> <li>Buckets: </li> <li>collection of docs that share  a common criteria</li> <li>More then 25 buckets strtegies are supported</li> </ul>"},{"location":"elk/Elasticsearch/concepts/#mapping","title":"Mapping","text":"<p>Can be: - Define at the creation - Added to exiting index</p>"},{"location":"elk/Elasticsearch/concepts/#inverted-index","title":"Inverted index","text":"<ul> <li>Make searching fast</li> </ul>"},{"location":"elk/Elasticsearch/concepts/#doc-values","title":"Doc values","text":"<ul> <li>Allows to perform aggregations and sort</li> </ul>"},{"location":"elk/Elasticsearch/concepts/#optimize-mapping","title":"Optimize mapping","text":"<ul> <li>Disable inverted index<ul> <li>Field available for quring but not for aggregation</li> <li><code>\"index\": false</code></li> </ul> </li> <li>Disable a doc values<ul> <li>Field  not available for quring but available for aggregation and sort</li> <li><code>\"doc_values\": false</code></li> </ul> </li> <li>Disable field<ul> <li>Cannot query or aggregate this field</li> <li><code>\"enabled\": false</code></li> </ul> </li> <li>Disable an object <code>\"enabled\": false</code></li> </ul>"},{"location":"elk/Elasticsearch/concepts/#dynamic-template","title":"Dynamic template","text":"<ul> <li>Make it easier to set up your own mappings by defining defaults for fields, based on their JSON type, name or path</li> </ul>"},{"location":"elk/Elasticsearch/concepts/#write-operation","title":"Write operation","text":""},{"location":"elk/Elasticsearch/concepts/#search-operation","title":"Search operation","text":"<ul> <li>Query Phase<ul> <li>Broadcast search query to all nodes</li> <li>Each node return doc IDs and sort values</li> </ul> </li> <li>Fetch Phase</li> </ul>"},{"location":"elk/Elasticsearch/concepts/#performance-issues","title":"Performance issues","text":"<ul> <li>Profile a query <code>\"profile\": true</code></li> <li>Use task API  <code>GET _task</code> to view task properties</li> <li>Debug thread pool and hot thread <code>GET _nodes/thread_pool</code>, <code>GET _nodes/hot_threads</code></li> <li>Check for sespending tasks <code>GET _cluster/pending_tasks</code></li> <li>Circuit breakers</li> </ul> <p>https://www.elastic.co/guide/index.html https://logz.io/blog/elasticsearch-tutorial/</p>"},{"location":"elk/Elasticsearch/crud-operation/","title":"Crud operation","text":"<ul> <li>Create index: <code>PUT name-of-index</code></li> </ul>"},{"location":"elk/Elasticsearch/crud-operation/#create","title":"Create","text":"<ul> <li>Add doc to an index: POST or PUT can be used.</li> <li> <p>POST is used when we want ES to auto-generate id for the doc</p> </li> <li> <p><code>POST name-of-index/_doc {     \"field\": \"value\"    }</code></p> </li> </ul>"},{"location":"elk/Elasticsearch/crud-operation/#read","title":"Read","text":"<ul> <li>GET is used to get a doc:  <code>GET name-of-index/_doc/id-of-doc</code></li> </ul>"},{"location":"elk/Elasticsearch/crud-operation/#update","title":"Update","text":"<ul> <li>_update endpoint is used:  </li> </ul> <pre><code>POST name-of-index/_doc/id-of-doc\n{\n  \"doc\": {\n    \"field\": \"new_value\"\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/crud-operation/#delete","title":"Delete","text":"<ul> <li>DELETE is used</li> <li><code>DELETE name-of-index/_doc/id-of-doc</code></li> </ul>"},{"location":"elk/Elasticsearch/examples/","title":"Examples","text":"<p>GET /_cluster/health</p>"},{"location":"elk/Elasticsearch/examples/#mapping","title":"Mapping","text":"<pre><code>PUT /users\n{\n  \"settings\": {\n    \"number_of_replicas\": 1,\n    \"number_of_shards\": 1,\n    // Time to wait before the doc become searcheable\n    \"refresh_interval\": \"1s\"\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"firstname\": {\"type\": \"text\"},\n      \"lastname\": {\n        \"type\": \"text\",\n        // Disable completely the field\n        \"enable\": false,\n         // Disable index\n         \"index\": false,\n         // Disable doc_values\n         \"doc_values\": false\n      },\n      \"nick_name\": {\n        \"type\": \"text\",\n        // Null value: default value if the field is null\n        \"null_value\": \"no nick_name\"\n      },\n      \"age\": {\n        \"type\": \"byte\",\n        // If data type is other than byte, indexing of document will fail\n        \"coerce\": false\n      },\n      \"email\": {\"type\": \"text\"},\n      // Copy_to\n      \"contry_name\": {\n        \"type\": \"keyword\",\n        \"copy_to\": \"combined_location\"\n      },\n      \"city_name\": {\n        \"type\": \"keyword\",\n        \"copy_to\": \"combined_location\"\n      },\n      // It's not part of _source but it's indexed\n      \"combined_location\": {\n        \"type\": \"text\"\n      },\n      \"address\": {\n        \"type\": \"nested\",\n        \"properties\": {\n          \"street_name\": {\"type\": \"text\"},\n          \"postal_code\": {\"type\": \"short\"},\n          \"city\": {\"type\": \"keyword\"},\n          \"county\":\n          {\n            \"type\": \"text\",\n            \"fields\": {\n              \"keyword\" : {\n                \"type\": \"keyword\",\n                \"ignore_above\" : 256\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>GET users/_mapping GET users/_settings DELETE users</p>"},{"location":"elk/Elasticsearch/examples/#create-only-if-not-exist","title":"Create only if not exist","text":"<pre><code>PUT /users/_create/eC-O73EB-Vx1LmGgtoA0\n{\n  \"age\": 18,\n  \"firstname\": \"test\",\n  \"lastname\": \"test\",\n  \"email\": \"mail@test.com\"\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#update-only-if-exists","title":"Update only if exists","text":"<pre><code>POST /users/_update/eC-O73EB-Vx1LmGgtoA0\n{\n  \"doc\": {\n    \"toto\": \"test.mail@test.com\"\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#bulk","title":"Bulk","text":"<pre><code>POST users/_bulk\n{\"index\": {\"_id\": 1}}\n{\"firstname\": \"test2\"}\n</code></pre> <p>GET /users/_doc/eC-O73EB-Vx1LmGgtoA0</p> <p>HEAD /users/_doc/eC-O73EB-Vx1LmGgtoA0</p>"},{"location":"elk/Elasticsearch/examples/#search","title":"Search","text":"<ul> <li>Get all</li> </ul> <p><code>GET /users/_search</code></p> <pre><code>GET users/_search\n{\n  \"query\": {\n    \"match\": {\n      // And operator rather than Or (default)\n      \"firstname\": {\n        \"query\": \"toto tata\",\n        \"operator\": \"and\",\n        // the numer of term to be present in field  \n        \"minimum_should_match\": 2,\n        \"fuzziness\": 1\n      }\n    },\n    // all terms must be in field\n    // the order of terms must be the same\n    \"match_phrase\": {\n      \"firstname\": \"toto tata\"\n    }\n    // Mutiple fields\n    \"multi_match\": {\n      \"query\": \"toto tata\",\n      \"fields\": [\n        \"email\",\n        \"firstname\",\n        \"lastname\"\n      ],\n      \"type\": \"best_fields\"\n    }\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#combine-multiple-queries","title":"Combine multiple queries","text":"<pre><code>GET users/_search\n{\n  \"track_total_hits\": true,\n  \"query\": {\n    \"bool\": {\n      //\n      \"must\": [\n        {\n          \"match\": {\n            \"age\": \"18\"\n          }\n        }\n      ],\n      //\n      \"must_not\": [\n        {}\n      ],\n      //\n      \"should\": [\n        {}\n      ],\n      //\n      \"filter\": [\n        {}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#aggregation","title":"Aggregation","text":""},{"location":"elk/Elasticsearch/examples/#metrics","title":"Metrics","text":"<ul> <li>To get only aggregation result</li> </ul> <pre><code>GET users/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_agg\": {\n      \"sum\": {\n        \"field\": \"age\"\n      }\n    },\n    \"avg_agg\": {\n      \"avg\": {\n        \"field\": \"age\"\n      }\n    },\n    \"max_agg\": {\n      \"max\": {\n        \"field\": \"age\"\n      }\n    },\n    \"carnidality_agg\": {\n      \"cardinality\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#buckets","title":"Buckets","text":"<ul> <li>In all buckets we can order user (order)</li> </ul> <pre><code>GET users/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_bucket\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"day\"\n      }\n    },\n    \"histogramm_bucket\": {\n      \"histogram\": {\n        \"field\": \"age\",\n        \"interval\": 50\n      }\n    },\n    \"range_bucket\": {\n      \"range\": {\n        \"field\": \"\",\n        \"ranges\": [\n          {\n            \"from\": 0,\n            \"to\": 50\n          },\n          {\n            \"from\": 50,\n            \"to\": 100\n          }\n        ]\n      }\n    },\n    \"terms_bucket\":{\n      \"terms\": {\n        \"field\": \"firstname\",\n        \"size\": 10,\n        \"order\": {\n          \"_term\": \"asc\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#cluster","title":"Cluster","text":"<pre><code>  GET _cluster/state\n  GET _cluster/health?level=cluster\n  GET _cluster/health?level=shards\n  GET _cluster/health?level=indices\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#health-of-specific-index","title":"Health of specific index","text":"<p><code>GET _cluster/health/users</code></p>"},{"location":"elk/Elasticsearch/examples/#explain-assign","title":"Explain assign","text":"<p><code>GET _cluster/allocation/explain</code></p>"},{"location":"elk/Elasticsearch/examples/#explain-assign-for-specific-index","title":"Explain assign for specific index","text":"<pre><code>GET _cluster/allocation/explain\n{\n  \"index\": \"users\",\n  \"shard\": 0,\n  \"primary\": true\n}\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#monitoring","title":"Monitoring","text":""},{"location":"elk/Elasticsearch/examples/#cat-api","title":"cat API","text":"<pre><code>  GET _cat/nodes?help\n  GET _cat/nodes?v&amp;h=name,ip,cpu\n</code></pre>"},{"location":"elk/Elasticsearch/examples/#tasks","title":"Tasks","text":"<p>GET _cluster/pending_tasks</p> <p>// current executing tasks GET _tasks</p>"},{"location":"elk/Elasticsearch/node/","title":"Node","text":""},{"location":"elk/Elasticsearch/node/#roles","title":"Roles","text":"<ul> <li> <p>Data</p> <ul> <li>Holds shards that contain indexed document</li> <li>Perform CRUD operations, search and aggregations</li> <li>Configured using node.data property</li> <li>By default, all nodes are data nodes</li> <li>They are, I/O and memory intensive</li> <li>Must be monitored and add more data node if they are overloaded</li> </ul> </li> <li> <p>Ingest node</p> <ul> <li>Pre-process a document defore it gets indexed</li> <li>All nodes are ingest nodes by default</li> <li>Configured using node.ingest property</li> </ul> </li> <li> <p>Machine learing node</p> <ul> <li>Runs machine learning jobs</li> <li>Handles ml API requests</li> <li>Configured using node.ml property</li> </ul> </li> <li>Coordinating node</li> </ul>"},{"location":"elk/Elasticsearch/search/","title":"Search","text":"<ul> <li>Two main ways to search in ELK:</li> <li>Queries: the query cluases to matche documents (retrieve docs that match a criteria)</li> <li>Aggregations: the aggregation clauses to summarize data</li> </ul>"},{"location":"elk/Elasticsearch/search/#hints","title":"Hints","text":"<p>In case of queries</p> <ul> <li>Limited to 10 docs</li> <li>total is limited to 10000 after &gt;= 7.0 ELK version</li> <li><code>track_total_hits: true</code>: search in all hints </li> <li>track_total_hints can be used to get total value</li> <li>hits are stored by _score and each hit has _score</li> </ul>"},{"location":"elk/Elasticsearch/search/#true-positives-negatives","title":"True positives / negatives","text":"<ul> <li>Positives: relevent docs that are returned to the user.</li> <li>Negatives: irrevelant docs that were not retured to the user.</li> </ul>"},{"location":"elk/Elasticsearch/search/#flase-positives-negatives","title":"Flase positives / negatives","text":"<ul> <li>Positives: irrelevant docs that are returned to the user.</li> <li>Negatives: relevent docs that were not returned to the user.</li> </ul>"},{"location":"elk/Elasticsearch/search/#precesion","title":"Precesion","text":"<ul> <li>What portion of retrieved data is actually relevant to the search query ?</li> <li><code>precesion = True positives / True positives + False positives</code></li> </ul>"},{"location":"elk/Elasticsearch/search/#recall","title":"Recall","text":""},{"location":"elk/Elasticsearch/search/#ranking","title":"Ranking","text":"<ul> <li>Gives the more relevant doc to the search query</li> <li>Docs are ranked by the score </li> <li>Scoring algorithm is used to rank the docs</li> <li>Term Frequency (TF)</li> <li>Inverse Document Frequency (IDF) </li> </ul>"},{"location":"elk/filebeat/concepts/","title":"Filebeat","text":"<ul> <li>Log shipper </li> <li>Is installed on the machine that generate logs </li> <li>Prospectors : are responsible for locating specific files and applying basic processing      on them </li> <li> <p>Output : where to ship data to </p> <ul> <li>output.elasticsearch</li> <li>output.logstash</li> </ul> </li> <li> <p>log_location =&gt; hovster (read a single log and send) =&gt; libbeat(aggregate the events and send) =&gt; output</p> </li> <li> <p>test</p> <pre><code>    ./filebeat test config -e -c path/to/filebeat.yml\n</code></pre> </li> </ul>"},{"location":"elk/kibana/concepts/","title":"Concepts","text":""},{"location":"elk/kibana/concepts/#charts","title":"Charts","text":"<ul> <li>Line: observe trends over period of time</li> <li>Area: visualize multiple data sets</li> <li>Pie: visulize a part to whole relatioship </li> <li>Bar: visualize a nominal variable</li> <li>Vertical chart: visualize ordinal variable</li> </ul>"},{"location":"elk/logstash/concepts/","title":"Logstash","text":"<ul> <li>Input plugins: pull data using</li> <li>Filter plugins: transform data</li> <li> <p>Output plugins: push data</p> </li> <li> <p>Default logstash installation enables Filebeat plugin. </p> </li> </ul>"},{"location":"elk/logstash/concepts/#links","title":"Links","text":"<ul> <li>https://logz.io/blog/logstash-tutorial/</li> <li>https://logz.io/blog/logstash-pipelines/ </li> </ul>"},{"location":"finance/market-abuse/","title":"Market abuse","text":""},{"location":"finance/market-abuse/#market-controls","title":"Market controls","text":"<ul> <li>Wash trade: trader tends to simulate market activity on an instrument by placing sell orders and buy order by himself</li> <li>Spoofing and Layering:</li> <li>Front running</li> <li>A broker had an insider knwoledge (not yet public) that his company is about to sell or buy and that knwoledge will affect the price of an asset</li> <li>Insider trading:</li> <li>Ramping</li> </ul>"},{"location":"finance/market-abuse/#market-data","title":"Market data","text":"<ul> <li>Fidessa</li> <li>Rtgen</li> <li>Ice </li> <li>EBS (Electronic Broking Service)</li> <li>Retuers</li> <li>Dow Jones new</li> <li>Bloomberg </li> </ul>"},{"location":"git/basics/","title":"Basics","text":"<ul> <li>Each commit has a hash code</li> <li>Master branch created by default</li> <li>Head: is pointer that indicates the current branch on which we are</li> <li><code>git log --all --decorate --oneline --graph</code></li> <li>Create new branch</li> <li><code>git branch [branch-name]</code></li> <li>Show all branch</li> <li><code>git branch</code></li> <li>Create and checkout to branch</li> <li><code>git checkout -b [branch-name]</code></li> <li>Head will move out to this branch</li> <li>Add to stage area and commit a same time</li> <li> <p><code>git -a -m  \"commit message\"</code></p> </li> <li> <p>Remove files from staging area</p> </li> <li> <p>Remove one file: <code>git reset [file-name]</code></p> </li> <li> <p>Remove all files: <code>git reset</code></p> </li> <li> <p>Merge</p> </li> <li>Fast-forward: when it exists direct path between the two branches</li> <li> <p>3-way: it will create a three-way commit</p> </li> <li> <p>Delete branch</p> </li> <li><code>git branch --merged</code> # confirm that the branch is already merged before to delete it</li> <li><code>git branch -d [branch-name]</code></li> <li>Detached head</li> <li><code>git checkout [hash]</code> # this will detach the head from the last commit</li> <li> <p>To comme back to last commit  <code>git checkout [branch-name]</code></p> </li> <li> <p>Rebase</p> </li> <li><code>git merge --squash [branch-name]</code> # summarize the last change on one commit and merge to branch</li> <li> <p>Rebase workflow</p> <pre><code>  git pull \n  git checkout my_feature\n  git rebase master\n  git checkout master\n  git rebase my_feature\n</code></pre> </li> </ul>"},{"location":"git/basics/#remote-repo","title":"Remote repo","text":"<ul> <li>Clone</li> <li>Pull and push</li> <li>Delete branch on remote   <code>git push origin --delete [branch-name]</code></li> </ul>"},{"location":"git/basics/#git-objects","title":"Git objects","text":"<ul> <li>Blobs</li> <li>Trees</li> <li>Commits</li> <li>Annotated tags</li> </ul>"},{"location":"git/concepts/","title":"Concepts","text":""},{"location":"git/concepts/#pull-request-or-merge-request","title":"Pull request Or Merge Request","text":"<ul> <li>A notification to alert that the development is finished and someone else could start the code review</li> </ul>"},{"location":"git/concepts/#basic-commands","title":"Basic commands","text":"<ul> <li>Stash</li> <li>Save changes to temporary place</li> </ul> <pre><code>  git stash \"[description]\"\n  git stash list\n</code></pre> <ul> <li>Apply stash</li> </ul> <pre><code>  ## stash will still remain in stash\n  git stash apply [stash-name]\n  ## stash will be deleted from stash\n  git stash pop  \n</code></pre> <ul> <li>Drop stash if it's no longer needed</li> </ul> <pre><code>  git stash drop [stash-name]\n</code></pre> <ul> <li>Clear all stash</li> </ul> <pre><code>  git stash clear\n</code></pre>"},{"location":"git/concepts/#rollback-changes","title":"Rollback changes","text":"<ul> <li>Rollback modification in file before commit</li> <li> <p><code>git checkout [file-name]</code></p> </li> <li> <p>Modify commit message</p> </li> <li> <p><code>git --amend -m \"correct message test\"</code></p> </li> <li> <p>Left off to add file within a commit (NB: this is to be done only if the change are not yet been commit to remote)</p> </li> <li> <p><code>git --amend</code></p> </li> <li> <p>Commit to the wrong branch</p> </li> <li><code>git log</code> and copy the hash of the wrong commit in wrong branch</li> <li><code>git checkout [correct-branch]</code></li> <li><code>git cherry-pick [hash-copied]</code> # it doest delete in the original branch</li> <li><code>get checkout [wrong-branch]</code></li> <li> <p><code>get rest --soft [hash]</code></p> </li> <li> <p>Three type of reset: soft, mixed (default), hard</p> </li> <li>For reset hard git keep commit for certain amount of time</li> <li>Remove untracked file or directory <code>git clean -df</code></li> <li>Important command to know in case we lost a commit <code>git reflog</code></li> </ul>"},{"location":"git/git-flow/","title":"Git flow","text":""},{"location":"git/git-flow/#git-flow","title":"Git flow","text":"<p>Uses two branches - Master: used for production releases. - Develop: contains stable features for the next releases.</p> <p>Other special branches are used - Feature: the develop branch is its parent - Release: develop is its parent, and it used to indicate the end of development phase - HostFix:      - Used to correct bug in prod.     - Master its parent (forked from master)     - Merged to master and develop at the end</p> <p></p>"},{"location":"jupyter/commands/","title":"Commands","text":"<ul> <li> <p>Show available kernel </p> <pre><code>    ipython kernelspec list\n</code></pre> </li> </ul>"},{"location":"linux/basics/","title":"Basics","text":"<ul> <li>for loop</li> </ul> <pre><code>    for i in {1..10}; do touch test_file_$i.txt; done;\n</code></pre> <ul> <li> <p>archive</p> <ul> <li>c: create <code>tar cf my_archive.tar file_name*</code></li> <li>t: list   <code>tar tf my_archive.tar</code></li> <li>r: append <code>tar rf my_archive.tar file1.txt</code></li> <li>x: extract <code>tar xf my_archive.tar</code></li> <li>f: read or write from file</li> </ul> </li> <li> <p>compression</p> <ul> <li>z: create an archive with gzip <code>tar czf my_archive.tar.gz file_name*</code></li> <li>use bzip2: <code>tar cjf my_archive.tar.bz2 file_name*</code></li> <li>use zip: <code>zip -r archive.zip file_name*</code></li> </ul> </li> <li> <p>pipe</p> <ul> <li>find word and count occurrence: <code>cat my_file.txt | grep my_word | wc -l</code></li> <li><code>cat /etc/passwd | grep massi | cut -d: -f6</code></li> </ul> </li> <li> <p>wc</p> </li> <li> <p>count number of lines: <code>wc my_file.txt</code></p> </li> <li> <p>grep</p> </li> <li> <p>find words start with M <code>grep -E ^M my_test.txt</code></p> </li> <li> <p>Rerun the last command I used</p> </li> <li>ex: <code>!cat</code></li> </ul>"},{"location":"linux/basics/#files-and-directories","title":"Files and Directories","text":"<ul> <li>IO redirection</li> <li><code>&gt;</code> : stout</li> <li><code>&lt;</code> : sdin</li> <li><code>&gt;&gt;</code> : append</li> </ul>"},{"location":"linux/basics/#security","title":"Security","text":"<ul> <li><code>/etc/sudoers:</code> contains all what users can do</li> <li><code>/etc/password:</code> users and their info (9 fields)</li> <li><code>/etc/shadow:</code> hashed password users</li> <li><code>/etc/group:</code> users groups</li> <li> <p><code>/etc/login.defs</code> contains login configurations</p> </li> <li> <p>Difference between system user and standard user ?</p> </li> <li>System user (Service account) do not have a login shell</li> </ul>"},{"location":"linux/basics/#users-and-groups","title":"Users and groups","text":""},{"location":"linux/basics/#users","title":"Users","text":"<ul> <li>Create user <code>useradd -m username</code> (-m: make home dire) or <code>adduser username</code></li> <li>Modify user <code>usermod</code></li> <li>Assign password to user <code>passwd usename</code></li> <li><code>/etc/skel:</code> folder contains all files to be added in user's home when it's created</li> <li>UID:</li> <li>0: root</li> <li>1-99: system users</li> <li>100+: standard users</li> <li>65534: user nobody</li> </ul>"},{"location":"linux/basics/#groups","title":"Groups","text":"<ul> <li>Create group <code>groupadd groupname</code> or <code>addgroup groupname</code></li> <li>Add user to existing group <code>usermod -a -G groupname username</code></li> <li>After a user was added to a group it should logout in to be added to the group</li> </ul>"},{"location":"linux/basics/#network","title":"Network","text":"<ul> <li><code>ip addr show</code> or <code>ifconfig</code> show host ip adress</li> <li><code>dig hostname</code> or  <code>nslookup hostname</code> or <code>host hostname</code> resolve hostname</li> <li><code>/etc/resolv.conf</code> file used to determine which hosts to use for DNS queries</li> <li> <p><code>/etc/hosts</code> map ip address to hostnames</p> </li> <li> <p><code>ip route show</code> show the route we are traversing</p> </li> <li><code>netstat</code> view services and active connections</li> </ul>"},{"location":"linux/basics/#useful-commands","title":"Useful commands","text":"<ul> <li><code>w</code> check how is login</li> <li><code>last</code> the history of login</li> </ul>"},{"location":"linux/basics/#arguments","title":"Arguments","text":"<ul> <li><code>$@</code> stores list of all arguments  </li> <li><code>$#</code> stores the numbers of arguments</li> <li><code>$0</code> get the script name</li> <li><code>$?</code> exist status of the last command</li> <li><code>$*</code> show command line arguments</li> </ul>"},{"location":"linux/to_learn/","title":"To learn","text":"<p>swap memory ? cgroupdriver ?</p>"},{"location":"linux/users/","title":"Users","text":""},{"location":"linux/users/#users","title":"Users","text":"<ul> <li>Add user <code>adduser user_name</code></li> <li> <p>Force user to change pawssword <code>sudo passwd -e user_name</code></p> </li> <li> <p>Add user <code>useradd</code></p> </li> <li>Change password info <code>chage</code></li> </ul>"},{"location":"linux/security/linuxacademy_security/","title":"Linuxacademy security","text":""},{"location":"linux/security/linuxacademy_security/#x509-certification","title":"X.509 Certification","text":"<ul> <li>Key:<ul> <li>Used to encrypt data</li> <li>Must be secret</li> </ul> </li> <li>Algorithm<ul> <li>Method for encryption (process applied to make data unreadable)</li> <li>May be public</li> </ul> </li> <li>Encryption<ul> <li>Symmetric<ul> <li>One key for both parties</li> <li>Faster than asymmetric</li> <li>Ex: Blowfish, AES</li> </ul> </li> <li>Asymmetric: uses two keys<ul> <li>Public key for encryption</li> <li>Private key for decryption</li> </ul> </li> </ul> </li> <li>Hashing<ul> <li>Convert input to output</li> <li>Each string has a unique hash</li> <li>Is one way</li> <li>Common algorithm: crc-32 (insecure) md5, sha-1, ...</li> </ul> </li> <li>Salt<ul> <li>Additional text value added to the ciphertext to improve the security</li> </ul> </li> </ul>"},{"location":"ml/intro./","title":"Intro to ml","text":""},{"location":"ml/intro./#type","title":"Type","text":"<ul> <li>Supervised: Task-driven and identify a goal</li> <li>Classification:<ul> <li>Binary classification</li> <li>Multiclass</li> <li>Regression  </li> </ul> </li> <li>Unsupervised: Data-driven and identify a pattern pattern</li> <li>Reinforcement</li> </ul>"},{"location":"ml/intro./#flow","title":"Flow","text":"<ol> <li>Ask right questions</li> <li>Define end goal, starting point and how to achieve goal</li> <li>Understand the features in data</li> <li>Identify critical features</li> <li>Prepare data</li> <li>Data rule #3: Accurately predicting rare events is difficult</li> <li>Data rule #4: track how you manipulate data</li> <li>Select the algorithm</li> <li>Algorithm decision factors<ul> <li>Learning type (supervised or unsupervised)</li> <li>Result (regression or classification)</li> <li>Complexity</li> <li>Basic vs enhanced</li> </ul> </li> <li>Training the model</li> <li>Testing the model</li> </ol>"},{"location":"ml/intro./#data","title":"Data","text":"<ul> <li>Labeled</li> <li>Unlabeled</li> </ul> <p>It is typically numeral or categorical:</p> <ul> <li>Numerical such as age, income</li> <li>Categorical such as gender</li> <li>Ordinal: list of values (small, medium and large)</li> </ul> <p>To type of datasets are used</p> <ul> <li>Training: to train model</li> <li>Validation</li> <li>Test: to evaluate the</li> </ul> <p>Data --&gt; Information --&gt; Knowledge</p> <ul> <li>Information: the data that had been interpreted and manipulated and has some inference for the users</li> <li>Knowledge: combination of inferred information, experiences, learning, and insights.</li> </ul>"},{"location":"monitoring/steps/","title":"Steps","text":""},{"location":"monitoring/steps/#steps","title":"Steps","text":"<ol> <li>Identify data sources</li> <li>Determine the of data</li> <li>Think about the solution</li> </ol>"},{"location":"monitoring/steps/#elk-for-monitoring","title":"ELK for monitoring","text":"<ul> <li>Filebeat to collect logs</li> <li>Metricbeat for metrics</li> <li>Heartbeat for API testing (check either API is up or down)</li> <li>Elastic APM Java Agent for distributed tracing of the application</li> </ul>"},{"location":"monitoring/steps/#workflow","title":"Workflow","text":"<ol> <li> <p>Deploy ELK platform</p> </li> <li> <p>Collect logs</p> <ul> <li>Install Filebeat<ul> <li>Create three containers and install Filebeat on each of those containers</li> <li>Enable required modules (postgres, system and audit for need)</li> </ul> </li> <li>Run postgreSQL database inside docker container (create it using docker-compose)</li> </ul> </li> <li> <p>Collect metrics</p> </li> </ol>"},{"location":"monitoring/promethues/","title":"Prometheus","text":"<ul> <li>PromQL language used to run queries</li> <li>promtool can be used to check file's syntax</li> </ul>"},{"location":"monitoring/promethues/#time-series","title":"Time series","text":"<ul> <li>It is identified by:</li> <li>Metric name</li> <li>Label (key/value format) they are Optional</li> </ul>"},{"location":"monitoring/promethues/#metrics","title":"Metrics","text":"<p>It has four metric types</p> <ul> <li>Counter: the value that can be incremented or rest to zero (call to an API endpoint)</li> <li>Gauge: numerical value that can be incremented and decremented</li> <li>Histogram: count the number of observations/events that fall into a set of configurable buckets, each with its   own separate time series.</li> <li>Summary: is similar to a histogram, but it exposes metrics in the form of quantiles instead of bucket. While     buckets divide values based on specific boundaries, quantiles divide values based on the percentiles into which they    fall.</li> </ul>"},{"location":"monitoring/promethues/#exporters","title":"Exporters","text":"<ul> <li>Any application that exposes metrics in the format that Prometheus can read</li> </ul>"},{"location":"monitoring/promethues/#rules","title":"Rules","text":"<p>Supports two types of rules</p> <ul> <li>Recording</li> <li>Precompute frequently or computationally expressive expressions and save their result as new set of time series</li> <li>They are fast and useful for dashboards where the same metric is queried multiple time  </li> <li>Alerting</li> <li>Used to fire on alert when the conditions defined by the rule are met</li> </ul>"},{"location":"monitoring/promethues/#alertmanager","title":"Alertmanager","text":"<ul> <li>Notification triggered by metrics data</li> <li>amtool used to interact with Alertmanager</li> <li><code>--config.file</code> specify Alertmanager configuration</li> <li>High availability is possible</li> </ul>"},{"location":"monitoring/promethues/#grouping","title":"Grouping","text":"<ul> <li>Combine multiple alters into a single notification</li> <li>Useful when multiple system may fire an alert of the same nature simultaneously</li> </ul>"},{"location":"monitoring/promethues/#inhibition","title":"Inhibition","text":"<ul> <li>Allows to suppress an alert if another alert is already firing.</li> </ul>"},{"location":"monitoring/promethues/#silences","title":"Silences","text":"<ul> <li>Temporary way to turn off (mute) notifications (mute an alter for certain period of time).</li> </ul>"},{"location":"monitoring/promethues/#commands","title":"Commands","text":"<ul> <li>Reload config: <code>sudo killall -HUP prometheus</code></li> <li>Get config: <code>curl localhost:9090/api/v1/status/config</code></li> </ul>"},{"location":"monitoring/promethues/#query","title":"Query","text":"<ul> <li>Matching rules: allows combining multiple metrics</li> <li><code>ignoring(list&lt;labels&gt;)</code> ignore the labels that don't match</li> <li><code>on(list&lt;labels&gt;)</code> combine based on labels</li> <li>Aggregation operator: sum, avg, min, max, ...</li> <li>HTTP API: <code>localhost:9090/api/v1/query?query=up</code></li> </ul>"},{"location":"monitoring/promethues/#add-kubernetes-metrics","title":"Add Kubernetes metrics","text":"<pre><code>    git clone https://github.com/kubernetes/kube-state-metrics.git\n    cd kube-state-metrics/\n    git checkout v1.8.0\n    kubectl apply -f kubernetes\n\n    touch $(pwd)/kube-state-metrics-nodeport-svc.yml\n    echo &lt;&lt; EOF &gt;&gt; $(pwd)/kube-state-metrics-nodeport-svc.yml\n     kind: Service\n     apiVersion: v1\n     metadata:\n       namespace: kube-system\n       name: kube-state-nodeport\n     spec:\n       selector:\n         k8s-app: kube-state-metrics\n       ports:\n       - protocol: TCP\n         port: 8080\n         nodePort: 30000\n       type: NodePort\n     EOF    \n</code></pre>"},{"location":"network/dns/","title":"Domain Name System","text":"<ul> <li>Translate IP to domain names</li> <li> <p>Example:  </p> <ul> <li>www.hello.example.com.</li> <li>Root: is the dot at the end</li> <li>Top-Level Domain (TLD): com</li> <li>Domain: example</li> <li>Subdomain: hello (may not exists)</li> <li>Node: www</li> </ul> </li> <li> <p>Components</p> <ul> <li>DNS Recursor</li> </ul> </li> </ul>"},{"location":"nosql/mongodb/commads/","title":"Commads","text":""},{"location":"nosql/mongodb/commads/#basic-commands","title":"Basic commands","text":""},{"location":"nosql/mongodb/commads/#database","title":"Database","text":"<ul> <li>Create db: <code>use &lt;bd_name&gt;</code></li> <li>Show dbs: <code>show dbs</code></li> <li>Drop db:<ul> <li>Switch to the db</li> <li>Execute <code>db.dropDatabase()</code></li> </ul> </li> </ul>"},{"location":"nosql/mongodb/commads/#collection","title":"Collection","text":"<ul> <li>Create: <code>db.createCollection(name, options)</code></li> <li>Drop: <code>db.COLLECTION_NAME.drop()</code></li> <li>Ex:</li> </ul> <pre><code> db.createCollection(\"users\", { capped: true, size: 10000, max: 10000 })\n db.users.drop()\n</code></pre>"},{"location":"nosql/mongodb/commads/#document","title":"Document","text":""},{"location":"nosql/mongodb/commads/#crud","title":"CRUD","text":""},{"location":"nosql/mongodb/commads/#insert-data","title":"Insert Data","text":"<ul> <li>insert or save: <code>db.COLLECTION_NAME.insert(document)</code></li> </ul> <pre><code>db.users.insert({title: \"test title\",  likes: 100,  tags: ['one', 'two', 'three']})\n</code></pre> <ul> <li>insertOne and insertMany</li> </ul> <pre><code>db.users.insertOne({title: \"test title\",  likes: 100,  tags: ['one', 'two', 'three']})\n</code></pre> <pre><code>db.users.insertMany([{title: \"test title\",  likes: 100,  tags: ['one', 'two', 'three']}, { title: \"user 2\", tags: ['ok', 'nok']}])\n</code></pre> <ul> <li> <p>Find (like select all in SQL): <code>db.users.find()</code>, to format the result <code>db.users.find().pretty()</code></p> </li> <li> <p>Where clause: <code>db.users.find({\"likes\": {$lt:100}}).pretty()</code></p> <ul> <li>lt: less than, (lte: less than equal)</li> <li>gt: grater than, (gte: grater than equal)</li> <li>ne: not equal</li> <li>in: in values, (nin: not in)</li> </ul> </li> </ul>"},{"location":"nosql/mongodb/commads/#update-data","title":"Update Data","text":"<ul> <li> <p>Update</p> <ul> <li><code>mutli: true</code>: allows to update all the documents with title equals to <code>user 1</code> <code>db.users.update({'title': 'user 1'}, {$set: {\"title\": \"user_1\"}}, {multi: true})</code></li> </ul> </li> <li> <p>save</p> </li> <li> <p>findAndUpdate</p> </li> <li> <p>updateOne and updateMany</p> </li> </ul>"},{"location":"nosql/mongodb/commads/#aggregation","title":"Aggregation","text":""},{"location":"nosql/mongodb/concepts/","title":"Mongodb","text":"<ul> <li>Oriented documents</li> <li>Database: contains a set of Collections</li> <li>Collection: is like a table in RDMS it contains a set of Documents</li> <li>Document: is similar to row in RDMS</li> <li>Uses two types of models<ul> <li>Embedded (denormalized)</li> <li>Normalized</li> </ul> </li> </ul>"},{"location":"nosql/mongodb/concepts/#rdbms-vs-mongodb","title":"RDBMS vs MongoDB","text":"<ul> <li>In RDBMS the data dictates the app usage.</li> <li>The steps to are:</li> <li>Define the schema of the data</li> <li> <p>Develop the App and queries</p> </li> <li> <p>In MongoDB the application needs decide how data will be modeled</p> </li> <li>The steps are:</li> <li>Develop App</li> <li>Define the data model</li> <li>Improve the app</li> <li>Approve the data model</li> <li>The improvement of the app and data model is done without any downtime  </li> </ul>"},{"location":"nosql/mongodb/concepts/#how-to-model-data-in-mongodb","title":"How to model data in MongoDB ?","text":""},{"location":"nosql/mongodb/concepts/#step-by-step-iteration","title":"Step-by-Step iteration","text":"<ol> <li>Evaluate app workload<ul> <li>Business domain expertise</li> <li>Current and predicted scenarios</li> <li>Production logs and stats</li> </ul> </li> <li>It'll allow to have<ul> <li>Data size</li> <li>List of operations ranked by importance</li> </ul> </li> <li>Map out entities and their relationships</li> <li>It'll allow to have<ul> <li>CRD: Collection Relation Diagram (Embedded or Link)</li> </ul> </li> <li>To link or embed ?<ul> <li>How often does the embedded information get accessed ?</li> <li>Is the data queried using the embedded information</li> <li>Does the embedded information change often</li> </ul> </li> <li>Finalize the data model for each collection</li> <li>Allows to: <ul> <li>Identify and apply relevant design patterns</li> </ul> </li> </ol>"},{"location":"nosql/mongodb/concepts/#patterns","title":"Patterns","text":"<ul> <li>Schema versioning pattern<ul> <li>Add the field schema_version to document</li> <li>Run a batch updater to update the data</li> </ul> </li> <li>Bucket pattern</li> <li>Compute pattern<ul> <li>Never recompute what you can precompute </li> <li>Reads are often more common than writes</li> <li>Compute on write is less work than compute on read</li> <li>When updating the database, update some summary records too</li> <li>Can be thought of as a caching pattern</li> </ul> </li> </ul>"},{"location":"programming/clean-architecture/","title":"Clean architecture","text":""},{"location":"programming/clean-architecture/#command-and-queries","title":"Command and Queries","text":"<ul> <li>Command</li> <li>Does something </li> <li>Should modify the state</li> <li> <p>Should not return a value </p> </li> <li> <p>Query</p> </li> <li>Answer a question </li> <li>Should not modify a state </li> <li>Should return a value</li> </ul>"},{"location":"programming/clean-code/","title":"Naming","text":"<ul> <li>Variables<ul> <li>Avoid disinformation (meaningfully names)</li> <li>Use Pronounceable Names</li> </ul> </li> </ul>"},{"location":"programming/clean-code/#error-handling","title":"Error handling","text":"<ul> <li>Use exception rather than return codes<ul> <li>It's easy to forget to check exception, to avoid that throw the exception</li> </ul> </li> <li>Catch relevant exceptions</li> <li>Provide context to the exception<ul> <li>Create informative error messages and pass them along with your exceptions</li> <li>Mention the operation that failed and the type of the failure</li> </ul> </li> <li>Define you custom exceptions</li> <li> <p>Wrap the third-party API is a best practice best because you minimize your dependencies upon it </p> <ul> <li>You can choose to move to different library without much penalty</li> <li>Makes it easier to mock </li> <li>Aren't tied to a particular vendor API design choices</li> </ul> </li> <li> <p>Don't return null</p> </li> <li>Don't pass null</li> </ul>"},{"location":"programming/clean-code/#unit-tests","title":"Unit Tests","text":""},{"location":"programming/code-clean/","title":"Code clean","text":""},{"location":"programming/code-clean/#linter","title":"Linter","text":"<p>Inspects your code and five you feedback about it.</p> <ul> <li>Flake8<ul> <li>Errors<ul> <li>E/W: pep8 errors and warnings</li> <li>F***: PyFlakes codes (see below)</li> <li>C9**: McCabe complexity plugin mccabe</li> <li>N8**: Naming Conventions plugin pep8-naming</li> </ul> </li> <li>Configs<ul> <li>See the link</li> </ul> </li> <li>The configs add defined in<ul> <li><code>.flake8</code> file in root folder</li> <li><code>setup.cfg</code> under the section <code>[flake8]</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"programming/code-smell/","title":"Code smell","text":"<ul> <li>Code duplication</li> <li>Feature envy<ul> <li>Method makes multiple calls to methods and attributes of another class, which means that the method isn't in  the right class</li> <li>Solution<ul> <li>Move the method, or some part of the method to the right class </li> </ul> </li> </ul> </li> <li> <p>God class (Winnebago)</p> <ul> <li>Class with a lot of attributes and dependencies to other classes. It's du to the fact that we're able    to design a correct architecture for the code</li> <li>Solutions<ul> <li>Divide and rule</li> <li>Use abstract classes</li> </ul> </li> </ul> </li> <li> <p>Large class</p> </li> <li> <p>Long method</p> </li> <li> <p>Long parameter list</p> <ul> <li>Solutions<ul> <li>Remove parameters that resulted from the call to another objet and use the objet as parameter</li> <li>Group the parameters which could form a separate method </li> </ul> </li> </ul> </li> <li> <p>cyclomatic complexity</p> </li> </ul>"},{"location":"programming/design-patterns/","title":"Design Patterns","text":""},{"location":"programming/design-patterns/#creational","title":"Creational","text":"<p>Used to create object</p> <ol> <li>Singleton </li> <li>Factory </li> <li>Abstract Factory </li> <li>Builder:</li> <li>Create a complex object from a simple one </li> <li>When some attributes are not required</li> <li>Prototype</li> </ol>"},{"location":"programming/design-patterns/#structural","title":"Structural","text":"<p>Relationship between objects</p>"},{"location":"programming/design-patterns/#behavior","title":"Behavior","text":"<p>Communication between objects</p>"},{"location":"programming/design-patterns/#combinator","title":"Combinator","text":"<ul> <li>Combinator pattern allows chaining functions on an object in order to perform some validations</li> <li> <p>Synchronization: </p> <ul> <li>Allows resolving concurrency problem </li> <li>Deadlock: when concurrency is not managed </li> </ul> </li> <li> <p>Data Access Object (DAO)</p> </li> <li>Separate business layer from persistence layer</li> <li>Permits both layer to evolve separately without knowing anything about each other</li> <li> <p>Data Transfer Object (DTO)</p> </li> <li> <p>Data Change Capture</p> </li> </ul>"},{"location":"programming/design-patterns/#archi","title":"Archi","text":"<ul> <li>Data Driven Architecture</li> </ul>"},{"location":"programming/functional/","title":"Functional Programming","text":"<ul> <li>Pure functions<ul> <li>Given the same inputs, always return the same output</li> <li>Perform no observable side effects</li> </ul> </li> </ul> <p>Info</p> <p>What is a side effect? A side effect is any observable interaction other than returning a value. - Examples: reading/writing mutable state, I/O (files, network, console/logging), reading the clock or random numbers, throwing exceptions, process exit. - Why it matters: side effects break referential transparency and make code harder to reason about and test. - FP approach: keep a pure core and isolate effects at the edges, often by modeling them as values (e.g., IO/Task/Future, algebraic effects) and composing them explicitly.</p> <ul> <li> <p>Immutability</p> <ul> <li>Data isn\u2019t modified in place; new values are created instead</li> </ul> </li> <li> <p>No shared mutable state</p> <ul> <li>Avoid hidden coupling; simplifies reasoning and concurrency</li> </ul> </li> <li> <p>Higher-order functions</p> <ul> <li>Take functions as arguments and/or return functions</li> </ul> </li> <li> <p>Referential transparency</p> <ul> <li>Any expression can be replaced by its value without changing behavior</li> </ul> </li> </ul>"},{"location":"programming/java/","title":"Java","text":""},{"location":"programming/java/#garbage-collector","title":"Garbage Collector","text":"<ul> <li>Free the heap by destroying the objects that don't contain a reference</li> <li>Steps<ul> <li>Mark objects</li> <li>Sweep dead objects </li> <li>Compact memory</li> </ul> </li> </ul>"},{"location":"programming/java/#young-generation","title":"Young generation","text":"<ul> <li><code>-Xmn:</code> Young generation size</li> <li><code>-Xms:</code> initial size of the heap </li> <li><code>-Xmx:</code> maximum size of the heap</li> </ul>"},{"location":"programming/java/#permanent-generation","title":"Permanent generation","text":"<ul> <li><code>XX:PermGen</code> and <code>-XX:MaxPermGen</code> initial and maximum size of permanent generation</li> </ul>"},{"location":"programming/java/#gc-types","title":"GC types","text":"<ul> <li>Serial GC<ul> <li>Useful for small app running in single thread</li> <li>The garbage collection occurs in one thread, and the compaction is performed after the  collection if finished</li> </ul> </li> </ul>"},{"location":"programming/java/#static-class","title":"Static class","text":""},{"location":"programming/java/#jvm","title":"JVM","text":"<ul> <li>Class loader<ul> <li>Loading</li> <li>Linking<ul> <li>Verify</li> <li>Prepare</li> <li>Resolve </li> </ul> </li> <li>Initialization</li> </ul> </li> <li>Runtime data area</li> <li>Execution engine: convert byte code to machine code<ul> <li>Interpreter</li> <li>JIT compiler: complies bytes code to native codes. Native code will be used for repetead methods calls</li> <li>Profiler</li> <li>CG </li> </ul> </li> </ul>"},{"location":"programming/multi-threading/","title":"Multi-threading","text":"<ul> <li>Deux sortes de thread<ul> <li>Lourd (processus): lanc\u00e9 par le maain</li> <li>L\u00e8gres::lanc\u00e9s en parall\u00e8less, on retrouve deux types<ul> <li>System thread: sont ceux lanc\u00e9s par la JVM (ex: GC)</li> <li>Defined thread: sont ceux d\u00e9finis par le d\u00e9veloppeur, sont g\u00e9r\u00e8 par ExecutorService in java</li> </ul> </li> </ul> </li> <li>volatile: <ul> <li>Do not cache the variable</li> <li>Always read it from the main memory</li> </ul> </li> </ul>"},{"location":"programming/python/","title":"Python","text":"<ul> <li>class method vs static method</li> <li>class method:<ul> <li>access class state</li> <li>used as factory method</li> </ul> </li> <li> <p>static method</p> <ul> <li>can't access class state</li> <li>used as utility class</li> </ul> </li> <li> <p>Scope</p> </li> <li>LEGB</li> </ul>"},{"location":"programming/scala/","title":"Scala","text":""},{"location":"programming/scala/#scala","title":"Scala","text":"<ul> <li> <p>case class: </p> <ul> <li>Implement immutable data </li> <li>Useful for Pattern match </li> <li>No need to use new to create an object (implement apply method)</li> <li>Methods: toString, equal (shallow copy)</li> </ul> </li> <li> <p>object</p> </li> <li>Class that have only one object (singleton)</li> <li> <p>It's created lazy</p> </li> <li> <p>case object</p> <ul> <li>It's serializable</li> <li>Has a default hashCode implementation</li> <li>Has an improved implementation of toString</li> <li>Can access private elements of its companion class</li> </ul> </li> <li> <p>High order function: </p> <ul> <li>Function that takes other function as parameter </li> <li>Return other function </li> </ul> </li> <li> <p>Try/ Catch </p> <ul> <li>Traditional try/catch allows to catch all suspected block using case statement</li> </ul> </li> <li> <p>Curring</p> </li> <li> <p>Abstract class vs Trait</p> <ul> <li>Abstract: to be used when<ul> <li>We require consustructor argument</li> <li>Code will be called by java</li> <li>class Extends only one abstract class</li> </ul> </li> </ul> </li> </ul>"},{"location":"programming/scala/#sbt","title":"Sbt","text":""},{"location":"programming/scala/#commands","title":"Commands","text":"<ul> <li>clean, compile, package, update, test,  inspect,... </li> </ul>"},{"location":"programming/scala/#test","title":"Test","text":""},{"location":"programming/scala/#scalatest","title":"ScalaTest","text":""},{"location":"programming/architectural-design/","title":"Home","text":"<p>What it is:</p> <ul> <li>The high-level structure of a software system.</li> <li>Defines how components interact, how the application is deployed, and how data flows.</li> <li>Focuses on scalability, performance, modularity, and maintainability.</li> </ul> <p>\ud83d\udccc Examples:</p> <ul> <li>Monolithic vs Microservices</li> <li>Layered architecture (Presentation \u2192 Business \u2192 Data)</li> <li>Event-driven architecture</li> <li>Hexagonal (Ports and Adapters), Onion, Clean Architecture</li> <li>Use of components: databases, message queues, APIs, frontends</li> </ul>"},{"location":"programming/architectural-design/dao/","title":"DAO","text":"<p>The DAO \u2013 Data Access Object pattern encapsulates all the logic for accessing data sources (like databases or APIs). It abstracts and hides the details of data access from the rest of the application.</p> <p>Tip</p> <p>\ud83e\udde0 Think of it as: A service class that talks to the database.</p> <ul> <li>Example</li> </ul> <pre><code>// Model class\ncase class User(id: Int, name: String)\n\n// DAO interface\ntrait UserDAO {\n  def findById(id: Int): Option[User]\n  def save(user: User): Unit\n}\n\n// DAO implementation (e.g., using JDBC or an ORM)\nclass UserDAOImpl extends UserDAO {\n  override def findById(id: Int): Option[User] = {\n    // Database query logic here\n    Some(User(id, \"Alice\"))\n  }\n\n  override def save(user: User): Unit = {\n    // Insert or update logic here\n    println(s\"Saved user: $user\")\n  }\n}\n</code></pre>"},{"location":"programming/architectural-design/dto/","title":"DTO","text":"<p>A DTO \u2013 Data Transfer Object is a plain object used to transfer data between layers or systems, especially between:</p> <ul> <li>Client and server</li> <li>API and service</li> <li>Service and DAO</li> </ul> <p>It usually contains no logic, only fields (and maybe some formatting/validation).</p> <p>Tip</p> <p>\ud83e\udde0 Think of it as: A suitcase full of data that travels across layers.</p>"},{"location":"programming/architectural-design/dto/#example","title":"Example","text":"<pre><code>// DTO representing input from a web form or JSON\ncase class UserDTO(name: String)\n\n// Service that converts DTO into a domain model\nclass UserService(userDAO: UserDAO) {\n  def createUser(dto: UserDTO): Unit = {\n    val user = User(0, dto.name) // Convert DTO to domain model\n    userDAO.save(user)\n  }\n}\n</code></pre>"},{"location":"programming/principals/dry/","title":"DRY (Don't Repeat Yourself)","text":"<p>Don't Repeat Yourself is a fundamental software engineering principle that encourages avoiding code  duplication. The idea is:</p> <p>\"Every piece of knowledge should have a single, unambiguous, authoritative representation within a system.\" \u2014 The Pragmatic Programmer</p> <p>DRY \u2260 Over-Abstraction</p> <p>Be careful: too much DRY can lead to over-engineering (creating generic code that's hard to understand). Strike a balance.</p>"},{"location":"programming/principals/dry/#why-dry-matters","title":"Why DRY Matters","text":"<ul> <li>Less duplication means fewer bugs</li> <li>Easier to update/change (change in one place not in multiple places)</li> <li>Improves readability and maintainability</li> </ul>"},{"location":"programming/principals/dry/#violating-dry-bad-practice","title":"Violating DRY (bad practice)","text":"<pre><code>println(\"Connecting to database...\")\n// ... more code\nprintln(\"Connecting to database...\") // Repeated logic\n</code></pre>"},{"location":"programming/principals/dry/#applying-dry-good-practice","title":"Applying DRY (good practice)","text":"<pre><code>def connectMessage(): Unit = println(\"Connecting to database...\")\n\nconnectMessage()\n// ... more code\nconnectMessage()\n</code></pre>"},{"location":"programming/principals/kiss/","title":"KISS (Keep It Simple, Stupid)","text":""},{"location":"programming/principals/kiss/#whats-kiss","title":"What's KISS","text":"<p>KISS stands for Keep It Simple and Stupid \u2014 a key design principle in software engineering (and many other fields). It promotes simplicity over unnecessary complexity.</p>"},{"location":"programming/principals/kiss/#the-core-idea","title":"The Core Idea","text":"<p>Tip</p> <p>The simplest solution is often the best one.</p> <ul> <li>Don't overengineer.</li> <li>Don\u2019t add things you might need later.</li> <li>Aim for clarity, not cleverness.</li> </ul>"},{"location":"programming/principals/kiss/#example-scala","title":"Example (Scala)","text":"<ul> <li>Without KISS \u2013 Overengineered</li> </ul> <pre><code>def add(a: Int, b: Int): Int = {\n  val result = Some(a + b)\n  result.getOrElse(throw new RuntimeException(\"Unexpected error\"))\n}\n</code></pre> <ul> <li>With KISS \u2013 Simple and clear</li> </ul> <pre><code>def add(a: Int, b: Int): Int = a + b\n</code></pre>"},{"location":"programming/principals/kiss/#common-violations-of-kiss","title":"Common Violations of KISS","text":"<ul> <li>Adding design patterns where they're not needed</li> <li>Overuse of abstractions or indirection</li> <li>Overengineering for features that don't exist yet</li> </ul>"},{"location":"programming/principals/kiss/#summary","title":"Summary","text":"<p>KISS reminds us to avoid complexity for its own sake. In practice, it means writing clean, direct code that does the job \u2014 and nothing more.</p>"},{"location":"programming/principals/solid/","title":"SOLID","text":"<p>Warning</p> <ul> <li>When not applied</li> <li>Code fragility: change in one module may create bugs in other modules</li> <li>Code rigidity: change in one place will imply change in other places</li> </ul>"},{"location":"programming/principals/solid/#single-responsibility-principal-srp","title":"Single Responsibility Principal (SRP)","text":"<ul> <li>Every method, class or module should have only one and only one reason to change</li> <li>Pros<ul> <li>Class less coupled and resilient for change </li> <li>Make code more testable</li> <li>Code easy to understand, fix and maintain </li> </ul> </li> <li>Identify the reason to change<ul> <li>if and else statement</li> <li>switch cases </li> <li>monster methods (long methods): identify the small responsibilities and put them in separates methods</li> <li>God classes: classes with multiple responsibilities</li> </ul> </li> </ul>"},{"location":"programming/principals/solid/#open-closed-principal-ocp","title":"Open Closed Principal (OCP)","text":"<ul> <li>Closed for modification and Open for extension</li> <li> <p>Pros:</p> <ul> <li>New feature can be added easily with minimal cost </li> <li>Minimize the risk of creating regression bugs </li> <li>Enforce the decoupling by isolating the changes in one specific component (Respects the SRP)</li> </ul> </li> <li> <p>Who to do</p> <ul> <li>Inheritance (small drawback: create coupling between base class and derived class)</li> <li>Strategy pattern </li> </ul> </li> </ul>"},{"location":"programming/principals/solid/#liskov-substitution-principal-lsp","title":"Liskov Substitution Principal (LSP)","text":"<ul> <li>If S is subtype of T, then the objects of the type T in the program may be replaced by objects of type S without modifying the functionality of the program.   </li> <li>Every time when creating a relationship between object as the question Is substitutable by</li> <li>Example: Base class Bird, subclass Ostrich but an Ostrich can't fly </li> <li>Pros<ul> <li>Eliminates incorrect relationship between objects </li> <li>Use Tell, don't ask! principal to eliminate type checking and casting</li> </ul> </li> <li>How to apply<ul> <li>Make sure that a derived type can substitute its base type completely</li> <li>Keep base class small and focus </li> <li>Keep interfaces lean </li> </ul> </li> </ul>"},{"location":"programming/principals/solid/#interface-segregation-principal-isp","title":"Interface Segregation Principal (ISP)","text":"<ul> <li>Client shouldn't be forced to depend on the methods they do not use</li> <li>How to identify flat interface<ul> <li>Interface with multiple methods</li> <li>Interface with Low Cohesion (method that doesn't fit with the purpose of the interface)</li> <li>Client that throws exception instead implementing method </li> <li>Client provides empty implementation </li> <li>Client forces implementation and becomes highly coupled</li> </ul> </li> </ul>"},{"location":"programming/principals/solid/#dependency-inversion-principal-dip","title":"Dependency Inversion Principal (DIP)","text":"<ul> <li>High level module should not depend on low level model, both should depend on abstraction </li> <li>Dependency Inversion, Dependency injection and Inversion of Control are combined to perform DIP </li> </ul>"},{"location":"programming/principals/solid/#other-best-practices","title":"Other best practices","text":"<p>Info</p> <ul> <li>Constant refactoring </li> <li>Design patterns </li> <li>Unit testing (TDD, BDD) </li> </ul>"},{"location":"programming/principals/yagni/","title":"YAGNI","text":""},{"location":"programming/principals/yagni/#what-is-yagni","title":"What Is YAGNI?","text":"<p>YAGNI stands for You Aren\u2019t Gonna Need It \u2014 a key principle from Agile and Extreme Programming (XP) that advises:</p> <pre><code>Don\u2019t implement something until it\u2019s actually needed.\n</code></pre>"},{"location":"programming/principals/yagni/#core-idea","title":"Core Idea","text":"<ul> <li>Don\u2019t build features, methods, or abstractions just in case.</li> <li>Only write code when there\u2019s a real, immediate need.</li> </ul>"},{"location":"programming/principals/yagni/#example","title":"Example","text":"<ul> <li>Violating YAGNI</li> </ul> <pre><code>def send_notification(user: User, method: int=\"email\", retries: int=3) -&gt; None:\n    if method == \"email\":\n        print(f\"Sending email to {user}\")\n    elif method == \"sms\":\n        print(f\"Sending SMS to {user}\")\n    # In reality, only 'email' is ever used in the app\n</code></pre> <p>You spent time building support for sms and retries, but you don\u2019t use them. This is extra logic to maintain, test, and potentially fix in the future \u2014 wasted effort.</p> <ul> <li>YAGNI Applied</li> </ul> <p><pre><code>def send_email(user: User):\n    print(f\"Sending email to {user}\")\n</code></pre> You can always refactor later if you do need SMS or retries. But until then, this is simpler, easier to test, and easier to maintain.</p>"},{"location":"programming/principals/yagni/#common-violations","title":"Common Violations","text":"<ul> <li>Writing code for features you might need later</li> <li>Adding configuration or flags for future use</li> <li>Building complex inheritance or plugin systems early</li> </ul>"},{"location":"programming/principals/yagni/#summary","title":"Summary","text":"<p>YAGNI = Don\u2019t do extra work for hypothetical needs. Build what you need now, not what you think you might need.</p>"},{"location":"programming/python-doc/scraping/","title":"Scraping","text":"<ul> <li>Scraping is an automated browsing </li> <li>Steps</li> <li>set url </li> <li>download html </li> <li>parse the html </li> <li>extract useful information </li> <li>transform or aggregate the data </li> <li> <p>save the data  </p> </li> <li> <p>CSS selectors and XPath</p> </li> <li> <p>Libraries </p> </li> <li>requests </li> <li>Beautiful soup </li> <li>scrapy <ul> <li>scrapy shell</li> <li>cli </li> <li>... </li> </ul> </li> <li>Selenium and Request html for pages genetated dynamically</li> </ul>"},{"location":"security/Oauth2/","title":"Oauth2","text":"<ul> <li>Authorization framework</li> <li>Two endpoints</li> <li>Authorization: handles interactions via user agent (browser)</li> <li>Token: meant for machines only</li> <li>Scope: a permission to do something within an API</li> <li>Grant Type</li> <li>Code</li> <li>Implicit</li> <li>...</li> <li>Response type</li> <li>Token</li> <li>Token type</li> <li>Expiration (expires_in)</li> <li>Scopes</li> </ul>"},{"location":"security/common-protocols/","title":"Common protocols","text":""},{"location":"security/common-protocols/#ssh","title":"SSH","text":""},{"location":"security/common-protocols/#ftp-sftp","title":"FTP (SFTP)","text":""},{"location":"security/common-protocols/#_1","title":"Common protocols","text":""},{"location":"security/concepts/","title":"Concepts","text":"<ul> <li>Threats</li> <li>Internal</li> <li>External</li> <li>Vulnerability: Is some kind of weakness</li> <li>Exploit: a way of taking advantage of a vulnerability</li> </ul>"},{"location":"security/concepts/#reduce-threat-exposure","title":"Reduce threat exposure","text":"<ul> <li>Zero Trust</li> <li>User / Admin access<ul> <li>Role Based Access</li> <li>Only have access to required system</li> <li>Least Privileges</li> <li>Allow as little access as required</li> <li>Applies to system process too</li> <li>Separation of Duties</li> <li>Processes require more than a single person</li> </ul> </li> <li>Network Access Control<ul> <li>Authenticate user</li> <li>Authenticate device</li> <li>Scan device</li> <li>Provide least privilege access</li> <li>Provide access based on role</li> </ul> </li> <li>Network Segmentation</li> <li>Honeypots</li> </ul>"},{"location":"security/concepts/#authentication-authorization-and-accounting-aaa","title":"Authentication, Authorization and Accounting (AAA)","text":"<ul> <li>Auth:</li> <li>Local auth</li> <li>Domain auth</li> <li> <p>Single Sing On (SSO)</p> </li> <li> <p>Tools</p> </li> <li>I Been Pwned</li> </ul>"},{"location":"security/encry-token/","title":"Encry token","text":""},{"location":"security/encry-token/#encryption","title":"Encryption","text":"<ul> <li>Data Encryption Standard (DES)</li> <li>Advanced Encryption Standard (AES)</li> <li>Cons</li> <li>Keys</li> <li>Unchanged algos</li> <li>Databases must be able to read specific data type and it length</li> </ul>"},{"location":"security/encry-token/#tokenization","title":"Tokenization","text":"<ul> <li>vault-based tokenization</li> <li> <p>Property Vaultless Tokenization (PVT)</p> </li> <li> <p>PCI Compliance ??</p> </li> </ul>"},{"location":"security/cybersecurity/01-five-pricipales/","title":"Principles","text":"<ol> <li>Defense in depth</li> <li>No SPOF</li> <li>Fail safe</li> <li>Least priviligeses</li> <li>Only</li> <li>Harden</li> <li>Privileges Creep</li> <li>Just in case</li> <li>Separation of Duties: we wont have any single point of control</li> <li>No Single point of control</li> <li>Collusion: the requester can't be an approver</li> <li>Secure by a design</li> <li>It should be an AFTERethought that we put security in</li> <li>Start to Finish</li> <li>Secure out of the boxe (OOTB)</li> <li>KISS</li> <li>Keep it simple and ...</li> </ol>"},{"location":"security/cybersecurity/02-cia/","title":"02 cia","text":"<ul> <li>CIA</li> <li>Confidentiality</li> <li>Integrity</li> <li>Availability</li> </ul>"},{"location":"security/cybersecurity/04-iam/","title":"IAM","text":"<ul> <li>Admin</li> <li>Authentication</li> <li>Who are you ? </li> <li>It uses<ul> <li>Know: based on something you know (password)</li> <li>Have: something you have </li> <li>Are: for instance if the device belong to you</li> <li>MFA (Multi-Factor Authentication): combines Have + Are</li> </ul> </li> <li>Authorization</li> <li>Use PAM with high privileges uses (sysroot, dba, ...)  </li> <li>Audit</li> <li>Abnormal actions should be detected (making transaction in a new way, updating password, ...) </li> <li>Tools to use<ul> <li>User Behavior Analytics</li> <li>User Entity Analytics</li> </ul> </li> </ul>"},{"location":"security/kerberos/concepts/","title":"Kerberos","text":"<ul> <li>It's a client/server based architecture</li> </ul> <ul> <li>Principal: an identity that needs to be verified</li> <li>User Principal Names (UPN): similar to usernames in operating systems.</li> <li> <p>Service Principal Names (SPN): is the service that the user needs to access (database, server, ...).</p> </li> <li> <p>Realm:</p> </li> <li>A realm in Kerberos refers to an authentication administrative domain.</li> <li> <p>Principals are assigned to specific realms in order to establish boundaries and simplify administration.</p> </li> <li> <p>Key Distribution Center (KDC): contains all information about principals and realm. It consists of:</p> </li> <li>Kerberos database: it stores  <ul> <li>UPN and SPN</li> <li>To which realm principal belongs to</li> <li>Encryption keys</li> <li>Tickets validation duration</li> <li>Expiration date</li> <li>...</li> </ul> </li> <li>Authentication Server (AS):<ul> <li>Delivers TGT (Ticket Granting Ticket)</li> <li>Authenticates users</li> <li>TGT is delivered if authentication is successful</li> </ul> </li> <li>Ticket Granting Server (TGS):<ul> <li>Validates TGT</li> <li>Delivers TS (Ticket Service)</li> </ul> </li> <li>KeyTab: file that contains all keys related to specific service</li> </ul>"},{"location":"security/kerberos/concepts/#principal-parts","title":"Principal parts","text":"<ul> <li>Primary</li> <li><code>&lt;shortname&gt;@&lt;REALM&gt;</code></li> <li>Example: <code>bob@EXAMPLE.COM</code> =&gt; bob belongs to realm EXAMPLE.COM</li> <li>Instance</li> <li><code>&lt;shortname&gt;/&lt;instance&gt;@&lt;REALM&gt;</code></li> <li>Example: <code>username/admin@EXAMPLE.COM</code></li> <li>Service</li> <li><code>&lt;shortname&gt;/&lt;hostname&gt;@&lt;REALM&gt;</code></li> <li>Example: <code>hdfs/node1.domain.com@EXAMPLE.COM</code> =&gt; service <code>hdfs</code> in the node <code>node1.domain.com</code></li> </ul> <p>PS: naming is case-sensitive</p>"},{"location":"security/kerberos/concepts/#trust","title":"Trust","text":"<ul> <li>One way</li> <li>bidirectional trust or full trust</li> </ul>"},{"location":"security/kerberos/concepts/#advantages","title":"Advantages","text":"<ul> <li>Is Single sign on</li> <li>Password do not travel in clear over the network</li> <li>A centralized repository for all users and services credentials</li> </ul>"},{"location":"security/kerberos/concepts/#example-access-hdfs","title":"Example: Access hdfs","text":"<ol> <li>Authenticate using kinit service</li> <li>Sends authentication to Authentication Server</li> <li>If OK: AS sends TGT to user and kinit will store TGT in a credential cache and user is Authenticated</li> <li>Now user wants to run command <code>hdfs dfs -ls</code><ol> <li>Hadoop will use TGT and reach Ticket Granting Server</li> <li>TGS will grant TS (Ticket Service) and client will cache TS</li> </ol> </li> <li>Hadoop RPC will use TS to reach the Namenode</li> <li>Client and Namenode exchange Tickets (Client ticket prove client identity and Namenode determines the identification of Namenode)</li> </ol>"},{"location":"security/kerberos/concepts/#kerberos-on-hadoop","title":"Kerberos on Hadoop","text":"<ol> <li>Create KDC</li> <li>Create service principal for each service (HDFS, Yarn, ...)</li> <li>Create Encrypted Kerberos Keys (Keytabs) for each service</li> <li>Distribute keytab for service principals to each service on the cluster nodes</li> <li>Configure all services (HDFS, Yarn, Hive, ...) to rely on Kerberos</li> </ol>"},{"location":"security/kerberos/concepts/#installation","title":"Installation","text":""},{"location":"security/kerberos/concepts/#server-side","title":"Server Side","text":"<p>1. Install Kerberos Admin Server: install the server and KDC</p> <p><code>yum install krb5-workstation krb5-libs krb5-server</code></p> <p>2. Configure Kerberos</p> <ul> <li>Two configuration files</li> <li><code>/etc/krb5.conf</code></li> <li> <p><code>/var/kerberos/krb5kdc/kdc.conf</code></p> </li> <li> <p>Configure the realm</p> </li> </ul> <p><code>sudo vi /etc/krb5.conf</code></p> <pre><code>[realms]\n # realm name\n HADOOP.COM = {  \n  # the name of KDC\n  kdc = server.hostname.com\n  # The admin Server\n  admin_server = server.hostname.com \n }\n</code></pre> <ul> <li>Configure KDC</li> </ul> <p><code>vi /var/kerberos/krb5kdc/kdc.conf</code></p> <p>4. Create KDC database</p> <pre><code>    kdb5_util create -r HADOOP.COM -s\n</code></pre> <p>5. Specify the admin principals and add admin principal</p> <ul> <li>All users in realm <code>*/admin@HDPCLUSTER.COM</code> will have admin access</li> <li><code>kadmin.local</code> utility used only on the krb server</li> </ul> <pre><code>vi /var/kerberos/krb5kdc/kadm5.acl\nkadmin.local -q \"addprinc root/admin\"\n</code></pre> <p>6. Start kdc and the server</p> <pre><code>service krb5kdc start\nservice kadmin start\n</code></pre> <p>7. Test</p> <pre><code># list if ticket is present in cache\nklist\n\n# get ticket\nkinit username\n\n# add principal\nkadmin\naddprinc username@REALM.COM\n</code></pre>"},{"location":"security/kerberos/concepts/#client-side","title":"Client Side","text":"<p>1.Install</p> <p><code>yum install krb5-workstation krb5-libs krb5-auth-dialog</code></p>"},{"location":"security/kerberos/concepts/#useful-links","title":"Useful links","text":"<ul> <li>https://examples.javacodegeeks.com/enterprise-java/apache-hadoop/hadoop-kerberos-authentication-tutorial</li> <li>https://www.oreilly.com/library/view/hadoop-security/9781491900970/ch04.html</li> </ul>"},{"location":"security/ldap/concept/","title":"LDAP","text":"<ul> <li>The data is stored in tree</li> <li> <p>Stores data that won't be changed many times</p> </li> <li> <p>By default:</p> </li> <li>LDAP uses the port 389</li> <li> <p>LDAPS (over TLS/SSL) uses the port 636</p> </li> <li> <p><code>dc:</code> Domain Component</p> </li> <li><code>ou:</code> Organization Units</li> <li><code>cn:</code> Common Name</li> <li><code>sn:</code> Surname Name</li> <li><code>dn</code> (Distinguished Name): allows unique identification by entry</li> </ul> <p>Example <code>dc=toto,dc=fr</code></p>"},{"location":"security/ssl/ssl/","title":"SSL/TLS","text":"<ul> <li>SSL (Secured Sockets Layer) replaced by TLS (Transport Layer Security)</li> <li>Protocol used to communicate over the internet with HTTPS</li> <li>Provides:</li> <li>Authentication</li> <li> <p>Encryption</p> </li> <li> <p>Crypto sym (probl\u00e8me c'est dans l'\u00e9change des cl\u00e9s)</p> </li> <li> <p>private key</p> </li> <li> <p>Crypto asym (assure la confidentialit\u00e9 de l'\u00e9change)</p> </li> <li>private key</li> <li>public key</li> <li>Use certificate to counter Man in middle attack</li> </ul>"},{"location":"security/ssl/ssl/#public-key-infrastructure-pki","title":"Public Key Infrastructure (PKI)","text":"<p>Can prove validity of the public key owner</p> <ul> <li>CA (Certification Authority) </li> <li>CSR (Certificate Signing Request): </li> <li>Validate the identity of the certificate requester by the CA</li> <li>Digital Certificate: once the identity is validated a digital Certificate is received. It's: </li> <li>A file container for a public key</li> <li>Digital signature from a (trusted) CA</li> <li>Additional metadata/attributes</li> </ul>"},{"location":"security/ssl/ssl/#algo-de-chiffrement","title":"Algo de chiffrement","text":"<ul> <li>AES : Advanced Encryption Standard       Key size (128 to 256 bytes)</li> <li>RSA :<ul> <li>private key : pour d\u00e9chiffrer  </li> <li>public key pour chiffrer  </li> <li>key size (1024 to 4096)</li> </ul> </li> <li> <p>SHA : Secure Hash Algorithm </p> <ul> <li>Un algortime de hashage, il permet pas de chiffrer les donn\u00e9es, mais de cr\u00e9er une empreinte </li> </ul> </li> <li> <p>Cl\u00e9 de chiffrement</p> </li> <li> <p>TLS certificate: data file encrypted and contains </p> <ul> <li>Public key </li> <li>Domain name</li> <li>Hostname </li> <li>Server details tied to an organization </li> </ul> </li> <li> <p>To use SSL, an organization needs to install SSL/TLS certificate on the webserver </p> </li> </ul> <p>https://dzone.com/articles/ssl-in-java</p>"},{"location":"security/sso/concepts/","title":"Single-Sing-On (SSO)","text":""},{"location":"security/sso/concepts/#security-assertion-markup-language-saml","title":"Security Assertion Markup Language (SAML)","text":"<p>Use one single point to authenticate user to different applications It can also be used for authorization Compromised of:</p> <ul> <li>User: to be identified</li> <li>Identity provider</li> <li>Service provider: applications to which user want to access</li> </ul> <p>Only one authentication is needed and then the SAML will provide an assertion.</p>"},{"location":"security/sso/concepts/#openid","title":"openID","text":"<ul> <li>Uses JWT</li> </ul>"},{"location":"security/vault/dynamic-secret/","title":"Dynamic secret","text":"<p>To follow this tutorial, you need a running vault server on kubernetes. If not you can run the following helm commands  to setup Vault. </p> <p>is standard security practice to isolate secrets from code, and developers should not concern themselves with the  origin of these secrets. This is where HashiCorp Vault comes in to centralize those secrets. In case one or multiple of these secrets get compromised, you'll need to revoke them and generate new ones to mitigate risks with minimal impact on the systems utilizing them.</p> <p>In this article, we'll cover how you can achieve that using Vault and to secure a PostgreSQL database. We will end by demonstrating how we can use these secrets with Python.</p>"},{"location":"security/vault/dynamic-secret/#why-dynamic-secrets","title":"Why Dynamic Secrets","text":"<p>Here are some leaks that lead us to consider using Dynamic Secrets instead of Static Secrets.</p> <p>Applications frequently log configurations, leaving them in log files or centralized logging systems like Elasticsearch, which can be accessed by unauthorized individuals viewing your credentials.</p> <p>Often, secrets are captured in exception tracebacks while attempting to access the database, for instance. These crash reports are sent to external monitoring systems, or they may be leaked via debugging endpoints and  diagnostic pages after encountering an error.</p> <p>It's common within organizations for multiple applications to share the same credentials to access other systems, such as a database. This means that rotating those passwords will impact all applications using those credentials.  Now, imagine one of these applications getting compromised. It will require you to rotate these credentials across all your applications.</p> <p>All these challenges lead us to use a secret that can be generated and revoked on demand with less impact.</p>"},{"location":"security/vault/dynamic-secret/#whats-dynamic-secrets","title":"What's Dynamic Secrets","text":"<p>From Harshicop Vault website:</p> <p>A dynamic secret is generated on demand and is unique to a client, instead of a static secret, which is defined ahead  of time and shared. Vault associates each dynamic secret with a lease and automatically destroys the credentials when  the lease expires.</p>"},{"location":"security/vault/dynamic-secret/#postgresql","title":"Postgresql","text":"<p>In this section, we assume that you already have a PostgreSQL database server running. Alternatively, if you're  a Kubernetes and Helm user, you can set up a PostgreSQL database server by executing the commands below.</p> <pre><code>helm install postgres oci://registry-1.docker.io/bitnamicharts/postgresql\nkubectl get secret --namespace default postgres-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d\n</code></pre> <p>The PostgreSQL server that we are using throughout this tutorial can be accessed using the information below.  Please note that this information is temporary and local, and you cannot use it in your own environment.</p> <p>user: postgres port: 5432 password: FAKE_PWD host: postgres-postgresql</p>"},{"location":"security/vault/dynamic-secret/#create-database","title":"Create database","text":"<p>Connect to the database and create a database named dbtest. psql host=localhost port=5432 user=postgres password=SgOdV1ctSe CREATE DATABSE IF NOT EXISTS dbtest</p> <p>Above, we establish a connection to the database using the psql tool (alternatively, you can use any other  PostgreSQL client tool). Subsequently, we execute the create database command to set up our schema.</p>"},{"location":"security/vault/dynamic-secret/#vault","title":"Vault","text":"<p>Generating secrets dynamically for a PostgreSQL database can be done in two steps: configuring the plugin and creating a role.</p>"},{"location":"security/vault/dynamic-secret/#configure-the-plugin","title":"Configure the plugin","text":"<p>vault login</p> <p>export CONNECTION_URL=\"postgresql://{{username}}:{{password}}@postgres-postgresql:5432/dbtest\" vault write database/config/my-postgresql-database \\     plugin_name=\"postgresql-database-plugin\" \\     allowed_roles=\"my-role\" \\     connection_url= ${CONNECTION_URL} \\     username=\"postgres\" \\     password=\"FAKE_PWD\" \\     password_authentication=\"scram-sha-256\"</p>"},{"location":"security/vault/dynamic-secret/#create-a-role","title":"Create a role","text":"<p>Below, we create a role called test-role and then we mapped it to an SQL statement that will create a new database  user with select permision granted to all tables inside the public schema.  </p> <p>vault write database/roles/test-role \\     db_name=\"my-postgresql-database\" \\     creation_statements=\"CREATE ROLE \\\"{{name}}\\\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}'; \\         GRANT SELECT ON ALL TABLES IN SCHEMA public TO \\\"{{name}}\\\";\" \\     default_ttl=\"1h\" \\     max_ttl=\"24h\"</p> <p>vault read database/creds/my-role</p>"},{"location":"security/vault/dynamic-secret/#read-from-python","title":"Read from python","text":""},{"location":"security/web/headers/","title":"Headers","text":"<ul> <li>Threats</li> <li>Cross Site Scripting   </li> <li>Insufficient Transport Layer Security </li> <li> <p>Clickjacking</p> </li> <li> <p>Browser has to implement the header</p> </li> <li>It's a control client side </li> <li>If it doesn't recognize a header it ignores it</li> </ul>"},{"location":"security/web/https/","title":"Https","text":"<ul> <li>X.509 is a standard that defines the format of the digital certificate</li> <li>X.509 uses a formal language called Abstract Syntax Notation One (ASN.1) to express the certificate's data structure</li> </ul>"},{"location":"sql/views/","title":"Views","text":""},{"location":"sql/views/#view-vs-materialized-view","title":"View vs Materialized View","text":"<ul> <li> <p>View:</p> <ul> <li>Never stored</li> <li>Is a virtual copy of one or more base tables or views</li> <li>Updated each time base table is update</li> <li>Slow processing</li> <li>Do not require disk space</li> </ul> </li> <li> <p>Materialized View</p> <ul> <li>Is stored on the disk</li> <li>Is a copy of physical base table</li> <li>Has to be updated manually or using trigger</li> <li>Fast processing</li> <li>Requires disk space</li> </ul> </li> </ul>"},{"location":"sql/window/","title":"Window function","text":"<pre><code>create table employee (employee_id id, full_name varchar(100), departement varchar(100), salary float);  \n\ninsert into employee values(100,'Mary Johns', 'SALES',  1000.00); \ninsert into employee values(101,'Sean Moldy',   'IT', 1500.00);\ninsert into employee values(102,'Peter Dugan',  'SALES', 2000.00);\ninsert into employee values(103,'Lilian Penn',  'SALES', 1700.00);\ninsert into employee values(104,'Milton Kowarsky',  'IT', 1800.00);\ninsert into employee values(105,'Mareen Bisset', 'ACCOUNTS', 1200.00);\ninsert into employee values(106,'Airton Graue', 'ACCOUNTS', 1100.00);\ncommit;\n</code></pre>"},{"location":"sql/window/#1-ranking-window","title":"1. Ranking Window","text":"<ul> <li> <p>Common use is to find N top record based on some value</p> </li> <li> <p>RANK():</p> <ul> <li>Get order with a frame (partition) </li> <li>If two records have same rank (ex: rank = 3) the next rank will be 5 (rank 4 will be skipped)</li> </ul> </li> </ul> <pre><code>SELECT\n    RANK() OVER (PARTITION BY departement ORDER BY salary DESC) AS dept_ranking,\n    departement, employee_id, full_name, salary\nFROM employee; \n</code></pre> <ul> <li>DENSE_RANK():<ul> <li>Is identical as RANK() function except that it does not skip any rank</li> </ul> </li> </ul> <pre><code>SELECT\n    DENSE_RANK() OVER (PARTITION BY departement ORDER BY salary DESC) AS dept_ranking,\n    departement, employee_id, full_name, salary\nFROM employee;\n</code></pre> <ul> <li>ROW_NUMBER():<ul> <li>Assign the number to each record</li> <li>Can be used with or without PARTITION BY</li> </ul> </li> </ul> <pre><code>SELECT\n    ROW_NUMBER() OVER (ORDER BY salary DESC) AS dept_ranking,\n    departement, employee_id, full_name, salary\nFROM employee;\n</code></pre>"},{"location":"sql/window/#2-aggregate-window","title":"2. Aggregate Window","text":"<pre><code>-- SUM, AVG, MIN, MAX, COUNT\nSELECT\n    SUM(salary) OVER (PARTITION BY departement) AS total_salary,\n    departement, employee_id, full_name, salary\nFROM employee; \n</code></pre> <p>https://www.sqlshack.com/use-window-functions-sql-server/</p>"},{"location":"tests/bbd_tdd/","title":"Bbd tdd","text":""},{"location":"tests/bbd_tdd/#behavior-driven-development-test","title":"Behavior Driven Development Test","text":"<ul> <li>Based on three </li> <li>Context (Given): starting state</li> <li>Event (When): what user does </li> <li> <p>Outcomes (Then): expected results </p> </li> <li> <p>Can be written at any time </p> </li> <li>Before </li> <li>During </li> <li>After </li> </ul>"},{"location":"tests/bbd_tdd/#test-types","title":"Test types","text":"<ul> <li>Unit</li> <li>Doest interact with external word</li> <li>It helps to: <ul> <li>Understand the system that we're building </li> <li>Document the code</li> <li>Design the units </li> <li>Protect against regression</li> </ul> </li> <li>Integration:</li> <li>Focus integrating different layers of the application, that means no mocking is involved</li> <li>Ideally, they should be keep separated from unit tests and should not run along with unit tests  </li> <li>Two types     <ul> <li>Component</li> <li>System</li> <li>To realize test we can use for example in memory databases</li> </ul> </li> <li>Functional</li> <li>Test the functionality of an application (create account, delete user, ...)</li> <li> <p>Verify only the functionality purpose</p> </li> <li> <p>Unit tests</p> </li> <li>Arrange</li> <li>Act</li> <li>Assert</li> </ul>"},{"location":"tests/bbd_tdd/#python","title":"Python","text":"<ul> <li>Main Components</li> <li>Test Case </li> <li>Test Suite </li> <li>Test Feature (setUp and tearDown)</li> <li> <p>Test Runner </p> </li> <li> <p>Unittest</p> </li> <li> <p><code>python -m unittest discover</code></p> </li> <li> <p>Pytest</p> <ul> <li>fixtures: are functions that create data or test doubles or initialize some system state   for the test suite.</li> <li><code>pytest</code></li> <li>Filtering</li> <li>Run one test <code>pytest -v tests/my-directory/test_demo.py::TestClassName::test_specific_method</code></li> <li>Uses <code>pytest.ini</code> as config file</li> <li>Uses <code>conftest.py</code> to define fixtures</li> <li>Marking is useful for categorizing by subsystem or dependencies<ul> <li>Run tests having mark <code>pytest -m mark_name</code></li> <li>Do not run tests having mark <code>pytest -m \"not mark_name\"</code></li> <li>Custom markers can be used</li> </ul> </li> </ul> </li> </ul>"},{"location":"tests/bbd_tdd/#java","title":"Java","text":"<ul> <li>Unit tests in Maven are managed by Surefire plugin and Integration test are managed by Failsafe plugin</li> <li> <p>Integration test names end by IT (convention in Java) which allows Maven to separate them from unit tests </p> </li> <li> <p>Define tests standard</p> </li> <li>Test types</li> <li>Code coverage  </li> <li>Put code quality check as step in CI/CD pipeline</li> <li>Add code review in dev process</li> <li>Apply SOLD and Twelve principals</li> </ul>"},{"location":"tests/mocking/","title":"Mocking","text":"<ul> <li>Mock: dummy implementation of interface or class</li> <li>Defines the output for methods</li> <li>Tow types of testing</li> <li>State</li> <li>Behavior (method was called once, ...)</li> <li>Spy: ??</li> </ul> <pre><code> @Mock\n private Myservice myService;\n\n public void testMyFunc() {\n  // theReturn: return value from mock\n  // AnyString, AnyInt\n  when(myService.getId()).thenReturn(10); \n  ...\n  // doReturn ... when \n\n\n  // verify\n  // ArgumentMatchers, times, never, atLeastOnce, atLeast\n\n\n }\n</code></pre>"}]}